---
title: "Theory: Basic model of a POMDP"
weight: 01
categories:
  - POMDP
tags:
  - POMDP
  - belief state
---

So far, we have considered a setup where the decision maker perfectly observes
the state of the system. In many applications, the decision maker may not
directly observe the state of the system but only observe a noisy version of
it. Such systems are modeled as partially observable Markov decision processes
(POMDPs). We will describe the simplest model of POMDPs, which builds upon the
[model of MDPs descibed earlier][MDP].

We assume that the system has a state $X_t \in \ALPHABET X$, control input
$U_t \in \ALPHABET U$, and process noise $W_t \in \ALPHABET W$. The state
evolves as
\begin{equation} \label{eq:state}
  X_{t+1} = f_t(X_t, U_t, W_t)
\end{equation}
However, unlike the MDP setup, the assumption is that the decision maker does
not observe $X_t$; rather, the observation of the decision maker at time $t$
is given by
\begin{equation} \label{eq:obs}
  Y_t = h_t(X_t, N_t)
\end{equation}
where $Y_t \in \ALPHABET Y$ is the observation and $N_t \in \ALPHABET N$ is
called the observation noise. As in the case of MDPs, we assume that the
_primitive random varaibles_ $(X_1, W_1, \dots, W_T$, $N_1, \dots, N_T)$ are
defined on a common probability space and are mutually independent. This
assumption is critical for the results to go through.  

As in the case of MDPs, we assume that the controller can be as sophisticated
as we want. It can analyze the entire history of observations and control
actions to choose the current control action. Thus, the control action can be
written as
$$
  U_t = g_t(Y_{1:t}, U_{1:t-1}).
$$

At each time, the system incurs a cost $c_t(X_t, U_t)$ which depends on the
current state and the current action. The system operates for a finite horizon
$T$ and incurs a total cost
$$
  \sum_{t=1}^T c_t(X_t, U_t).
$$

Given the above system model, we want to choose a _control strategy_ $g =
(g_1, \dots, g_T)$ to minimize the expected total cost
$$
  J(g) := \EXP\Bigl[ \sum_{t=1}^T c_t(X_t, U_t) \Bigr].
$$
How should we proceed?

Note that the only difference from the MDP model is decision maker observes
$Y_t$ instead of $X_t$. Apart from this, the other modeling assumptions are
the same. So, the conceptual difficulties of the model are the same as that of
MDPs:

> The data $(Y_{1:t}, U_{1:t-1})$ available at the controller is increasing
> with time. Therefore, the number of possible control laws at time $t$ are
> increasing exponentially with time. How can we search for efficiently search
> for optimal control strategies?

Recall that for MDPs, we first showed that there is no loss of optimality in
restricting attention to Markov strategies. That structural result was
instrumental in developing an efficient search algorithm (dynamic
programming). So, what is the equivalent result for POMDPs?

# History dependent dynamic program

Our first step to develop an efficient dynamic programming decomposition is to
simply ignore efficiency and develop _a_ dynamic programming decomposition.
We start by deriving a recursive formula to compute the performance of a
generic history dependent strategy $g = (g_1, \dots, g_T)$. 


## Performace of history-dependent strategies

Let $H_t = (Y_{1:t}, U_{1:t-1})$ denote all the information available to the
decision maker at time $t$. Thus, given any history dependent strategy $g$, we
can write $U_t = g_t(H_t)$. Define _the cost-to-go functions_ as follows:
$$
  J_t(h_t; g) = \EXP^g\biggl[ \sum_{s=t}^T c_s(X_s, U_s) \biggm| H_t = h_t
  \biggr].
$$
Note that $J_t(h_t; g)$ only depends on the future strategy $(g_t, \dots,
g_T)$. These functions can be computed recursively as follows: 
\begin{align*}
  J_t(h_t; g) &= \EXP^g\biggl[ \sum_{s=t}^T c_s(H_s, g_s(H_s)) \biggm|
    H_t = h_t \biggr] \\
    &\stackrel{(a)}= \EXP^g \biggl[ c_t(h_t, g_t(h_t)) + \EXP^g\biggl[ 
    \sum_{s=t+1}^T c_s(X_s, g_s(X_s)) \biggm| H_{t+1} \biggr] \biggm| 
    H_t = h_t \biggr]  \\
    &= \EXP^g[ c_t(h_t, g_t(h_t)) + J_{t+1}(H_{t+1}; g) \mid H_t = h_t ],
\end{align*}
where $(a)$ follows from the towering property of conditional expectation and
the fact that $H_t \subseteq H_{t+1}$. 

Thus, we can use the following dynamic program to recursively compute the
performance of a history-dependent strategy: $J_{T+1}(h_{T+1}) = 0$ and for $t
\in \{T, \dots, 1\}$, 
$$
J_t(h_t; g) = \EXP^g [ c_t(h_t, g_t(h_t)) + J_{t+1}(H_{t+1}; g) \mid
  H_t = h_t ].
$$

## History-dependent dynamic programming decomposition
We can use the above recursive formulation for performance evaluation to
derive a history-dependent dynamic program.

::: highlight :::
Theorem #history-dp

:   Recursively define _value functions $\{V_t\}_{t = 1}^{T+1}$, where $V_t
    \colon \ALPHABET H_t \to \reals$ as follows:
    \begin{equation}
      V_{T+1}(h_{T+1}) = 0 
    \end{equation}
    and for $t \in \{T, \dots, 1\}$: 
    \begin{align}
      Q_t(h_t, u_t) &= \EXP[ c_t(X_t, u_t) + V_{t+1}(H_{t+1}) \mid
      H_t = h_t, U_t = u_t ] \\
      V_t(h_t) &= \min_{u_t \in \ALPHABET U} Q_t(h_t, u_t)
    \end{align}
    Then, a history-dependent policy $g$ is optimal if and only if it
    satisfies
    \begin{equation} \label{eq:history-verification}
      g_t(h_t) \in \arg \min_{u_t \in \ALPHABET U} Q_t(h_t, u_t).
    \end{equation}
:::

The proof idea is similar to the proof for MDPs. Instead of proving the above
result, we prove a related result.

::: highlight :::
Theorem #history-comparison

:   **(The comparison principle)** For any history-dependent strategy $g$
    $$ J_t(h_t; g) \ge V_t(h_t) $$
    with equality at $t$ if and only if the _future straegy $g_{t:T}$
    satisfies the verification step \\eqref{eq:history-verification}.
::: 

Note that the comparison principle immediately implies that the strategy
obtained using dynamic program of @theorem:history-dp is optimal. The proof of
the comparison principle is almost identical to the proof for MDPs.

#### Proof of the comparison principle {-}

The proof proceeds by backward induction. Consider any history dependent
policy $g = (g_1, \dots, g_T)$. For $t = T+1$, the comparison principle is
satisfied by definition and this forms the basis of induction. We assume that
the result holds for time $t+1$, which is the induction hypothesis. Then for
time $t$, we have 
\begin{align*}
  V_t(h_t) &= \min_{u_t \in \ALPHABET U} Q_t(h_t, u_t) \\
  &\stackrel{(a)}= \min_{u_t \in \ALPHABET U}
   \EXP^g[ c_t(X_t, g_t(h_t)) + V_{t+1}(H_{t+1}) \mid
  H_t = h_t, U_t = g_t(h_t) ] 
  \\
  &\stackrel{(b)}\le 
   \EXP^g[ c_t(X_t, g_t(h_t)) + V_{t+1}(H_{t+1}) \mid
  H_t = h_t, U_t = g_t(h_t)] 
  \\
  &\stackrel{(c)}\le
   \EXP^g[ c_t(X_t, g_t(h_t)) + J_{t+1}(H_{t+1}; g) \mid
  H_t = h_t, U_t = g_t(h_t)] 
  \\
  &= J_t(h_t, g).
\end{align*}
where $(a)$ follows from the definition of the $Q$-function; $(b)$ follows
from the definition of minimization; and $(c)$ follows from the induction
hyothesis. We have the equality at step $(b)$ iff $g_t$ satisfies the
verification step \\eqref{eq:history-verification} and have the equality in step $(c)$
iff $g_{t+1:T}$ is optimal (this is part of the induction hypothesis). Thus,
the result is true for time $t$ and, by the principle of induction, is true
for all time. $\Box$

# The notion of an information state

Now that we have obtained a dynamic programming decomposition, let's try to
simplify it. To do so, we define the notion of an _information state_. 

A stochastic process $\{Z_t\}_{t = 1}^T$, $Z_t \in \ALPHABET Z$, is called an
_information state_ if $Z_t$ be a function of $H_t$ (which we denote by $Z_t =
φ_t(H_t)$) and satisfies the following two properties:

**P1. Sufficient for performance evaluation**, i.e., 
    $$ \EXP^g[ c_t(X_t, U_t) \mid H_t = h_t, U_t = u_t] 
    =  \EXP[ c_t(X_t, U_t) \mid Z_t = φ_t(h_t), U_t = u_t ] $$

**P2. Sufficient to predict itself**, i.e., for any Borel measurable subset
    $B$ of $\ALPHABET Z$, we have
    $$ \PR^g(Z_{t+1} \in B \mid H_t = h_t, U_t = u_t) = 
       \PR(Z_{t+1} \in B \mid Z_t = φ_t(h_t), U_t = u_t).
    $$


    
Instead of (P2), the following sufficient conditions are easier to verify in
some models:

**P2a. Evolves in a state-like manner**, i.e., there exist measurable
     functions $\{ψ_t\}_{t=1}^T$ such that
     $$ Z_{t+1} = ψ_t(Z_t, Y_{t+1}, U_t). $$

**P2b. Is sufficient for predicting future observations**, i.e., for any Borel
     subset $B$ of $\ALPHABET Y$, 
     $$ \PR^g(Y_{t+1} \in B | H_t = h_t, U_t = u_t) = 
        \PR(Y_{t+1} \in B | Z_t = φ_t(h_t), U_t = u_t). 
     $$

Remark

:   The right hand sides of (P1) and (P2) as well as (P2a) and (P2b) do not
    depend on the choice of the policy $g$.

::: highlight :::
Proposition #info-state

:  (P2a) and (P2b) imply (P2).

:::

### Proof

For any Borel measurable subset $B$ of $\ALPHABET Z$, we have
\begin{align*}
  \hskip 1em & \hskip -1em 
  \PR(Z_{t+1} \in B \mid H_t = h_t, U_t = u_t)  
  \stackrel{(a)}= \sum_{y_{t+1} \in \ALPHABET Y} \PR(Y_t = y_t, Z_{t+1} \in B
  \mid H_t = h_t, U_t = u_t ] 
  \\
  &\stackrel{(b)}= \sum_{y_t \in \ALPHABET Y} \IND\{ ψ_t(φ_t(h_t), y_t, u_t) \}
  \PR(Y_t = y_t \mid H_t = h_t, U_t = u_t) 
  \\
  &\stackrel{(c)}= \sum_{y_t \in \ALPHABET Y} \IND\{ ψ_t(φ_t(h_t), y_t, u_t) \}
  \PR(Y_t = y_t \mid Z_t = φ_t(h_t), U_t = u_t) 
  \\
  &\stackrel{(d)}=
  \PR(Z_{t+1} \in B \mid Z_t = φ_t(h_t), U_t = u_t)  
\end{align*}
where $(a)$ follows from the law of total probability, $(b)$ follows from
(P2a), $(c)$ follows from (P2b), and $(d)$ from the law of total probability. 

---

## Examples of an information state

::: highlight :::

Example #history

: The complete history $H_t$ is an information state.

:::

To establish this result, we define the _belief state_ $π_t \in Δ(\ALPHABET
X)$ as follows: for any $x \in \ALPHABET X$
$$ π_t(x) = \PR^g(X_t = x \mid H_t = h_t). $$
The belief state is a function of the history $h_t$. When we want to
explicitly show the dependence of $π_t$ on $h_t$, we write it as $π_t[h_t]$.

::: highlight :::

Lemma #belief-independence

:   The belief state $π_t$ does not depend on the policy $g$.

:::

This is an extremely important result which has wide-ranging implications in
stochastic control. For a general discussion of this point, see
@Witsenhausen1975.

### Proof of @lemma:belief-independence

From the law of total probability and Bayes rule, we have
\begin{equation} \label{eq:belief}
  \PR(x_t | y_{1:t}, u_{1:t-1}) 
  = \sum_{x_{1:t-1}} \PR(x_{1:t} | y_{1:t}, u_{1:t-1}) 
  = \sum_{x_{1:t-1}}
   \frac{\PR(x_{1:t}, y_{1:t}, u_{1:t-1})}
   {\sum_{x'_{1:t}} \PR(x'_{1:t}, y_{1:t}, u_{1:t-1})}
\end{equation}

Now consider
\begin{align*}
  \PR(x_{1:t}, y_{1:t}, u_{1:t-1}) &=
  \PR(x_1) \PR(y_1 | x_1) \IND\{ u_1 = g_1(y_1) \} \\
  & \times
  \PR(x_2 | x_1, u_1) \PR(y_2 | x_2) \IND \{ u_2 = g_2(y_{1:2}, u_1)\} \\
  & \times \cdots \\
  & \times
  \PR(x_{t-1} | x_{t-2}, u_{t-2}) \PR(y_{t-1} | x_{t-1}) \IND \{ u_{t-1} =
  g_{t-1}(y_{1:t-1}, u_{1:t-2}) \} \\
  & \times
  \PR(x_{t} | x_{t-1}, u_{t-1}) \PR(y_{t} | x_{t}).
\end{align*}
Substitute the above expression in both the numerator and the denominator of
\\eqref{eq:belief}. Observe that the terms of the form $\IND\{ u_s =
g_s(y_{1:s}, u_{1:s-1})$ are common to both the numerator and the denominator
and cancel each other. Thus,
\begin{equation} \label{eq:belief-fn}
  \PR(x_t | y_{1:t}, u_{1:t-1}) = \sum_{x_{1:t-1}}
  \frac{ \prod_{s=1}^t \PR(x_s \mid x_{s-1}, u_{s-1}) \PR(y_s \mid x_s) }
  { \sum_{x'_{1:t}} \prod_{s=1}^t \PR(x'_s \mid x'_{s-1}, u_{s-1}) \PR(y_s \mid x'_s) }.
\end{equation}
None of the terms here depend on the policy $g$. Hence, the belief state does
not depend on the policy $g$. $\Box$

#### Proof of @example:history {-}

We will prove that $Z_t = H_t$ satisfies properties (P1), (P2a), and (P2b).

P1. $\displaystyle \EXP^g[ c_t(X_t, U_t) | H_t = h_t, U_t = u_t ] 
= \sum_{x_t \in \ALPHABET X} c_t(x_t, u_t) π_t[h_t](x_t)$.

P2a. $H_{t+1} = (H_t, Y_{t+1}, U_t)$

P2b. $\displaystyle \PR^g(y_{t+1} | y_{1:t}, u_{1:t}) 
= \sum_{x_{t:t+1}} \PR(y_{t+1} | x_{t+1}) \PR( x_{t+1} | x_t, u_t) \PR(x_t |
y_{1:t}, u_{1:t})$. Note that in the last term $\PR^g(x_t | y_{1:t}, u_{1:t})$
we can drop $u_t$ from the conditioning because it is a function of $(y_{1:t},
u_{1:t-1})$. Thus, 
$$ \PR^g(x_t | y_{1:t}, u_{1:t}) = \PR^g(x_t | y_{1:t}, u_{1:t-1}) =
π_t[h_t](x_t).$$ 
Thus,
$\displaystyle \PR^g(y_{t+1} | y_{1:t}, u_{1:t}) 
= \sum_{x_{t:t+1}} \PR(y_{t+1} | x_{t+1}) \PR( x_{t+1} | x_t, u_t)
π_t[h_t](x_t)$. 

::: highlight :::

Example #belief

:   The belief state $π_t$ is an information state.

:::

#### Proof {-}

The belief state $π_t$ is a function of the history $h_t$. (The exact form of
this function is given by \\eqref{eq:belief-fn}). In the proof of
@example:history, we have already shown that $π_t$ satisfies (P1) and (P2b).
So, we only need to show that $π_t$ satisfies (P2a). This can be shown as
follows, which is a standard result in non-linear filtering.

::: highlight :::

Lemma #belief-update

:   The belief state $π_t$ updates in a state like manner. In particular, for
any $x_{t+1} \in \ALPHABET X$, we have
$$
  π_{t+1}(x_{t+1}) = \sum_{x_t \in \ALPHABET X}
  \frac{ \PR(y_{t+1} | x_{t+1}) \PR(x_{t+1} | x_t, u_t) π_t(x_t) }
   { \sum_{x'_{t:t+1}} \PR(y_{t+1} | x'_{t+1}) \PR(x'_{t+1} | x'_t, u_t) π_t(x'_t) }.
$$

:::

#### Proof of @lemma:belief-update {-}

For any $x_{t+1} \in \ALPHABET X$, consider

\begin{align}
π_{t+1}(x_{t+1}) &= \PR(x_{t+1} | y_{1:t+1}, u_{1:t}) \notag \\
&= \sum_{x_t} \PR(x_{t:t+1} | y_{1:t+1}, u_{1:t}) \notag \\
&= \sum_{x_t} \frac{ \PR(x_{t:t+1}, y_{t+1}, u_t | y_{1:t}, u_{1:t-1}) }
  {\sum_{x'_{t:t+1}}\PR(x'_{t:t+1}, y_{t+1}, u_t | y_{1:t}, u_{1:t-1}) }.
\label{eq:update-1}
\end{align}

Now consider
\begin{align}
\hskip 1em & \hskip -1em 
\PR(x_{t:t+1}, y_{t+1}, u_t | y_{1:t}, u_{1:t-1}) \notag \\
&= \PR(y_{t+1} | x_{t+1}) \PR(x_{t+1} | x_t, u_t) 
   \IND\{ u_t = g_t(y_{1:t}, u_{1:t-1}) \}
   \PR(x_t | y_{1:t}, u_{1_t-1}) \notag \\
&= \PR(y_{t+1} | x_{t+1}) \PR(x_{t+1} | x_t, u_t) 
   \IND\{ u_t = g_t(y_{1:t}, u_{1:t-1}) \}
   π_t(x_t). \label{eq:belief-2}
\end{align}
Substitute the above expression in both the numerator and the denominator of
\\eqref{eq:update-1}. Observe that $\IND\{ u_t = g_t(y_{1:t}, u_{1:t-1}) \}$
is common to both the numerator and the denominator and cancels out. Thus, we
get the result of the lemma. $\Box$

Remark 

: Both the above information states are generic information states which work
  for all models. For specific models, it is possible to identify other
  information states as well. We will present an example of such an information
  state later.

# Information state based dynamic program

The main feature of an information state is that one can always write a
dynamic program based on an information state. 

::: highlight :::

Theorem #info-state

: Let $\{Z_t\}_{t=1}^T$ be any information state, where $Z_t = φ_t(H_t)$.
  Recursively define value functions $\{ \hat V_t \}_{t=1}^T$, where $\hat V_t
  \colon \ALPHABET Z \to \reals$, as follows: 
  $$ \hat V_{T+1}(z_{T+1}) = 0 $$
  and for $t \in \{T, \dots, 1\}$: 
  \begin{align}
    \hat Q_t(z_t, u_t) &= \EXP[ c_t(X_t, U_t) + \hat V_{t+1}(Z_{t+1}) \mid
    Z_t = z_t, U_t = u_t] \\
    \hat V_t(z_t) &= \min_{u_t \in \ALPHABET U} \hat Q_t(z_t, u_t).
  \end{align}
  Then, we have the following: for any $h_t$ and $u_t$,
  \begin{equation} \label{eq:history-info}
    Q_t(h_t, u_t) = \hat Q_t(φ_t(h_t), u_t)
    \quad\text{and}\quad
    V_t(h_t) = \hat V_t(φ_t(h_t)).
  \end{equation}
  Any stategy $\hat g = (\hat g_1, \dots, \hat g_T)$, where $\hat g_t \colon
  \ALPHABET Z \to \ALPHABET U$, is optimal if and only if 
  \begin{equation}\label{eq:info-verification}
      \hat g_t(z_t) \in \arg\min_{u_t \in \ALPHABET U} \hat Q_t(z_t, u_t).
  \end{equation}

:::

#### Proof {-}

As usual, we prove the result by backward induction. By constuction,
Eq. \\eqref{eq:history-info} is true at time $T+1$. This forms the basis of
induction. Now assume that \\eqref{eq:history-info} is true at time $t+1$ and
consider the system at time $t$. Then, 
\begin{align*}
Q_t(h_t, u_t) &= \EXP[ c_t(X_t, U_t) + V_{t+1}(H_{t+1}) | H_t = h_t, U_t = u_t
] \\
&\stackrel{(a)}= \EXP[ c_t(X_t, U_t) + \hat V_{t+1}( φ_t(H_{t+1}) ) | H_t =
h_t, U_t = u_t ]  \\
&\stackrel{(b)}= \EXP[ c_t(X_t, U_t) + \hat V_{t+1}( φ_t(H_{t+1}) ) | Z_t =
φ_t(h_t), U_t = u_t ]  \\
&\stackrel{(c)}= \hat Q_t(φ_t(h_t), u_t),
\end{align*}
where $(a)$ follows from the induction hypothesis, $(b)$ follows from the
properties (P1) and (P2) of the information state, and $(c)$ follows from the
definition of $\hat Q_t$. This shows that the action value functions are
equal. By minimizing over the actions, we get that the value functions are
also equal. $\Box$


# References {-}

The discussion in this section is taken from @Subramanian2019.




[MDP]: ../../mdp/mdp-functional


