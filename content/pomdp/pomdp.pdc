---
title: "Theory: Basic model of a POMDP"
weight: 01
categories:
  - POMDP
tags:
  - POMDP
  - belief state
---

So far, we have considered a setup where the decision maker perfectly observes
the state of the system. In many applications, the decision maker may not
directly observe the state of the system but only observe a noisy version of
it. Such systems are modeled as partially observable Markov decision processes
(POMDPs). We will describe the simplest model of POMDPs, which builds upon the
[model of MDPs descibed earlier][MDP].

We assume that the system has a state $X_t \in \ALPHABET X$, control input
$U_t \in \ALPHABET U$, and process noise $W_t \in \ALPHABET W$. The state
evolves as
\begin{equation} \label{eq:state}
  X_{t+1} = f_t(X_t, U_t, W_t)
\end{equation}
However, unlike the MDP setup, the assumption is that the decision maker does
not observe $X_t$; rather, the observation of the decision maker at timeÂ $t$
is given by
\begin{equation}
  Y_t = h_t(X_t, N_t)
\end{equation}
where $Y_t \in \ALPHABET Y$ is the observation and $N_t \in \ALPHABET N$ is
called the observation noise. As in the case of MDPs, we assume that the
_primitive random varaibles_ $(X_1, W_1, \dots, W_T$, $N_1, \dots, N_T)$ are
defined on a common probability space and are mutually independent. This
assumption is critical for the results to go through.  

As in the case of MDPs, we assume that the controller can be as sophisticated
as we want. It can analyze the entire history of observations and control
actions to choose the current control action. Thus, the control action can be
written as
$$
  U_t = g_t(Y_{1:t}, U_{1:t-1}).
$$

At each time, the system incurs a cost $c_t(X_t, U_t)$ which depends on the
current state and the current action. The system operates for a finite horizon
$T$ and incurs a total cost
$$
  \sum_{t=1}^T c_t(X_t, U_t).
$$

Given the above system model, we want to choose a _control strategy_ $g =
(g_1, \dots, g_T)$ to minimize the expected total cost
$$
  J(g) := \EXP\Bigl[ \sum_{t=1}^T c_t(X_t, U_t) \Bigr].
$$
How should we proceed?

Note that the only difference from the MDP model is decision maker observes
$Y_t$ instead of $X_t$. Apart from this, the other modeling assumptions are
the same. So, the conceptual difficulties of the model are the same as that of
MDPs:

> The data $(Y_{1:t}, U_{1:t-1})$ available at the controller is increasing
> with time. Therefore, the number of possible control laws at time $t$ are
> increasing exponentially with time. How can we search for efficiently search
> for optimal control strategies?

Recall that for MDPs, we first showed that there is no loss of optimality in
restricting attention to Markov strategies. That structural result was
instrumental in developing an efficient search algorithm (dynamic
programming). So, what is the equivalent result for POMDPs?

## History dependent dynamic program

Our first step to develop an efficient dynamic programming decomposition is to
simply ignore efficiency and develop _a_ dynamic programming decomposition. 



[MDP]: ../../mdp/mdp-functional


