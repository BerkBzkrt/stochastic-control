---
title: Infinite horizon discounted MDP
weight: 01
categories:
  - MDP
tags:
  - infinite horizon
  - discounted cost
  - Bellman operator
  - value iteration
  - policy iteration
---

_Note_: Throughout this section, we assume that $\ALPHABET X$ and $\ALPHABET U$ are
finite and $|\ALPHABET X|= n$ and $|\ALPHABET U| = m$. 

## Performance of a time-homogeneous Markov policy

For any $g \colon \ALPHABET X \to \ALPHABET U$, consider the time homogeneous
policy $(g, g, \dots)$. For ease of notation, we denote this policy simply by
$g$. The expected discounted cost under this policy is given by
$$ V_g(x) = \EXP^g\bigg[ \sum_{t=1}^∞ β^{t-1} c(X_t, U_t) \biggm| X_1 = x
\bigg].$$

To get a compact expression for this, define a $n × 1$ vector $c_g$ and a $n
× n$ matrix $P_g$ as follows:
$$ [c_g]_x = c(x, g(x))
   \quad\text{and}\quad
   [P_g]_{xy} = P_{xy}(g(x)).
$$
Then the dynamics under policy $g$ are Markovian with transition probability
matrix $P_g$ and a cost function $c_g$. Then
$$ \begin{align*}
\EXP^g\big[ c(X_t, g(X_t)) \bigm| X_1 = x \big]
  &= \sum_{y \in \ALPHABET X} \PR^g(X_t = y | X_1 = x) c(y, g(y))
  \\
  &= \sum_{y \in \ALPHABET X} [P_g^{t-1}]_{xy} [c_g]_y
  \\
  &= δ_x P_g^{t-1} c_g.
\end{align*} $$

Let $V_g$ denote the $n × 1$ vector given by $[V_g]_x = V_g(x)$. Then,
$$ \begin{align*}
V_g &= c_g + β P_g c_g + β^2 P_g^2 c_g + \cdots \\
    &= c_g + β P_g \big( c_g + β P_g c_g + \cdots \big) \\
    &= c_g + β P_g V_g,
\end{align*} $$
which can be rewritten as
$$ (I - β P_g) V_g = c_g. $$

The [spectral radius] of $ρ(β P_d)$ is upper bounded by $\lVert β P_d \rVert = β < 1$. Therefore, the matrix $(I - β P_g)$ has an inverse and by left multiplying both sides by $(I - β P_g)^{-1}$, we get
$$ V_g = (I - βP_g)^{-1} c_g. $$

[spectral radius]: https://en.wikipedia.org/wiki/Spectral_radius

The equation 
$$ V_g = c_g + β P_g V_g $$
is sometimes also written as
$$ V_g = \mathcal B_g V_g $$
where the operator $\mathcal B_g$, which is called the _Bellman operator_,
is an operator from $\reals^n$ to $\reals^n$
given by
$$ \mathcal B_g v = c_g + β P_g v.$$

---

## Bellman operator 

Definition

:   Define the _Bellman operator_ $\mathcal B : \reals^n \to \reals^n$ as
    follows: for any $v \in \reals^n$
    $$ [\mathcal B v]_x = \min_{u \in \ALPHABET U}
    \Big\{ c(x,u) + β \sum_{y \in \ALPHABET Y} P_{xy}(u) v_y \Big\}.
    $$

Note that the above may also be written as[^1]
$$ \mathcal B v = \min_{g \in \ALPHABET G} \mathcal B_g v, $$
where $\ALPHABET G$ denotes the set of all deterministic Markov policies. 

[^1]: This is true for general models only when the arg min at each state
  exists.

::: highlight :::
Prop. #prop:contraction

:   For any $v \in \reals^n$, define the norm $\NORM{V} := \sup_{x \in
    \ALPHABET X} \ABS{V_x}$. Then, the Bellman operator is a contraction,
    i.e., for any $v, w \in \reals^n$,
    $$ \NORM{\mathcal B v - \mathcal B w} \le β \NORM{v - w}. $$
:::


#### Proof

Fix a state $x \in \ALPHABET X$ and consider $[\mathcal B v](x) - [\mathcal B
w](x)$. In particular, let $u^*$ be the optimal action in the right hand side
of $[\mathcal B w](x)$. Then,
$$\begin{align*}
  [\mathcal B v - \mathcal B w](x) &= 
  \min_{u \in \ALPHABET U}\bigl\{ c(x,u) + \beta \sum_{y \in \ALPHABET X}
  P_{xy}(u) v(y) \bigr\} - 
  \min_{u \in \ALPHABET U}\bigl\{ c(x,u) + \beta \sum_{y \in \ALPHABET X}
  P_{xy}(u) w(y) \bigr\} 
  \\
  &\le c(x,u^*) + \beta \sum_{y\in \ALPHABET X} P_{xy}(u^*) v(y) -
       c(x,u^*) - \beta \sum_{y\in \ALPHABET X} P_{xy}(u^*) w(y)
  \\
  &\le \beta \sum_{y \in \ALPHABET X} P_{xy}(u^*) \| v - w \|
  \\
  &= \beta \| v - w \|.
\end{align*} $$

By a similar argument, we can show that $[\mathcal B w - \mathcal B v](x) \le
\beta \| v - w \|$, which proves the other side of the inequality. 
&nbsp;$\Box$

An immediate consequence of the contraction property is the following.

::: highlight :::
Theorem #thm:fixed-point

:   There is a unique bounded $V^* \in \reals^n$ that satisfies the 
    _Bellman equation_
    $$ V = \mathcal B V $$

    Moreover, if we start from any $V_0 \in \reals^n$ and recursively define
    $$ V_n = \mathcal B V_{n-1} $$
    then
    $$ \lim_{n \to ∞} V_n = V^*. $$
:::

#### Proof

This follows immediately from the [Banach fixed point
theorem](https://proofwiki.org/wiki/Banach_Fixed-Point_Theorem)
&nbsp;$\Box$

## Optimal time-homogeneous policy

<!--
::: highlight :::
Prop.

:   Let $h$ be any (possibly randomized) history dependent policy. Then,      
    there exists a randomized Markov policy $g$ that has the same performance
    as $h$.
:::

#### Proof

For any policy $h$, define the _occupation measure_
$$ \mu(x,u | x^∘; h) 
  := \EXP^h \bigg[ \sum_{t=1}^∞ β^{t-1} \IND\{X_t = x, U_t = u\} 
  \biggm| X_1 = x^∘ \bigg]
$$
Then 
$$ J(x^∘; h) = \sum_{x \in \ALPHABET X, u \in \ALPHABET U}
   μ(x,u | x^∘; h) c(x,u).
$$ {#eq:performance}

Now, define a randomized Markov policy $g$ as follows:
$$\PR(U_t = u | X_t = x; g) = \frac{ μ(x, u| x^∘; h) } { μ(x | x^∘; h) }$$
where $μ(x|x^∘;h) = \sum_{u \in \ALPHABET U} μ(x,u | x^∘; h)$. Then, by
construction, 
$ μ(x,u | x^∘; g) = μ(x,u | x^∘; h) $
and therefore by \\eqref{eq:performance}, we have that $J(x^∘; g) = J(x^∘; h).$

-->


::: highlight :::
Prop. #prop:optimal

:   Define 
    $$ V^{opt}_∞(x) := \min_{g} \EXP^g \bigg[ \sum_{t=1}^∞ β^{t-1} c(X_t, U_t) 
    \biggm| X_1 = x \bigg], $$
    where the minimum is over all (possibly randomized) history dependent
    policies. Then, 
    $$ V^{opt} = V^*, $$
    where $V^*$ is the solution of the Bellman equation.
:::

#### Proof

Since the state and action space are finite, without loss of optimality, we
can assume that $0 \le c(x,u) \le M$. 

Consider the finite horizon truncation 
$$ V^{opt}_T(x) =  \min_{g} \EXP^g\bigg[ \sum_{t=1}^T β^{t-1} c(X_t, U_t) | X_1 = x \bigg].
$$
From the results for finite horizon MDP, we have that
$$ V^{opt}_T = \mathcal B^T V_0 $$
where $V_0$ is the all zeros vector. 

Now by construction, 
$$V^{opt}_∞(x) \ge V^{opt}_T(x) = [\mathcal B^T V_0](x). $$
Taking limit as $T \to ∞$, we get that
$$\begin{equation}\label{eq:1}
  V^{opt}_∞(x) \ge \lim_{T \to ∞} [\mathcal B^T V_0](x) = V^*(x). 
\end{equation}$$

Since $0 \le c(x,u) \le M$,  for any $T$,
$$ \begin{align*}
V^{opt}_∞(x) &\le \min_g \EXP^g \bigg[ \sum_{t=1}^T β^{t-1} c(X_t, U_t) 
\biggm| X_1 = x \bigg] + \sum_{t=T+1}^∞ β^{t-1} M \\
&= V^{opt}_T(x) + β^T M / (1 - β) \\
&= [\mathcal B^T V_0](x) + β^T M / (1-β). 
\end{align*} $$
Taking limit as $T \to ∞$, we get that
$$\begin{equation}\label{eq:2}
  V^{opt}_∞(x) \le \lim_{T \to ∞}
  \big\{ [\mathcal B^T V_0](x) + β^T M / (1-β) \big\} = V^*(x). 
\end{equation}$$

From \\eqref{eq:1} and \\eqref{eq:2}, we get that $V^{opt}_∞ = V^*$. 
&nbsp;$\Box$

## Properties of Bellman operator

::: highlight :::
Prop. #prop:properties

:   The Bellman operator satisfies the following properties

    * _Monotonicity_. For any $v, w \in \reals^n$, if $v \le w$, then
      $\mathcal B_g v \le \mathcal B_g w$ and 
      $\mathcal B v \le \mathcal B w$. 
    * _Discounting_. For any $v \in \reals^n$ and $m \in \reals$, 
      $\mathcal B_g (v + m) = \mathcal B_g v + β m$
      and 
      $\mathcal B (v + m) = \mathcal B v + β m$.
:::

Note that we interpret $v + m$ as $[v + m]_x = v_x + m$. 

#### Proof
We first prove the monotonicity property. Recall that
$$ \mathcal B_g v = c_g + β P_g v. $$
So, monotonicity of $\mathcal B_g$ follows immediately from monotonicity of
matrix multiplication for positive matrices. 

Let $h$ be such that $\mathcal B w = \mathcal B_h w$. 
Then,
$$ v = \mathcal B v \le \mathcal B_h v 
\stackrel{(a)} \le \mathcal B_h w = \mathcal B w = w,
$$
where $(a)$ uses the monotonicity of $\mathcal B_h$. 

We now prove the discounting property. 
Recall that
$$ \mathcal B_g v = c_g + β P_g v. $$
Thus,
$$ \mathcal B_g(v+m) = c_g + β P_g (v+m) = c_g + β P_g v + β m = \mathcal B_g
v + β m.$$ 
Thus, $\mathcal B_g$ is discounting. Now consider 
$$ \mathcal B (v + m ) = \min_{g} \mathcal B_g (v+m)
= \min_g \mathcal (B_g v + β m) = \mathcal B v + β m.$$
Thus, $\mathcal B$ is discounting.
&nbsp;$\Box$

::: highlight :::
Prop. #prop:coarse-bounds

:   For any $V \in \reals^n$,

    * If $V \ge \mathcal B V$, then $V \ge V^*$;
    * If $V \le \mathcal B V$, then $V \le V^*$;
    * If $V = \mathcal B V$, then $V$ is the only vector with this property
      and $V = V^*$.

    The same bounds are true when $(\mathcal B, V^*)$ is replaced with
    $(\mathcal B_g, V_g)$. 
:::

#### Proof
We prove the first part. The proof of the other parts is similar.

We are given that
$$V \ge \mathcal B V.$$ 
Then, by monotonicity of the Bellman operator, 
$$ \mathcal B V \ge \mathcal B^2 V.$$ 
Continuing this way, we get
$$ \mathcal B^k V \ge \mathcal B^{k+1} V.$$
Adding the above equations, we get
$$ V \ge \mathcal B^{k+1} V.$$
Taking limit as $k \to ∞$, we get 
$$V \ge V^*. \qquad \Box$$

::: highlight :::
Prop. #prop:bounds

:   For any $V \in \reals^n$ and $m \in \reals$, 

    * If $V + m \ge \mathcal B V$, then $V  + m/(1-β) \ge V^*$;
    * If $V + m \le \mathcal B V$, then $V  + m/(1-β) \le V^*$;

    The same bounds are true when $(\mathcal B, V^*)$ is replaced with
    $(\mathcal B_g, V_g)$. 
:::

#### Proof
Again, we only prove the first part. The proof of the second part is the same. We have that
$$ V + m \ge \mathcal B V. $$
From discounting and monotonicity properties, we get
$$ \mathcal B V + β m \ge \mathcal B^2 V. $$
Again, from discounting and monotonitiy properties, we get
$$ \mathcal B^2 V + β^2 m \ge \mathcal B^3 V. $$
Continuing this way, we get
$$ \mathcal B^k V + β^k m \ge \mathcal B^{k+1} V. $$
Adding all the above equations, we get
$$ V + \sum_{\ell = 0}^k β^\ell m \ge \mathcal B^{k+1} V. $$
Taking the limit as $k \to ∞$, we get
$$ V + m/(1-β) \ge V^*. \Box$$


---

# Value Iteration Algorithm

::: highlight :::

#### Value Iteration Algorithm

1. Start with any $V_0 \in \reals^n$.
2. Recursively compute $V_{k+1} = \mathcal B V_k = \mathcal B_{g_k} V_k.$
3. Define 
   $$ \begin{align*}
      \underline δ_k &= \frac{β}{1-β} \min_x \{ V_k(x) - V_{k-1}(x) \}, \\
      \bar δ_k &=       \frac{β}{1-β} \max_x \{ V_k(x) - V_{k-1}(x) \}.
    \end{align*} $$

Then, for all $k$

1. $V_k + \underline δ_k \le V^* \le V_k + \bar δ_k$.
2. $\underline δ_k - \bar δ_k  \le \NORM{V_{g_k} - V^*} \le \bar δ_k -
   \underline δ_k.$
   
:::  

#### Proof

By construction, 
$$ \begin{align*} 
   \mathcal B V_k - V_k &= \mathcal B V_k - \mathcal B V_{k-1} \\ 
   & \le \mathcal B_{g_{k-1}} V_k - \mathcal B_{g_{k-1}} V_{k-1}\\
   & \le β P_{g_{k-1}}[ V_k - V_{k-1} ] \\
   &= (1-β) \bar δ_k.
\end{align*} $$
Thus, by the previous proposition, we have
$$ V^* \le V_k + \bar δ_k.$$
Note that $\mathcal B V_k = \mathcal B_{g_k} V_k$. So, we have also show that
$\mathcal B_{g_k} V_k - V_k \le (1-\beta) \bar \delta_k$. Thus,
$$ V_{g_k} \le V_k + \bar δ_k. $$

By a similar argument, we can show
$$ V^* \ge V_k + \underline δ_k
\quad\text{and}\quad
 V_{g_k} \ge V_k + \underline δ_k. $$

Now, by the triangle inequality
$$ \max\{ V_{g_k} - V^* \} \le 
   \max\{ V_{g_k} - V_k \} + \max\{ V_{k} - V^* \}
   \le \bar δ_k - \underline δ_k.
$$
Similarly,
$$ \max\{ V^* - V_{g_k} \} \le
   \max \{ V^* - V_k \} + \max\{ V_k - V_{g_k} \}
   \le \bar δ_k - \underline δ_k.
$$

Thus, 
$$ \NORM{ V_{g_k} - V^* } \le \bar δ_k - \underline δ_k. $$

By a similar argument, we can show that
$$ \NORM{ V_{g_k} - V^* } \ge \underline δ_k - \bar δ_k. 
\qquad \Box$$

---

# Policy Iteration Algorithm


::: highlight :::
Lemma #lemma:PI

:   **Policy Improvement** Suppose $V_g$ is the fixed point of $\mathcal B_g$ and
    $$\mathcal B_{h} V_g = \mathcal B V_g. $$
    Then,
    $$V_{h}(x) \le V_g(x), \quad \forall x \in \ALPHABET X. $$
    Moreover, if $g$ is not optimal, then at least one inequality is strict. 
:::

#### Proof

$$ V_g = \mathcal B_g V_g \ge \mathcal B V_g = \mathcal B_{h} V_g.$$
Thus,
$$ V_g \ge V_{h}. $$

Finally, suppose $V_h = V_g$. Then, 
$$ V_h = \mathcal B_h V_h = \mathcal B_h V_g = \mathcal B V_g = \mathcal B
V_h. $$
Thus, $V_h$ (and $V_g$) is the unique fixed point of $\mathcal B$. Hence $h$
and $g$ are optimal.
&nbsp;$\Box$

::: highlight :::

#### Policy Iteration Algorithm

1. Start with an arbitrary policy $g_0$. Compute $V_0 = \mathcal B_{g_0} V_0$.
2. Recursively compute a policy $g_k$ such that
   $$\mathcal B V_{k-1} = \mathcal B_{g_k} V_{k-1}$$
   and compute the performance of the policy using
   $$ V_k = \mathcal B_{g_k} V_k.$$ 

3. Stop if $V_k = V_{k-1}$ (or $g_k = g_{k-1}$). 

:::

The policy improvement lemma ([Lemma #](#lemma:PI)) implies that $V_{k-1} \ge
V_k$. Since the state and action spaces are finite, there are only a finite
number of policies. The value function improves at each step. So the process
must converge in finite number of iterations. At convergence, $V_k = V_{k-1}$
and the policy improvement lemma implies that the corresponding policies $g_k$
or $g_{k-1}$ are optimal. 

---

## Optimistic Policy Iteration

::: highlight :::

#### Optimistic Policy Iteration Algorithm

1. Fix a sequence of integers $\{\ell_k\}_{k \in \integers_{\ge 0}}$. 

2. Start with an initial guess $V_0 \in \reals^n$. 

3. For $k=0, 1, 2, \dots$, recursively compute a policy $g_k$ such that
   $$ \mathcal B_{g_k} V_k = \mathcal B V_k $$
   and then update the value function
   $$ V_{k+1} = \mathcal B_{g_k}^{\ell_k} V_k. $$

:::

Note that if $\ell_k = 1$, the optimistic policy iteration is equivalent to
value iteration and if $\ell_k = \infty$, then optimistic policy iteration is
equal to policy iteration. 

In the remainder of this section, we state the modifications of the main
results to establish the convergence bounds for optimistic policy iteration. 

::: highlight :::
Prop. #prop:multi-step

:   For any $V \in \reals^n$ and $m \in \reals$ 

    * If $V + m \ge \mathcal B V = \mathcal B_g V$, then for any $\ell \in \integers_{> 0}$, 
      $$ \mathcal B V + \frac{\beta}{1 - \beta} m \ge \mathcal B_g^\ell V $$
      and
      $$ \mathcal B_g^\ell V + \beta^\ell m \ge \mathcal B( \mathcal B_g^\ell V). $$

:::

The proof is left as an exercise.

::: highlight :::
Prop. #prop:optimistic-bound

:   Let $\{(V_k, g_k)\}_{k \ge 0}$ be generated as per the optimistic policy
    iteration algorithm. Define
    $$ \alpha_k = \begin{cases}
      1, & \text{if } k = 0 \\
      \beta^{\ell_0 + \ell_1 + \dots + \ell_{k-1}}, & \text{if } k > 0.
     \end{cases}$$
     Suppose there exists an $m \in \reals$ such that 
     $$ \| V_0 - \mathcal B V_0 \| \le m. $$
     Then, for all $k \ge 0$,
     $$ \mathcal B V_{k+1} - \alpha_{k+1} m \le V_{k+1} \le 
     \mathcal B V_k + \frac{\beta}{1-\beta} \alpha_k m.
     $$
     Moreover,
     $$ V_{k} - \frac{(k+1) \beta^k}{1-\beta} m \le
        V^* \le 
        V_k + \frac{\alpha_k}{1 - \beta} m \le 
        V_k + \frac{\beta^k}{1 - \beta} m. $$

::: 

## Exercises

1. Show that the error bound for value iteration is monotone with the number
   of iterations, i.e,
   $$ V_k + \underline δ_k \le V_{k+1} + \underline δ_{k+1} 
   \le V^* 
   \le V_{k+1} + \bar δ_{k+1} \le V_k + \bar δ_k. $$

2. **One-step look-ahead error bounds.**

   Given any $V \in \reals^n$, let $g$ be such that $\mathcal B V = \mathcal
   B_g V$. Moreover, let $V^*$ denote the unique fixed point of $\mathcal B$
   and $V_g$ denote the unique fixed point of $\mathcal B_g$.
   Then, show that

   a. $$ \| V^* - V \| \le \frac{1}{1-\beta} \| \mathcal B V - V \|. $$
   b. $$ \| V^* - \mathcal B V \| \le \frac{\beta}{1-\beta} \| \mathcal B V - V \|. $$
   c. $$ \| V_g - V \| \le \frac{1}{1-\beta} \| \mathcal B_g V - V \|. $$
   d. $$ \| V_g - \mathcal B_g V \| \le \frac{\beta}{1-\beta} \| \mathcal B_g V - V \|. $$
   e. $$ \| V_g - V^* \| \le \frac{2}{1-\beta} \| \mathcal B V - V \|. $$
   f. $$ \| V_g - V^* \| \le \frac{2\beta}{1 - \beta} \| V - V^* \|. $$

## References

The material included here is referenced from different sources. Perhaps the
best sources to study this material are the books by @Puterman2014,
@Whittle1982, and @Bertsekas:book.
