---
title: Approximate dynamic programming
weight: 10
categories:
  - MDP
tags:
  - infinite horizon
  - discounted cost
  - value iteration
  - policy iteration
  - ADP
---

The value and policy iteration algorithms for discounted cost MDPs rely on
exact computation of the Bellman update $W = \mathcal B V$ and the
corresponding optimal policy $g$ such that $\mathcal B V = \mathcal B_g V$.
Suppose we cannot compute these updates exactly, but can find approximate
solutions $W$ and $g$ such that
$$ \NORM{W - \mathcal B V} \le δ
\quad\text{and}\quad
   \NORM{\mathcal B_g V - \mathcal B V} \le ε$$
where $δ$ and $ε$ are positive constants. (In general, these constants are
unknown, so the results are quantitative in nature). 

The error $δ$ may be non-zero due to state aggregation in large state spaces,
or using simulation to compute the Bellman update, or using least square fit
to approximate the value function. 

The error $ε$ may be non-zero due to large or infinite action space. 

Often $δ > 0$ and $ε = 0$. We may also first pick a policy $g$ such that 
$\NORM{\mathcal B_g V - \mathcal B V} \le ε$ and then set $W = \mathcal B_g
V$, in which case $δ = ε$. 

## Approximate value iteration

::: highlight :::
Theorem #

:   Generate $\{V_k\}_{k \ge 0}$ and $\{g_k\}_{k \ge 0}$ such that
    $$\NORM{V_{k+1} - \mathcal B V_k} \le δ 
    \quad\text{and}\quad
      \NORM{\mathcal B_{g_k} V_k - \mathcal B V_k} \le ε. $$
    Then,

    1.  $\displaystyle \lim_{k \to ∞} \NORM{V_k - V_k^*} \le
         \frac {δ}{(1-β)}.$
    2.  $\displaystyle \lim_{k \to ∞} \NORM{V_{g_k} - V^*} \le
         \frac {ε}{(1-β)} + \frac{2βε}{(1-β)^2}.$
:::

If we use a periodic policy with period $M$, then the above bound can be
improved by a factor of $1/(1-β)$. 

#### Proof
To prove the first part, note that repeatedly combining the [contraction
property of the Bellman operator](../discounted-mdp#prop:contraction) with the fact that $\NORM{V_{k+1} - \mathcal
B V_k} \le δ$, we get that
$$ \NORM{\mathcal B^m V_{k+1} - \mathcal B^{m+1} V_k} \le β^m δ. $$ {#eq:B1}

Now, from the triangle inequality, we have that
$$\begin{align}
\NORM{V_k - \mathcal B^k V_0} &\le
  \NORM{V_k - \mathcal B V_{k-1}} + \NORM{V_{k-1} - \mathcal B^2 V_{k-2}}
  + \cdots + \NORM{B^{k-1} V_1 - \mathcal B^k V_0} \\
  &\stackrel{(a)}\le δ + βδ + \dots + β^{k-1}δ \\
  &= \left(\frac{1 - β^k}{1-β}\right) δ,
\end{align}$$
where $(a)$ follows from (@eq:B1). Taking the limit as $k \to ∞$ gives the
first result.

Now, to prove the second part, we again apply the triangle inequality
$$\begin{align}
  \NORM{\mathcal B_{g_k} V^* - V^*} &\le
  \NORM{\mathcal B_{g_k} V^* - \mathcal B_{g_k} V_k} + 
  \NORM{\mathcal B_{g_k} V_k - \mathcal B V_k} + 
  \NORM{\mathcal B V_k - V^*} \\
  &\stackrel{(b)}\le β \NORM{V^* - V_k} + ε + β \NORM{V_k - V^*} \\
  &\stackrel{(c)}\le ε + \frac{2βδ}{(1-β)} =: m,
\end{align}$$
where the first term in $(b)$ uses the contraction property, the second term
uses the fact that $g_k$ is an $ε$-optimal policy and the third term uses the
fact that $V^*$ is the fixed point of $\mathcal B$ and thus $V^* = \mathcal B
V^*$ and then uses the contraction property. The inequality in $(c)$ use the
result from the first part. 

Now, from [the discounting property of the Bellman
operator](../discounted-mdp#prop:bounds), $\NORM{\mathcal B_{g_k} V^* - V^*}
\le m$ implies
$$ \NORM{V_{g_k} - V^*} \le \frac{m}{(1-β)}$$
which proves the second part.

## Approximate policy iteration

Before stating the approximate policy iteration algorithm, we state a
preliminary result that serves as the main step in proving the error bounds
for approximate policy iteration.

::: highlight :::
Proposition #prop:prelim

:   Suppose $V$, $g$, and $h$ satisfy
    $$ \NORM{V - V_g} \le δ
       \quad\text{and}\quad
       \NORM{\mathcal B_h V - \mathcal B V} \le ε.
    $$
    Then,
    $$ \NORM{V_h - V^*} \le β \NORM{V_g - V^*} + \frac{ε + 2βδ}{(1-β)}. $$
:::

#### Proof
From the bounds on $V$, $g$, and $h$ and the [discounting property of the
Bellman operator](../discounted-mdp#prop:properties), we have that
$$\mathcal B_h V_g \le \mathcal B_h V + βδ \le \mathcal B V + ε + β δ.$$ {#eq:P1}

Again, from the bounds on $V$ and $g$ and the discounting property of the
Bellman operator, we have $\mathcal B V \le \mathcal B V_g + βδ$. Thus,
$$\mathcal B_h V_g \le \mathcal B V_g + \underbrace{ε + 2βδ}_{:=m}.$$ {#eq:P2}

Moreover, from the definition of the Bellman operator
$$ \mathcal B V_g \le \mathcal B_g V_g = V_g.$$
Substituting the above in (@eq:P2), we get that
$$ \mathcal B_h V_g \le V_g + m. $$
Therefore, by the discounting property of Bellman operator, we get
$$ V_h \le V_g + \frac{m}{(1-β)}. $$ {#eq:P3}

Using (@eq:P3) and the discounting property, we get that
$$V_h = \mathcal B_h V_h = \mathcal B_h V_g + \big( \mathcal B_h V_h -
\mathcal B_h V_g \big) \le \mathcal B_h V_g + β \frac{m}{(1-β)}. $$

Subtracting $V^*$ from both sides we get
$$\begin{align}
V_h - V^* &\le \mathcal B_h V_g - V^* + \frac{mβ}{(1-β)} \\
&\stackrel{(a)}\le \mathcal B V_g + m - V^* + \frac{mβ}{(1-β)} \\
&\stackrel{(b)}= \mathcal B V_g - \mathcal B V^* + \frac{m}{(1-β)} \\
&\stackrel{(c)}{\le} β \NORM{V_g - V^*} + \frac{m}{(1-β)},
\end{align}$$
where $(a)$ follows from (@eq:P1), $(b)$ uses the fact that $V^*$ is the fixed
point of $\mathcal B$ and $(c)$ uses the contraction property. Substituting
the value of $m$ in the above equation gives the result.

::: highlight :::
Theorem #

:   Generate a sequence $\{g_k\}_{k \ge 0}$ and $\{V_k\}_{k \ge 0}$ such that
    $$ \NORM{V_k - V_{g_k}} \le δ
    \quad\text{and}\quad
    \NORM{\mathcal B_{g_k} V_k - \mathcal B V_k} \le ε.$$
    Then,
    $$ \limsup_{k\to ∞} \NORM{V_{g_k} - V^*} \le
       \frac{ε+2βδ}{(1-β)^2}. $$
:::

Remark

:   *   Both approximate VI and approximate PI have similar error bounds
        (proportional to $1/(1-β)^2$.)
    *   When $ε = δ = 0$, then @prop:prelim implies that $\NORM{V_{g_{k+1}} -
        V^*} \le β \NORM{V_{g_k} - V^*}$. Thus, standard policy iteration has
        a geometeric rate of convergence (similar to value iteration), though
        in practice it converges much faster. 

#### Proof

From @prop:prelim, we have

$$ \NORM{V_{g_{k+1}} - V^*} \le β \NORM{V_{g_k} - V^*} + \frac{ε + 2βδ}{(1-β)}.$$

The result follows from taking the limit $k \to ∞$. 

::: highlight :::
Proposition #prop:convergence

:   If the successive policies in approximate policy iteration converge[^1],
    i.e. 
    $$ g_{k+1} = g_k = g, 
       \quad \text{for some $k$}. $$
    Then,
    $$ \NORM{V_g - V^*} \le \frac{ε + 2βδ}{(1-β)}. $$
:::

[^1]: In general the policies may not converge.

#### Proof

Let $V$ be the approximate value function obtained at iteration $k$, i.e., 
$$\NORM{V - V_g} \le δ 
  \quad\text{and}\quad
  \NORM{\mathcal B_g V - \mathcal V} \le ε.$$

Then, from the triangle inequality, we have
$$\begin{align}
  \NORM{\mathcal B V_g - V_g } &\le
  \NORM{\mathcal B V_g - \mathcal B V} + 
  \NORM{\mathcal B V - \mathcal B_g V} +
  \NORM{\mathcal B_g V - \mathcal B_g V_g} \\
  &\stackrel{(a)}\le
  β\NORM{V_g - V} + ε + β \NORM{V - V_g} \\
  &\stackrel{(b)}\le
  ε + 2βδ,
\end{align}$$
where $(a)$ follows from the fact that $V_g = \mathcal B_g V_g$ and the
contraction property and $(b)$ follows from the assumption on $V$. Now, from
the discounting property, we get the result. 

::: highlight :::
Proposition #prop:approx

:   Suppose the successive value functions obtained by approximate policy
    iteration are "not too different", i.e.,
    $$ \NORM{V - V_g} \le δ, \quad
       \NORM{B_h V - \mathcal B V} \le ε,
       \quad\text{and}\quad
       \NORM{B_g V - \mathcal B_h V} \le ζ.$$
    Then,
    $$ \NORM{V_g - V^*} \le \frac{ε + ζ + 2βδ}{(1-β)}. $$
:::

#### Proof

The result follows by replacing $ε$ in $(a)$ above by $ε+ζ$.

## References

The results presented in this section are taken from @Bertsekas2013.
