---
title: Assignment 7 (solution)
---

1. A person is offered $N$ free plays to be distributed as he pleases between
   two slot machines $A$ and $B$. Machine $A$ pays $\alpha$ dollars with known
   probability $s$ and nothing with probability $(1-s)$. Machine $B$ pays
   $\beta$ dollars with probability $p$ and nothing with probability $(1-p)$.
   The person does not know $p$ but has an a priori probability distribution
   function $f(p)$ for $p$. The problem is to find a playing policy that
   maximizes expected profit.

     Suppose at the $t$-th time step, the person has played machine $B$, $m + n$
     times, where $m$ denotes the number of successes and $n$ the number of
     failures. Let $\xi(m,n)$ denote the probability that the next play of
     machine $B$ will yield a success, that is,
     $$\xi(m,n) = \frac {\int_{0}^1 p^{m+1} (1 - p)^n f(p) dp }
                        {\int_{0}^1 p^m     (1 - p)^n f(p) dp }$$
     Note that $\xi(m,n)$ can be precomputed for all values of $m$ and $n$. 

     a. Show that $(m,n)$ is an information state for the above problem. 
     b. Write down a dynamic program in terms of $(m,n)$. 
     c. Find an optimal strategy for $T = 6$, $\alpha = \beta = 1$, $s = 0.6$,
        $f(p) = 1$ for $0 \le p \le 1$. (For each time, you need to identify the
        pairs $(m,n)$ where machine $B$ should be playerd.)

1. **Part a.** Let $Y_t \in \{0, 1\}$ denote the observation of the player
   where $Y_t = 0$ means that the output was a failure and $Y_t = 1$ means
   that the output was a success.  Let $I_t = \{Y_{1:t-1}, U_{1:t-1} \}$
   denote all the information available to the player. 
  
   From the definition of $(M_t, N_t)$, the evolution of $(M_t, N_t)$ can be
   written as
   $$ (M_{t+1}, N_{t+1}) = 
   \begin{cases}
      (M_t, N_t), & U_t = A \\
      (M_t + 1, N_t), & U_t = B, Y_t = 1 \\
      (M_t , N_t + 1), & U_t = B, Y_t = 0 
   \end{cases}$$

   Furthermore, the probability of next observation is given as
   $$\PR(Y_t = 1 | I_t, U_t) = 
   \begin{cases}
      s, & U_t = A \\
      ξ(M_t, N_t), & U_t = B 
   \end{cases}$$

   Combing the two, we can write
   $$\begin{align}
   \PR( (M_{t+1}, N_{t+1}) | I_t, U_t) &=
   \begin{cases}
      1, & \hbox{if } U_t = A \hbox{ and } (M_{t+1}, N_{t+1}) = (M_t, N_t) \\
      ξ(M_t, N_t), & \hbox{if } U_t = B \hbox{ and } (M_{t+1}, N_{t+1}) = (M_t
      + 1, N_t) \\
      1 - ξ(M_t, N_t), & \hbox{if } U_t = B \hbox{ and } (M_{t+1}, N_{t+1}) =
      (M_t, N_t + 1) 
   \end{cases} \\
   &= 
   \PR( (M_{t+1}, N_{t+1}) | (M_t, N_t), U_t)
   \end{align}$$
   Thus, $(M_t, N_t)$ evolves in a controlled Markovian manner. 

   Now, let's consider the per-step reward. 
   $$R_t(Y_t, U_t) = 
   \begin{cases}
      α, Y_t = 1, U_t = A \\
      β, Y_t = 1, U_t = B 
   \end{cases}$$

   Thus, 
   $$\begin{align}
   \EXP[ R_t(Y_t, U_t) | I_t, U_t ] &=
   \begin{cases}
      α s, & \hbox{if } U_t = A \\
      β ξ(M_t, N_t), & \hbox{if } U_t = B
   \end{cases} \\
   &= \EXP[ R_t(Y_t, U_t) | (M_t, N_t), U_t ].
   \end{align}$$
   Thus, $(M_t, N_t)$ is a sufficient statistic for evaluating current reward.

   Therefore, $(M_t, N_t)$ satisfies the two properties of information states.
   Hence, it is an information state. 

   **Part b.** Based on the properties of the information state, we can write
   the dynamic program as follows.

   $$V_{T+1}(m,n) = 0 $$
   and for $t \in \{T, \dots, 1\}$, we have
   $$\begin{align}
      Q_t(m,n,A) &= α s + V_{t+1}(m,n) \\
      Q_t(m,n,B) &= β ξ(m,n) + ξ(m,n) V_{t+1}(m+1, n) + (1 - ξ(m,n)) 
      V_{t+1}(m, n+1) \\
      V_t(m, n) &= \min\{ Q_t(m,n,A), Q_t(m,n,B) \}.
   \end{align}$$

   **Part c.** First note that 
   $$\int_0^1 p^m (1-p)^n dp = B(m+1, n+1)$$
   where $B(m,n)$ is the [Beta function] which is given by 
   $$B(m,n) = \frac{Γ(m) Γ(n)}{Γ(m+n)}$$
   where $Γ(\cdot)$ is the [Gamma function]. Thus, for integer valued $m$ and
   $n$, $Γ(m) = (m-1)!$ and $Γ(n) = (n-1)!$. Hence,
   $$\int_0^1 p^m (1-p)^n dp = \frac{m! n!}{(m+n+1)!}$$.
   Therefore, we have
   $$\begin{align}
     ξ(m,n) &= \frac{ (m+1)! n! }{ (m+n+1)! } 
     \frac{ (m+n)!}{ m! n!} \\
     &= \frac{ m + 1 } { m + n +1 }.
   \end{align}$$

   See [this Jupyter notebook](../07-2/) for the code for numerically solving
   the dynamic program. 

[Beta function]: https://www.statlect.com/mathematical-tools/beta-function
[Gamma function]: https://www.statlect.com/mathematical-tools/gamma-function

   

   

2. Consider the following modification of the sequential hypothesis testing
   problem considered in class. As in the model discussed in class, there are
   two hypothesis $h_0$ and $h_1$. The a priori probability that the hypothesis
   is $h_0$ is $p$.

      In contrast to the model discussed in class, there are $N$ sensors. If
      sensor $m$ is used at time $t$ and the underlying hypothesis is $h_i$,
      then the observation $Y_t$ is distrubted according to pdf (or pmf)
      $f^m_j(y)$. The cost of using sensor $m$ is $c_m$. 

      Whenever the decision maker takes a measurement, he picks a sensor $m$
      uniformly at random from $\{1, \dots, N\}$ and observes $Y_t$ according to
      the distribution $f^m_j(\cdot)$ and incurs a cost $c_m$. 

      The system continues for a finite time $T$. At each time $t < T$, the
      decision maker has three options: stop and declare $h_0$, stop and declare
      $h_1$, or continue to take another measurement. At time $T$, the continue
      alternative is unavailable.

      a. Formulate the above problem as a POMDP. Identify an information state
         and write the dynamic programming decomposition for the problem.

      b. Show that the optimal control law has a threshold property, similar to
         the threshold propertly for the model considered in class.

