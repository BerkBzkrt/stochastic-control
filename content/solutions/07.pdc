---
title: Assignment 7 (solution)
---

1. **Part a.** Let $Y_t \in \{0, 1\}$ denote the observation of the player
   where $Y_t = 0$ means that the output was a failure and $Y_t = 1$ means
   that the output was a success.  Let $I_t = \{Y_{1:t-1}, U_{1:t-1} \}$
   denote all the information available to the player. 
  
   From the definition of $(M_t, N_t)$, the evolution of $(M_t, N_t)$ can be
   written as
   $$ (M_{t+1}, N_{t+1}) = 
   \begin{cases}
      (M_t, N_t), & U_t = A \\
      (M_t + 1, N_t), & U_t = B, Y_t = 1 \\
      (M_t , N_t + 1), & U_t = B, Y_t = 0 
   \end{cases}$$

   Furthermore, the probability of next observation is given as
   $$\PR(Y_t = 1 | I_t, U_t) = 
   \begin{cases}
      s, & U_t = A \\
      ξ(M_t, N_t), & U_t = B 
   \end{cases}$$

   Combing the two, we can write
   $$\begin{align}
   \PR( (M_{t+1}, N_{t+1}) | I_t, U_t) &=
   \begin{cases}
      1, & \hbox{if } U_t = A \hbox{ and } (M_{t+1}, N_{t+1}) = (M_t, N_t) \\
      ξ(M_t, N_t), & \hbox{if } U_t = B \hbox{ and } (M_{t+1}, N_{t+1}) = (M_t
      + 1, N_t) \\
      1 - ξ(M_t, N_t), & \hbox{if } U_t = B \hbox{ and } (M_{t+1}, N_{t+1}) =
      (M_t, N_t + 1) 
   \end{cases} \\
   &= 
   \PR( (M_{t+1}, N_{t+1}) | (M_t, N_t), U_t)
   \end{align}$$
   Thus, $(M_t, N_t)$ evolves in a controlled Markovian manner. 

   Now, let's consider the per-step reward. 
   $$R_t(Y_t, U_t) = 
   \begin{cases}
      α, Y_t = 1, U_t = A \\
      β, Y_t = 1, U_t = B 
   \end{cases}$$

   Thus, 
   $$\begin{align}
   \EXP[ R_t(Y_t, U_t) | I_t, U_t ] &=
   \begin{cases}
      α s, & \hbox{if } U_t = A \\
      β ξ(M_t, N_t), & \hbox{if } U_t = B
   \end{cases} \\
   &= \EXP[ R_t(Y_t, U_t) | (M_t, N_t), U_t ].
   \end{align}$$
   Thus, $(M_t, N_t)$ is a sufficient statistic for evaluating current reward.

   Therefore, $(M_t, N_t)$ satisfies the two properties of information states.
   Hence, it is an information state. 

   **Part b.** Based on the properties of the information state, we can write
   the dynamic program as follows.

   $$V_{T+1}(m,n) = 0 $$
   and for $t \in \{T, \dots, 1\}$, we have
   $$\begin{align}
      Q_t(m,n,A) &= α s + V_{t+1}(m,n) \\
      Q_t(m,n,B) &= β ξ(m,n) + ξ(m,n) V_{t+1}(m+1, n) + (1 - ξ(m,n)) 
      V_{t+1}(m, n+1) \\
      V_t(m, n) &= \max\{ Q_t(m,n,A), Q_t(m,n,B) \}.
   \end{align}$$

   **Part c.** First note that 
   $$\int_0^1 p^m (1-p)^n dp = B(m+1, n+1)$$
   where $B(m,n)$ is the [Beta function] which is given by 
   $$B(m,n) = \frac{Γ(m) Γ(n)}{Γ(m+n)}$$
   where $Γ(\cdot)$ is the [Gamma function]. Thus, for integer valued $m$ and
   $n$, $Γ(m) = (m-1)!$ and $Γ(n) = (n-1)!$. Hence,
   $$\int_0^1 p^m (1-p)^n dp = \frac{m! n!}{(m+n+1)!}$$.
   Therefore, we have
   $$\begin{align}
     ξ(m,n) &= \frac{ (m+1)! n! }{ (m+n+2)! } 
     \frac{ (m+n+1)!}{ m! n!} \\
     &= \frac{ m + 1 } { m + n +2 }.
   \end{align}$$

   See [this Jupyter notebook](../07-2/) for the code for numerically solving
   the dynamic program. 

[Beta function]: https://www.statlect.com/mathematical-tools/beta-function
[Gamma function]: https://www.statlect.com/mathematical-tools/gamma-function

