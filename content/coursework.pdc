---
type: coursework
---

Project

:   
    *  In the revised syllabus, the course project must be done individually.
       Each student must select a project topic by April 10, and the project
       report due on April 30, 2020.

    * The report must not be any longer than 10-15 pages. The grade
      distribution for different parts of the report are as follows. 

      * Background, presentation of the problem setting, and literature overview: 20%
      * Summary of the results, proof outlines, and examples to illustrate the main results: 50%
      * Critical evaluation of the contributions: 20%
      * Clarity of the presentation: 10%

    * The potential project topics are listed below. You can also pick up on
      the discussion of the literature in the reference section of the notes.
      You are also free to pick topics outside of these topics. Please send me
      an email with the topic that you want to work on, and we can finalize
      which papers should be covered in the term project. 


----

# Selected Projects

* Alex: Stochastic dual dynamic programming
* Alper: "Thrify" and "Equalizing" policies: @Karatzas2010.
* Armo: Adaptive control
* Erfan: Options
* Fatih: Finite state approximations of MDPs
* Feras: Witsenhausen's counterexample
* Imen: Average cost MDPs based on @Bertsekas2013; @Puterman2014
* Imene: Reinforcement learning
* Jilan: Numerical algorithms for solving POMDPs
* Mohammad: Remote estimation 
* Sadia: Reinforcement learning
* Yacine: Multi-armed bandit

------

# Potential Project topics

This is a partial list of topics. If you have a specific topic or direction in
mind, please feel free to discuss that with me. 

##  Interesting papers

These are a biased collection of interesting papers and could form the basis
of a term project. For some of these, you might have to combine them with
other follow up work to have substantial material for a term project.

* An interesting characterization of optimal policies (for general decision
  problems) is that they must be "thrifty" and "equalizing". An interpretation
  of this property for MDPs is presented in @Karatzas2010.

* In the class, we primarily focussed on structural properties for finite
  horizon models. An obvious next question is---when do these properties hold
  for infinite horizon models. An answer to this question in broad generality
  is provided in @Smith2002. They show that many of these results can be
  viewed as properties of closed convex cones. 

* 

##  Average cost MDPs

This is a topic which I usually cover in the course, but could not cover it
this term because of the lost time. A basic treatment of average cost MDPs
with finite state and finite action is covered in all standard books:
@KumarVaraiya1986, @Puterman2014, @Bertsekas:book. An accessible treatment for
countable or uncountable state spaces is presented in @Sennott:book. A
detailed survey of many of the technical results for average cost models over
general state spaces is in @Arapostathis1993.

* A potential term project on this topic could cover the derivation of the
  average cost dynamic program under the weak accessibility conditions and a
  discussion of the value iteration, policy iteration, and linear programming
  algorithms to solve average cost MDPs. 

* An interesting connection with discounted cost models is the vanishing
  discount approach. This is not deep enough to be a term project on its own,
  but it can either be part of a broader project or connect with Tauberian
  theorems in analysis.

* Another potential topic for a term project related to this topic is
  approximation bounds for average cost problems. There is some material on
  approximate dynamic programming for average cost MDPs in @Bertsekas2013.
  Some recent results on error bounds for discretization of average cost MDPs
  are in Saldi, Linder, Yuksel (add citation). 

## Multi-armed bandits

Multi-armed bandits[^MAB] are a special class of resouce allocation problems which
have a low complexity solution. In general, scheduling problems can be modeled
as MDPs, but the state space of such problems in exponential in the number of
alternatives. This is sometimes referred to as the _curse of dimensionality_.
A conceptual breakthrough in such problem was achieved when @Gittins1974;
@Gittins1979 showed that the a low complexity solution exists. 

* A potential term project on this topic is to review the basic result 
  of @Gittins1979. The proof in @Gittins1979 uses interchange argument, which we
  did not cover in the course. One option is to overview the method of
  interchange arguments, and then explain Gittins result using that. Another
  alternative is to present the dynamic programming proof of Gittins result,
  which was given by @Whittle1980. Since then multiple proofs of Gittins
  result have been presented in the literature. A survey of the four most
  popular results is presented in @Frostig2016.

* Another potential term project is to present the restart-in-state
  formulation of the multi-armed problem presented in @Katehakis1987. An
  interesting generalization of this idea for multi-armed bandits with
  commitments is presented in @Cowan2015.

* The result of @Gittins1979 is derived under the assumption that the
  alternatives that are not selected do not evolve. Models where this
  assumption is violated are called restless bandits [@Whittle1988]. Another
  potential term project is to present an overview of @Gittins1979,
  @Whittle1988, and other generalizations of multi-armed bandits. See
  @Mahajan2008 for an overview. 

[^MAB]: There are two classes of models that go under the heading of
multi-armed bandits. One is the class of models that I have described here.
The second are a different class of models which build on the work of
@LaiRobbins1985, which derived regret bounds for such models. These days the
latter class of models are more popular, primarily because they have been used
extensively in algorithms which display online advertisements. 

## Monotonicity of optimal policies

When we studied monotonicity of optimal policies in class, we assumed that the
state and action space were totally ordered. Some of these results generalize
to models where the state and action spaces are not totally ordered. An
interesting example of such a result is presented in @Papadaki2007. General
conditions for the value function and optimal policy to be monotone are
derived in @Topkis1998. 

* One potential term project on this topic is to review the the lattice
  structure of partially ordered sets and explain how the monotonicity results
  derived in class can be extended to such sets following the exposition 
  in @Topkis1998. 

* Another potential term project is to review monotonicity results for
  queueing models. There is a rich history on this topic but perhaps the most
  comprehensive survey is by @Koole2006. 

* Another potential term project is to present the generalized monotonicity
  results for optimal stopping. For example @Oh2016 presents sufficient
  conditions under which the optimal policy has a control-band type structure.
  Oh's PhD thesis also has interesting results and examples on this topic.

## Sensitivity-based analysis and event-based optimization of MDPs

For infinite horizon MDPs, an alternative approach to value iteration and
policy iteration methods studied in class is to investigate the sensitivity of
the average performance to the parameters of the Markov policy. Such an
approach is called perturbation analysis in the MDP literature and
policy-gradient approach in the RL literature. The sensitivity based view of
MDPs can be used to derive an event-based approach to MDPs. The idea of the
event based approach is to optimize based on "events" rather than on states. 

* One potential term project is to review the results on event-based
  optimization following the suvery paper @Cao2005 or specific chapters 
  from @Cao2007.

* Another potential term project is to present a specific applications of
  event-based optimization. Look at the references in @Cao2005; @Cao2007 for
  examples. 

* Another option is to review these ideas as used in the RL literature. For
  senstivity-based analysis (which is known as policy-gradient methods), see
  the recent survey by @Agarwal2019.

## Constrained MDPs.

We briefly talked about constrained MDPs in the notes on [linear programming
formulation](../inf-mdp/linear-programming). Potential term projects on this
topic include:

* A detailed discussion of the linear programming formulation of constrained
  MDPs, following the presentation of @Altman1999.

* A discussion of the convex analytic approach of @Borkar1988. An accessible
  introduction to this approach is presented in @Borkar2002. The main idea of
  this approach is to think in terms of the occupancy measure of MDPs and
  cast the optimization problem as a convex programming problem in the space
  of probability measures. 

## Stochastic dual dynamic programming (SDDP)

SDDP uses ideas from convex optimization to efficiently solve MDPs over Euclean
state space. The method originated in @Pereira1991 and has been extensively
used in the energy optimization sector. A potential term project on this topic
is to review the results of @Shapiro2011. 

## Robust MDPs

Distributionally robust MDPs.

## Numerical methods to efficiently solve POMDPs 

See the remark at the end of the notes on [POMDPs](../pomdp/pomdp)

## Risk sensitive POMDPs

Baras et al., Fernandez et al. 

## Adaptive control of linear systems

See the chapter in @KumarVaraiya1986 for references.

## Adaptive control of Markov chains.

See the chapter in @KumarVaraiya1986 for references.

## Reinforcement learning

Value based methods (Q-learning, linear function approximation), policy-based
method (policy gradient methods, discussed above). Options (related to
event-based optimization).

## Applications of MDPs

You could pick any topic on application of MDPs. Here I list some of the
references which survey applications in different areas:

* Communication: @Altman2002
* Water Reservoir Management: @Lamond2002

## Applications of POMDPs

## Applications of decentralized control systems.

* Real-time communication: Witsenhausen, Walrand Varaiya, Teneketzis, Mahajan
  and Teneketzis.

* Networked control systems: Walrand Varaiya, Mahajan and Teneketzis

* Remote estimation: Lipsa and Martins, Charkavorty and Mahajan, Nayyar et al.

* Mean-field teams: Arabneydi and Mahajan.



---

# References {-}

