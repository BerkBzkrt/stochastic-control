---
title: Optimal Gambling
weight: 01
---

Imagine  a gambler who goes to a casino with an initial fortune of $x_1$
dollars and places bets over time and must leave after $T$ bets. Let $X_t$
denote the gambler's fortune after $t$ bets. In this example, time denotes the
number of times that the gambler has bet.

At time $t$, the gambler may place a bet for any amount $U_t$ less that his
current fortune $X_t$. If he wins the bet (denoted by the even $W_t = 1$), the
casino gives him the amount that he had bet. If he loses the bet (denoted by
the event $W_t = -1$), he pays the casino the amount that he had bet. Thus,
the dynamics can be written as 

$$ X_{t+1} = X_t + W_t U_t. $$

The outcomes of the bets $\{W_t\}_{t \ge 1}$ are _primitive random variables_,
i.e., they are independent of each other, the gambler's initial fortune, and
his betting strategy. 

The gambler's utility is $\log X_T$, the logarithm of his final fortune. Thus,
the reward function may be written as

$$ r_t(x, u) = 0 
   \quad \text{and} \quad
   r_T(x) = \log x.
$$

Find the strategy that maximizes the gambler's utility, $\EXP[\log X_T]$. 

## Optimal gambling strategy and value functions

The above model of optimal gambling is a Markov decision process. Therefore,
the optimal solution is given by dynamic programming. 

<div class="highlight">
Dynamic program

:   Define the following value function $V_t \colon \reals_{\ge 0} \to \reals$
as follows:
    $$ V_T(x) = \log x $$
    and for $t \in \{T-1, \dots, 1\}$:
    $$ \begin{align}
      Q_t(x,u) &= \EXP[ r_t(x,u) + V_{t+1}(X_{t+1}) \,|\, X_t = x, U_t = u] \\
      &= p V_{t+1}(x+u) + (1-p) V_{t+1}(x-u),
      \end{align}
    $$
    and
    $$ \begin{align}
      V_t(x) &=  \max_{u \in [0, x]} Q_t(x,u), \\
      g_t(x) &= \arg \max_{u \in [0, x]} Q_t(x,u). \\
      \end{align}
    $$

    Then the strategy $g = (g_1, \dots, g_T)$ is optimal.

</div>

The above model is one of the rare instances when the optimal strategy and
value function can be identified in closed form. 

<div class="highlight">
Theorem 

:   When $p \le 0.5$:

    - the optimal strategy is to _not gamble_, specifically $g_t(x) = 0$;
    - the value function is $V_t(x) = \log x$. 

    When $p > 0.5$:

    - the optimal strategy is _to bet a fraction of the current fortune_,
      specifically $g_t(x) = (2p - 1)x$;
    - the value function is $V_t(x) = \log x + (T - t) C$, where[^C]
      $$ C = \log 2 + p \log p + (1-p) \log (1-p).$$

</div>

[^C]: The constant $C$ defined above is equal to the capacity of a binary
  symmetric channel! In fact, the above model was introduced by @Kelly1956 to
  show a gambling interpretation of information rates. 

We prove the two cases separately. 

#### Proof when $p \le 0.5$

Let $p = \PR(W_t = 1)$ and $q = \PR(W_t = -1)$. Then $p \le 0.5$ implies that
$p \le 1 - p = q$. 

We proceed by backward induction. For $t = T$, we have that $V_T(x) = \log x$.
This forms the basis of induction. Now assume that for $t+1$, $V_{t+1}(x) =
\log x$. Now consider

$$ Q_t(x,u) = p V_{t+1}(x+u) + qV_{t+1}(x-u). $$

Differentiating both sides w.r.t. $u$, we get
$$ \begin{align} 
  \frac { \partial Q_t(x,u) } {\partial u} &= 
   \frac p { x + u} - \frac q { x - u } 
   \\
   & = \frac { (p - q) x - (p + q) u } { x^2 - u^2 } 
   \\
   & =
   \frac { - (q - p) x - u } {x^2 - u^2 } 
   \\
   &< 0.
  \end{align}   
$$

This implies that $Q_t(x,u)$ is decreasing in $u$. Therefore,

$$ g_t(x) = \arg\max_{u \in [0, x]} Q_t(x,u) = 0. $$

Moreover,
$$ V_t(x) = Q_t(x, g_t(x)) = \log x.$$

This completes the induction step. 

#### Proof when $p > 0.5$

As in the previous case, let $p = \PR(W_t = 1)$ and $q = \PR(W_t = -1)$. Then
$p > 0.5$ implies that $p > 1 - p = q$. 

We proceed by backward induction. For $t = T$, we have that $V_T(x) = \log x$.
This forms the basis of induction. Now assume that for $t+1$, $V_{t+1}(x) =
\log x + (T -t - 1)C$. Now consider

$$ Q_t(x,u) = p V_{t+1}(x+u) + qV_{t+1}(x-u). $$

Differentiating both sides w.r.t. $u$, we get
$$ \begin{align} 
  \frac { \partial Q_t(x,u) } {\partial u} &= 
   \frac p { x + u} - \frac q { x - u } 
   \\
   & = \frac { (p - q) x - (p + q) u } { x^2 - u^2 } 
   \\
   & =
   \frac { (p - q) x - u } {x^2 - u^2 } 
  \end{align}   
$$

Setting $\partial Q_t(x,u)/\partial u = 0$, we get that the optimal action is

$$ g_t(x) = (p-q) x. $$

Note that $(p-q) \in (0,1)$ 

$$ 
  \frac { \partial^2 Q_t(x,u) } {\partial u^2} = 
   - \frac p { (x + u)^2 } - \frac q { (x - u)^2 } 
  < 0;
$$
hence the above action is indeed the minimizer. 
Moreover,
$$ \begin{align} 
  V_t(x) &= Q_t(x, g_t(x))  \\
  &= p V_{t+1}(x + g_t(x)) + q V_{t+1}( x - g_t(x) )\\
  &= \log x + p \log (1 + (p-q)) + q \log (1 - (p-q)) + (T - t -1)C \\
  &= \log x + p \log 2p + q \log 2q + (T - t + 1)C \\
  &= \log x + (T - t) C
  \end{align}   
$$

This completes the induction step. 

## Generalized model

Suppose that the terminal reward $r_T(x)$ is monotone increasing[^increasing] in $x$.

[^increasing]: I use the convention that _increasing_ means _weakly
  increasing_. The alternative term _non-decreasing_ implicitly assumes that
  we are talking about a totally ordered set. 

<div class="highlight">
Theorem

:   For the generalized optimal gambling problem:

    - For each $t$, the value function $V_t(x)$ is monotone increasing in $x$.
    - For each $x$, the value function $V_t(x)$ is monotone decreasing in $t$.

</div>

#### Proof of monotonicity in $x$

We proceed by backward induction. $V_T(x) = r_T(x)$ which is monotone
increasing in $x$. Assume that $V_{t+1}(x)$ is increasing in $x$. Now,
consider $V_t(x)$. Consider $x_1, x_2 \in \reals_{\ge 0}$ such that $x_1 \le
x_2$. Then for any $u \le x_1$, we have that

$$ \begin{align}
    Q_t(x_1, u) &= p V_{t+1}(x_1+u) + q V_{t+1}(x_1-u) \\
    & \stackrel{(a)}{\le} p V_{t+1}(x_2 + u) + q V_{t+1}(x_2  - u) \\
    & = Q_t(x_2, u),
  \end{align}
$$
where $(a)$ uses the induction hypothesis. Now consider

$$ \begin{align}
  V_t(x_1) &= \max_{u \in [0, x_1]} Q_t(x_1, u) \\
  & \stackrel{(b)}{\le} \max_{u \in [0, x_1]} Q_t(x_2, u) \\
  & \le \max_{u \in [0, x_2]} Q_t(x_2, u) \\
  &= V_t(x_2),
  \end{align}
$$
where $(b)$ uses monotonicity of $Q_t$ in $x$. This completes the induction
step. 

#### Proof of monotonicity in $t$

This is a simple consequence of the following:

$$V_t(x) = \max_{u \in [0, x]} Q_t(x,u) \ge Q_t(x,0) = V_{t+1}(x).$$ 

## References

The above model was introduced by @Kelly1956. For generalizations, see
@Ross1974.

--- 
