---
title: Assignment 2
marks: 10
due: 28 Jan, 2022
---

1. **Inventory Management**. Consider a variation of the [inventory
   management] problem considered in class. Suppose that the inventory $S_t$
   and the actions $A_t$ takes values in the set $\mathbb S = \{0, 1, \dots,
   L \}$, and the dynamics are given by 
   $$
      S_{t+1} = \bigl[ S_t + A_t - W_t \bigr]_0^L,
   $$
   where $[ s ]_{0}^L$ is a function which clips the values between $0$ and
   $L$, i.e., 
   $$
      [ s ]_0^L =
      \begin{cases}
      0, & \text{if $s < 0$} \\
      s, & \text{if $0 \le s \le L$} \\
      L, & \text{if $s > L$}.
      \end{cases}
   $$
   The demand $W_t$ takes values in the set $\ALPHABET W = \{0, 1, 2\}$ with
   probability mass function $P_W = [ 0.1, 0.7, 0.2 ]$. 

   The per-step cost is given by 
   $$
      c_t(S_t, A_t, W_t) = (S_t + A_t - W_t)^2.
   $$
   Note that in this case, the inventoy cannot be negative but the per-step
   cost does take shortage into account.

   Write the dynamic programming equation and numerically solve it for a
   horizon $T = 10$ and inventory level $L = 5$. Plot the value function and
   the optimal policy for $t=1$ and $t=2$.

[inventory management]: ../../mdp/inventory-management

2.  **Monotonicity in time**. Consider the standard model of finite-horizon
    MDP discussed in class. Suppose that the dynamics is time homogeneous
    and so is the per-step cost $c_t(\cdot)$ 
    (except, possibly at the terminal time $t=T$). Suppose we
    have that $V_{T-1}(s) \le V_T(s)$ for all $s \in \ALPHABET S$. Then, show
    that
    $$ V_{t}(s) \le V_{t+1}(s), \quad
       \text{for all $s \in \ALPHABET S$ and $t$}.$$

    Similarly, if we have that $V_{T-1}(s) \ge V_T(s)$ for all $s \in
    \ALPHABET S$, then
    $$ V_{t}(s) \ge V_{t+1}(s), \quad
       \text{for all $s \in \ALPHABET S$ and $t$}.$$


3.  **Dynamic programming for maximizing tail probabilities.**
    Consider a dynamical system that evolves as follows:
    $$
      S_{t+1} = f_t(S_t, A_t, W_t)
    $$
    where $\{S_1, W_1, \dots, W_T\}$ are independent random variables and the
    control actions $A_t$ are chosen according to a history dependent policy 
    $π = (π_1, \dots, π_T)$:
    $$
      A_t = π_t(S_{1:t}, A_{1:t-1}).
    $$
    Given a sequence of functions $h_t \colon \ALPHABET S \mapsto \reals$, the
    cost of a policy $π$ is given by the probability that $h_t(S_t)$
    exceeds a given threshold $α \in \reals$ at some time, i.e.,
    $$
      J(π) = \PR^{π}\left( \max_{0 \le t \le T} h_t(S_t) \ge α \right).
    $$

    Show that the above cost can be put in an additive form that would enable
    us to use the theory developed in the class to tackle this setup.

