---
title: Lectures
---

Whenever possible, I will post notes on some of the material covered in class,
but that is not guaranteed. This is a graduate class and you are responsible
for taking notes in class and reading the appropriate chapters of the
textbooks.

The notes will be updated as we move along in the course. Please check the
dates on the first page to keep track. If you find any typos/mistakes in the
notes, please let me know. [Pull requests
welcome](https://github.com/adityam/stochastic-control/tree/master/content).

Week 1
:   **Introduction and course overview**.

    * Reading: Kumar and Varaiya (Ch 1, 2); Bertsekas (Ch 1).
    * Introduction to [stochastic optimization](../stochastic/stochastic-optimization)
    * [Newsvendor problem](../stochastic/newsvendor)
    * Introduction to [MDPs][MDP theory] and dynamic programming
    * [Matrix formulation] of MDPs
    * [Assignment 1](../assignments/01).

Week 2
:   **Examples of MDPs**

    * [Optimal gambling]
    * [Inventory management]
    * [Assignment 2](../assignments/02).

Week 3
:   **Proof of optimality of dynamic programming**

    * See notes on [MDP theory]
    * Cost vs reward, cost which depends on the next state, minimax
      optimality, risk senstive control.
    * See notes on [MDP theory]
    * [Assignment 3](../assignment/03).

Week 4
:   **Monotonicty in Markov decision processes**

    * [Stochastic dominance, monotonicity][SD], and submodularity
    * [Sufficient conditions for value function and optimal policy to be
      monotone][monotonicity].
    * Example of [power-delay trade-off] in wireless communication
    * [Assignment 4](../assignment/04).

Week 5
:   **Introduction to infinite horizon discounted problems**

    * [Reward shaping]
    * Infinite horizon [inventory management][inf-inventory]
    * [Sevice Migration in mobile edge computing][MEC]


Week 6
:    **Bellman operators, value iteration, and policy iteration**

     * Monotonicity, contraction, and their implications
     * Value iteration and stopping conditions
     * Policy iteration and convergence guarantees
     * See notes on [infinite horizon MDPs][inf-mdp]

Week 7
:   **Properties of value functions**

    * [Lipschitz continuity of MDPs][Lipschitz]
    * C4 properties

Week 8 
:   **Approximate dynamic programming**

    * [ADP]  

Week 9
:   **Model approximation**

    * Integral probability metrics and comparing MDPs.
    * State quantization (see notes on [state aggregation])
    * State compression (see notes on [state aggregation])

Week 10
:   **POMDPs**

    * Basic model of POMDPs (see notes on [POMDPs])
    * Information state
    * [Sequential hypothesis testing][HT]

Week 11
:   **Approximations for POMDPs**

    * Approximate information states

Week 12
:   **Decentralized control**

    * Decentralized POMDPs / Dynamic team problems
    * Common information approach
    * Delayed sharing and control sharing models

[Optimal gambling]: ../mdp/optimal-gambling
[MDP theory]: ../mdp/mdp-functional
[LQR]: ../linear-systems/lqr
[Inventory management]: ../mdp/inventory-management
[Matrix formulation]: ../mdp/mdp-matrix
[SD]: ../mdp/stochastic-dominance
[monotonicity]: ../mdp/monotonicity
[power-delay trade-off]: ../mdp/power-delay-tradeoff
[optimal choice]: ../optimal-stopping/optimal-choice
[call options]: ../optimal-stopping/call-options
[optimal stopping policies]: ../optimal-stopping/monotonicity-optimal-stopping
[Reward shaping]: ../mdp/reward-shaping
[inf-inventory]: ../inf-mdp/inventory-management
[MEC]: ../inf-mdp/service-migration-in-mec
[Lipschitz]: ../inf-mdp/lipschitz-mdp
[inf-mdp]: ../inf-mdp/discounted-mdp
[state aggregation]: ../inf-mdp/state-aggregation
[POMDPs]: ../pomdp/pomdp
[HT]: ../pomdp/sequential-hypothesis.pdc
[ADP]: ../inf-mdp/adp
