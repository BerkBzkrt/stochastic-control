---
title: "Example: Inventory Management (revisited)"
weight: 02
categories:
  - MDP
tags:
  - inventory management
  - reward shaping
  - structural results
---

TL;DR _One of the potential benefits of modeling a system as infinite horizon
discounted cost MDP is that it can be simpler to identify an optimal policy.
We illustrate this using the inventory management example_.

Consider the model for [inventory management] and assume that it runs for an
infinite horizon. We assume that the per-step cost is given by
$$c(s,a,s_{+}) = p a + γ h(s), $$
where 
$$ h(s) = \begin{cases}
  c_h s, & \text{if $s \ge 0$} \\
  -c_s s, & \text{if $s < 0$},
\end{cases}$$
where $a$ is the per-unit holding cost, $b$ is the per-unit shortage cost, and
$p$ is the per-unit procurement cost. Note that in the per-step cost, we are
assuming that the holding or shortage cost is dicounted because this cost is
incurred at the end of the time period.  

As a first step, we modify the per-step cost using [reward shaping]. In
particular, we consider the following potential function

$$\varphi(s) = h(s) + \frac1{γ} p s - \frac{1}{1-γ}p\mu,$$
where $\mu = \EXP[W]$ is the expected number of arrivals at each time period. 

Now consider a new cost function
$$\begin{align*}
  c'(s,a,s_{+}) &= c(s,a,s_{+}) + \varphi(s) - γ \varphi(s_{+}) \\
  &= pa + γ h(s_{+}) + h(s) + \frac{1}{γ} p s - \frac{1}{1-γ} p \mu
  - γ h(s_{+}) - p s_{+} - \frac{γ}{1-γ} p \mu \\
  &= h(s) + \frac{1-γ}{γ} ps + p w - p \mu.
\end{align*} $$
Note that 
$$ \EXP[ c'(s,a,S_{+}) | S = s, A = a ] = h(s) + \frac{1-γ}{γ} ps 
=: c^*(s). $$
Thus, the optimal policy of the original model is the same as that in which
the per-step cost is given by $c^*(s)$. 

Recall that the optimal policy in the original model was a control limit
policy. For the infinite horizon model, the threshold will become
time-invariant. Thus, the optimal policy will be of the form
$$
  π(s) = \begin{cases}
  s^* - s, & \text{if $s \le s^*$} \\
  0, & \text{otherwise}.
\end{cases}$$

The infinite horizon dynamic programming with this modified cost is given by 
$$ V(s) = \min_{a \in \reals_{\ge 0}} 
   \bigl\{ c^*(s) + γ \EXP[ V(s + a - W) ] \bigr\}. $$

Using the structure of the optimal policy identified above, we have that
$$\begin{equation}\label{eq:opt}
V(s) = c^*(s) + γ \EXP[ V(s^* - W) ], \qquad s \le s^*.
\end{equation}$$
Let $F(s^*)$ denote $\EXP[V(s^*-W)]$. Then, substituting $s = s^* - W$ in 
\\eqref{eq:opt} and taking expectations, we get
$$F(s^*) = \EXP[ c^*(s^* - W) ] + γ F(s^*).$$
Thus, 
$$ F(s^*) = \frac{1}{1-γ} \EXP[ c^*(s^*-W) ]. $$

Substituting the above in \\eqref{eq:opt} for $s = 0$, (note that $c^*(0) =
0$) we get
$$ s^* = \arg\min_{s^* \ge 0} \frac{γ}{1-γ} \EXP[ c^*(s^* - W) ].$$
Consequently, we have the following:

::: highlight :::
Theorem #thm:inventory

: The optimal threshold $s^*$ is given by the value of $s^*$ which minimizes
$\EXP[ c^*(s^*-W) ]$. 

::: 

@thm:inventory provides a almost closed form characterization of
the optimal policy.

---

# Exercises {-}

1. Suppose that the arrival process is exponential with rate $1/\mu$, i.e.,
   the density of $W$ is given by $e^{-s/\mu}/\mu$. Show that the optimal
   threshold is given by
   $$ s^* = \mu \log \left[ \frac{ a + b} { a + p (1-γ)/γ} \right]. $$

   _Hint_: Show that for exponential distribution
   $$ \EXP[ (s^*-W)^+ ] = (s^* - \mu) + \mu e^{-s^*/\mu} $$
   and
   $$ \EXP[ (s^*-W)^- ] = - \mu e^{-s^*/\mu}. $$

---

# References {-}

The idea of using reward shaping to derive a closed form expression for
inventory management is taken from @Whittle1982. It is interesting to note
that @Whittle1982 uses the idea of reward shaping more than 17 years before
the paper by @Ng1999 on reward shaping. 


[inventory management]: ../../mdp/inventory-management
[reward shaping]: ../../mdp/reward-shaping


