<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>reward shaping on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/tags/reward-shaping/</link>
    <description>Recent content in reward shaping on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://adityam.github.io/stochastic-control/tags/reward-shaping/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Theory: Reward Shaping</title>
      <link>https://adityam.github.io/stochastic-control/mdp/reward-shaping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/reward-shaping/</guid>
      <description>What are the conditions under which two MDPs which have the same dynamics but different cost functions have the same optimal policy? This is an important question in reinforcement learning (where one often shapes the reward function to speed up learning) and inverse reinforcement learning (where one learns the reward function from the behavior of an expert). The following result provides a complete answer to this question.
Let \(M^1\) and \(M^2\) denote two MDPs on the same state space \(\ALPHABET X\) and action space \(\ALPHABET U\).</description>
    </item>
    
  </channel>
</rss>