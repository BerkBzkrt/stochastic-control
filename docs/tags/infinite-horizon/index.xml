<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>infinite horizon on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/tags/infinite-horizon/</link>
    <description>Recent content in infinite horizon on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://adityam.github.io/stochastic-control/tags/infinite-horizon/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Theory: Infinite horizon discounted MDP</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/discounted-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/discounted-mdp/</guid>
      <description>A common way to approximate systems that run for a very large horizon is to assume that they run for an infinite horizon. There is an inherent homogeneity over time for infinite horizon system: the future depends only on the current state and not on the current time. Due to this homogeneity over time, we expect that the optimal policy should also be time-homogeneous. Therefore, the optimal policy for an infinite-horizon system should be easier to implement than the optimal policy for a finite horizon system, especially so when the horizon is large.</description>
    </item>
    
    <item>
      <title>Infinite horizon Linear Quadratic Regulation (LQR)</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/inf-lqr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/inf-lqr/</guid>
      <description>1 Preliminaries We first start with some properties of deterministic time-homogeous linear system: \[\begin{equation} \label{eq:simple} x_{t+1} = A x_{t-1}. \end{equation}\] The solution to this system is given by \(x_t = A^t x_0\). It obviously has an equilibrium point of \(x = 0\). This will be the unique equilibrium point if \(A\) is non-singular, when the only solution of \(x = Ax\) is \(x = 0\). Suppose this is true. One may now ask whether this equilibrium is stable in that \(x_t \to 0\) with increasing \(t\) for any \(x_0\).</description>
    </item>
    
    <item>
      <title>Theory: Linear Programming Formulation and Constrained MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/linear-programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/linear-programming/</guid>
      <description>Note: Throughout this section, we assume that \(\ALPHABET X\) and \(\ALPHABET U\) are finite and \(|\ALPHABET X|= n\) and \(|\ALPHABET U| = m\).
We know that if \(V \le \mathcal B V\) then \(V \le V^* = \mathcal B V^*\). Thus, \(V^*\) is the “largest” \(V\) that satisfies the constraint \(V \le \mathcal B V\). This constraint can be written as a finite system of linear equations: \[\begin{equation} \label{eq:constaints} V(x) \le c(x,u) + β\sum_{y \in \ALPHABET X} P(y|x,u) V(y), \qquad x \in \ALPHABET X, u \in \ALPHABET U.</description>
    </item>
    
    <item>
      <title>Theory: Lipschitz MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/lipschitz-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/lipschitz-mdp/</guid>
      <description>1 Preliminaries 1.1 Lipschitz continuous functions Given two metric spaces \((\ALPHABET X, d_X)\) and \((\ALPHABET Y, d_Y)\), the Lipschitz constant of function \(f \colon \ALPHABET X \to \ALPHABET Y\) is defined by \[ \| f\|_{L} = \sup_{x_1 \neq x_2} \left\{ \frac{ d_Y(f(x_1), f(x_2)) } { d_X(x_1, x_2) } : x_1, x_2 \in \ALPHABET X \right\} \in [0, ∞]. \] The function is called Lipschitz continuous if its Lipschitz constant is finite.</description>
    </item>
    
    <item>
      <title>Theory: State aggregation in MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/state-aggregation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/state-aggregation/</guid>
      <description>1 State quantization Consider an MDP with continuous state space \(\ALPHABET X\) and finite action space \(\ALPHABET U\). We denote this MDP by \(M = (\ALPHABET X, \ALPHABET U, c, p)\), where for simplicity we assume that the \(p\) is the density of the transition kernel.
Since the state space is continuous, in general, we cannot compute the value functions exactly. The simplest way to proceed is to discretize the state space \(\ALPHABET X\).</description>
    </item>
    
    <item>
      <title>Theory: Approximate dynamic programming</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/adp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/adp/</guid>
      <description>The value and policy iteration algorithms for discounted cost MDPs rely on exact computation of the Bellman update \(W = \mathcal B V\) and the corresponding optimal policy \(g\) such that \(\mathcal B V = \mathcal B_g V\). Suppose we cannot compute these updates exactly, but can find approximate solutions \(W\) and \(g\) such that \[ \NORM{W - \mathcal B V} \le δ \quad\text{and}\quad \NORM{\mathcal B_g V - \mathcal B V} \le ε\] where \(δ\) and \(ε\) are positive constants.</description>
    </item>
    
  </channel>
</rss>