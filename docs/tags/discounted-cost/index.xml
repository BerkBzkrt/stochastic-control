<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>discounted cost on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/tags/discounted-cost/</link>
    <description>Recent content in discounted cost on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://adityam.github.io/stochastic-control/tags/discounted-cost/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Infinite horizon discounted MDP</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/discounted-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/discounted-mdp/</guid>
      <description>Note: Throughout this section, we assume that \(\ALPHABET X\) and \(\ALPHABET U\) are finite and \(|\ALPHABET X|= n\) and \(|\ALPHABET U| = m\).
Performance of a time-homogeneous Markov policy For any \(g \colon \ALPHABET X \to \ALPHABET U\), consider the time homogeneous policy \((g, g, \dots)\). For ease of notation, we denote this policy simply by \(g\). The expected discounted cost under this policy is given by \[ V_g(x) = \EXP^g\bigg[ \sum_{t=1}^∞ β^{t-1} c(X_t, U_t) \biggm| X_1 = x \bigg].</description>
    </item>
    
    <item>
      <title>Lipschitz MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/lipschitz-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/lipschitz-mdp/</guid>
      <description>Preliminaries Lipschitz continuous functions Given two metric spaces \((\ALPHABET X, d_X)\) and \((\ALPHABET Y, d_Y)\), the Lipschitz constant of function \(f \colon \ALPHABET X \to \ALPHABET Y\) is defined by \[ \| f\|_{L} = \sup_{x_1 \neq x_2} \left\{ \frac{ d_Y(f(x_1), f(x_2)) } { d_X(x_1, x_2) } : x_1, x_2 \in \ALPHABET X \right\} \in [0, ∞]. \] The function is called Lipschitz continuous if its Lipschitz constant is finite.
Intuitively, a Lipschitz continuous function is limited by how fast it can change.</description>
    </item>
    
    <item>
      <title>State aggregation in MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/state-aggregation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/state-aggregation/</guid>
      <description>State discretization Consider an MDP with continuous state space \(\ALPHABET X\) and finite action space \(\ALPHABET U\). We denote this MDP by \(M = (\ALPHABET X, \ALPHABET U, c, p)\), where for simplicity we assume that the \(p\) is the density of the transition kernel.
Since the state space is continuous, in general, we cannot compute the value functions exactly. The simplest way to proceed is to discretize the state space \(\ALPHABET X\).</description>
    </item>
    
    <item>
      <title>Approximate dynamic programming</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/adp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/adp/</guid>
      <description>The value and policy iteration algorithms for discounted cost MDPs rely on exact computation of the Bellman update \(W = \mathcal B V\) and the corresponding optimal policy \(g\) such that \(\mathcal B V = \mathcal B_g V\). Suppose we cannot compute these updates exactly, but can find approximate solutions \(W\) and \(g\) such that \[ \NORM{W - \mathcal B V} \le δ \quad\text{and}\quad \NORM{\mathcal B_g V - \mathcal B V} \le ε\] where \(δ\) and \(ε\) are positive constants.</description>
    </item>
    
  </channel>
</rss>