<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MDP on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/categories/mdp/</link>
    <description>Recent content in MDP on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://adityam.github.io/stochastic-control/categories/mdp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Linear Quadratic Regulation (LQR)</title>
      <link>https://adityam.github.io/stochastic-control/theory/lqr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/lqr/</guid>
      <description>Note: To be consistent with the notation used in linear systems, we denote the state and action by lowercase \(x\) and \(u\), even for stochastic systems (unlike the notation used for other models where we use uppercase \(X\) and \(U\) for state and actions to emphasize the fact they are random variables.
We start by considering a determinisitc linear system with state \(x_t \in \reals^n\) and control actions \(u_t \in \reals^m\) and dynamics \[ x_{t+1} = A_t x_t + B_t u_t,\] where \(A_t \in \reals^{n \times n}\) and \(B_t \in \reals^{n \times m}\) are known matrices.</description>
    </item>
    
    <item>
      <title>Markov decision processes</title>
      <link>https://adityam.github.io/stochastic-control/theory/mdp-functional/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/mdp-functional/</guid>
      <description>Markov decision processes (MDP) model model the simplest stochastic control architecture. The dynamic behavior of an MDP is modeled by an equation of the form \[ X_{t+1} = f_t(X_t, U_t, W_t) \] where \(X_t \in \ALPHABET X\) is the state, \(U_t \in \ALPHABET U\) is the control input, and \(W_t \in \ALPHABET W\) is the noise. An agent/controller observes the state and chooses the control input \(U_t\).
The controller can be as sophisticated as we want.</description>
    </item>
    
    <item>
      <title>Matrix formulation of Markov decision processes</title>
      <link>https://adityam.github.io/stochastic-control/theory/mdp-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/mdp-matrix/</guid>
      <description>In this section, we present a matrix formulation for finite state finite action MDPs, which is useful for computing the solutions numerically. Letâ€™s start with the function model described earlier and assume that \(\ALPHABET X\) and \(\ALPHABET U\) are finite sets and that the cost function and the probability distribution of \(\{W_t\}_{t \ge 1}\) are time-homogeneous. Then, the following is a fundamental property of MDPs:
 Lemma For any \(x_1, x_2, \dots, x_T \in \ALPHABET X\) and \(u_1, \dots, u_T \in \ALPHABET U\), we have \[ \PR(X_{t+1} = x_{t+1} | X_{1:t} = x_{1:t}, U_{1:t} = u_{1:t}) = \PR(X_{t+1} = x_{t+1} | X_{t} = x_t , U_t = u_t).</description>
    </item>
    
    <item>
      <title>Optimal Gambling</title>
      <link>https://adityam.github.io/stochastic-control/examples/optimal-gambling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/examples/optimal-gambling/</guid>
      <description>TL;DR This stylized model of optimal gambling was introduced by Kelly (1956) to highlight a relationship between channel capacity (which had been proposed recently by Shannon), and gambling. Our motivation for studying this model is to use it as an illustrative example to show that sometimes it is possible to identify the optimal strategy and value function of MDPs in closed form.
Imagine a gambler who goes to a casino with an initial fortune of \(x_1\) dollars and places bets over time and must leave after \(T\) bets.</description>
    </item>
    
  </channel>
</rss>