<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>POMDP on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/categories/pomdp/</link>
    <description>Recent content in POMDP on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://adityam.github.io/stochastic-control/categories/pomdp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Theory: Basic model of a POMDP</title>
      <link>https://adityam.github.io/stochastic-control/pomdp/pomdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/pomdp/pomdp/</guid>
      <description>So far, we have considered a setup where the decision maker perfectly observes the state of the system. In many applications, the decision maker may not directly observe the state of the system but only observe a noisy version of it. Such systems are modeled as partially observable Markov decision processes (POMDPs). We will describe the simplest model of POMDPs, which builds upon the model of MDPs descibed earlier.
We assume that the system has a state \(S_t \in \ALPHABET S\), control input \(A_t \in \ALPHABET A\), and process noise \(W_t \in \ALPHABET W\).</description>
    </item>
    
    <item>
      <title>Example: Sequential hypothesis testing</title>
      <link>https://adityam.github.io/stochastic-control/pomdp/sequential-hypothesis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/pomdp/sequential-hypothesis/</guid>
      <description>Consider a decision maker (DM) that makes a series of i.i.d. observations which may be distributed according to PDF \(f_0\) or \(f_1\). Let \(Y_t\) denote the observaion at time \(t\). The DM wants to differentiate between two hypothesis: \[\begin{gather*} h_0 : Y_t \sim f_0 \\ h_1 : Y_t \sim f_1 \end{gather*}\] Typically, we think of \(h_0\) as the normal situation (or the null hypothesis) and \(h_1\) as an anomaly. For example, the hypothesis may be \[ h_0: Y_t \sim {\cal N}(0, σ^2) \quad h_1: Y_t \sim {\cal N}(μ, σ^2) \] or \[ h_0: Y_t \sim \text{Ber}(p) \quad h_1: Y_t \sim \text{Ber}(q).</description>
    </item>
    
    <item>
      <title>Partially observed linear quadratic regulator</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/lqg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/lqg/</guid>
      <description>Consider a stochastic linear system as in the case of LQR. The system has state \(x_t \in \reals^n\) and actions \(u_t \in \reals^m\). The initial state \(x_1\) has zero mean and finite variance \(\Sigma^x_1\). The system dynamics are given by \[ x_{t+1} = A_t x_t + B_t u_t + w_t, \] where \(A_t \in \reals^{n×n}\) and \(B_t \in \reals^{n×m}\) are known matrices and \(\{w_t\}_{t \ge 1}\) is \(\reals^n\)-valued i.i.d. noise process with zero mean and finite variance \(\Sigma^w\).</description>
    </item>
    
  </channel>
</rss>
