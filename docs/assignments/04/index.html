<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" />
  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/github.css" type="text/css" />
  <script src="https://adityam.github.io/stochastic-control//js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script type="text/javascript"
    src="https://cdn.plot.ly/plotly-1.2.0.min.js">
  </script>
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_SVG">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "HTML-CSS": { 
          fonts: ["TeX"]
      }, 
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
    },
      TeX : {
          Macros : {
            PR: "\\mathbb{P}",
            EXP: "\\mathbb{E}",
            IND: "\\mathbb{I}",
            reals: "\\mathbb{R}",
            integers: "\\mathbb{Z}",
            TRANS: "\\intercal",
            VEC: "\\operatorname{vec}",
            TR:  "\\operatorname{Tr}",
            // mathcal: "\\mathscr",
            ALPHABET: ["\\mathcal{#1}", 1],
            MATRIX: ["\\begin{bmatrix} #1 \\end{bmatrix}", 1],
            NORM: ["\\left\\lVert #1 \\right\\rVert", 1],
            ABS: ["\\left\\lvert #1 \\right\\rvert", 1],
          }
      }
    });
  </script>
</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2018
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<h1>Assignment 4</h1>

<ol type="1">
<li><p>A hunter is interested in optimal scheduling of hunting during an expedition that may last up to <span class="math inline">\(T\)</span> days. The hunter has two options on each day: he may go out to hunt or he may decide to stop the expedition and go back home.</p>
<ul>
<li>Going out to hunt costs <span class="math inline">\(c\)</span> units per day with the possibility of capturing one animal. The probability of capture depends on the population <span class="math inline">\(n\)</span>: probability of a capture is <span class="math inline">\(p(n)\)</span>; probability of no capture is <span class="math inline">\(1 - p(n)\)</span>. In case of a capture, the hunter gets a reward of one unit; otherwise, he gets no reward.</li>
<li>If he decides to stop the hunting expedition, the hunter does not get any opportunity to hunt in the future, and as such receives no reward in the future. There is no cost associated with taking this action.</li>
<li>The hunter must return back at the end of <span class="math inline">\(T\)</span> days.</li>
</ul>
<p>Assume the following:</p>
<ol type="i">
<li>The hunter knows the initial population <span class="math inline">\(N\)</span>.</li>
<li>The time scale is such that the population does not change due to natural causes (birth or death).</li>
<li>The probability that the hunter makes more than one capture a day is negligible.</li>
<li>The function <span class="math inline">\(p(n)\)</span> is monotonically increasing in <span class="math inline">\(n\)</span>.</li>
</ol>
<p>For the above system:</p>
<ol type="a">
<li><p>Write a dynamical model with the state as the current population. Argue that the above is an optimal stopping problem with perfect state observations and write the dynamic program for this system.</p>
<p><strong>Note</strong>: You can write the dynamic program in two ways: either to maximize reward or to minimize cost. You may use either of these alternatives.</p></li>
<li><p>Let <span class="math inline">\(位\)</span> be the smallest integer such that <span class="math inline">\(p(位) &gt; c\)</span>. Show that it is optimal to stop hunting when the current population falls below <span class="math inline">\(位\)</span>.</p>
<p><strong>Hint</strong>: The above result is equivalent to saying that the value function is zero if and only if the current population is less than <span class="math inline">\(位\)</span>.</p></li>
</ol></li>
<li><p>Consider a discrete-time time-homogeneous Markov chain with state space <span class="math inline">\(\integers _ {&gt; 0}\)</span> and transition probabilities <span class="math inline">\(\{ P(i,j) : i, j \in \integers _ {&gt; 0} \}\)</span>.</p>
<p>At each time instant, a decision maker perfectly observes the state of the system and decides to either: (i) explore further and incurs an per-step cost of <span class="math inline">\(c\)</span>; (ii) exploit the system in its current state, that gives a terminal reward of <span class="math inline">\(r(X_t)\)</span>. The exploit decision is terminal in the sense that once an exploit decision is made, no further costs or rewards occur. At time <span class="math inline">\(T\)</span>, the decision maker must choose the exploit option.</p>
<ol type="a">
<li><p>Write down the dynamic programming equation for the above model. (Expand the expectation in the DP equation in terms of the variables defined above).</p></li>
<li><p>Show that for a fixed <span class="math inline">\(x\)</span>, the value function is non-increasing in <span class="math inline">\(t\)</span>.</p></li>
<li><p>Define for <span class="math inline">\(t = 1, \dots, T-1\)</span> the set <span class="math display">\[H_t = \{ x \in \integers_{ &gt; 0 } : g^*_t(x) = \text{exploit} \}.\]</span><br />
Assume that <span class="math inline">\(H_{T-1}\)</span> has the following property:</p>
<p><strong>(P1)</strong> Whenever <span class="math inline">\(i \in H_{T-1}\)</span> and <span class="math inline">\(j \not\in H_{T-1}\)</span>, then <span class="math inline">\(P(i,j) = 0\)</span>.</p>
<p>Show that under property (P1): <span class="math display">\[H_{T-1} = H_{T-2} = \cdots = H_1.\]</span></p></li>
<li><p>Suppose that for every <span class="math inline">\(x \in \integers _ {&gt; 0}\)</span>: <span class="math display">\[ P(x,x) = P(x,x+1) = \frac 12 
   \quad\text{and}\quad
   r(x) = k ( 1 - 2^{-x})\]</span> where <span class="math inline">\(k\)</span> is a positive constant. Prove that in this case it is optimal to stop at the first instant of time the Markov chain enters the set <span class="math display">\[ M = \{ m, m + 1, m + 2, \dots \}\]</span> where <span class="math display">\[ m = \min \{ x \ge 0 : 2^x \ge k/(4c) \}\]</span></p></li>
</ol></li>
</ol>


<p class="categories">
This entry was last updated on
26 Mar 2018

</p>



      <script type="text/javascript">
      
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-6887174-4']);
            _gaq.push(['_trackPageview']);
            (function() {
                var ga   = document.createElement('script');
                ga.type  = 'text/javascript';
                ga.async = true;
                ga.src   = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
              })();
      
      </script>
    </div>
  </body>
</html>


