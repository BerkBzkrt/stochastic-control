<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" />
  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/github.css" type="text/css" />
  <script src="https://adityam.github.io/stochastic-control//js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script type="text/javascript"
    src="https://cdn.plot.ly/plotly-basic-latest.min.js">
  </script>
  <script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_SVG,https://adityam.github.io/stochastic-control/js/mathjax-local.js">
  </script>
</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2018
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<h1>Assignment 7</h1>

<ol type="1">
<li><p>A person is offered <span class="math inline">\(N\)</span> free plays to be distributed as he pleases between two slot machines <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. Machine <span class="math inline">\(A\)</span> pays <span class="math inline">\(\alpha\)</span> dollars with known probability <span class="math inline">\(s\)</span> and nothing with probability <span class="math inline">\((1-s)\)</span>. Machine <span class="math inline">\(B\)</span> pays <span class="math inline">\(\beta\)</span> dollars with probability <span class="math inline">\(p\)</span> and nothing with probability <span class="math inline">\((1-p)\)</span>. The person does not know <span class="math inline">\(p\)</span> but has an a priori probability distribution function <span class="math inline">\(f(p)\)</span> for <span class="math inline">\(p\)</span>. The problem is to find a playing policy that maximizes expected profit.</p>
<p>Suppose at the <span class="math inline">\(t\)</span>-th time step, the person has played machine <span class="math inline">\(B\)</span>, <span class="math inline">\(m + n\)</span> times, where <span class="math inline">\(m\)</span> denotes the number of successes and <span class="math inline">\(n\)</span> the number of failures. Let <span class="math inline">\(\xi(m,n)\)</span> denote the probability that the next play of machine <span class="math inline">\(B\)</span> will yield a success, that is, <span class="math display">\[\xi(m,n) = \frac {\int_{0}^1 p^{m+1} (1 - p)^n f(p) dp }
                     {\int_{0}^1 p^m     (1 - p)^n f(p) dp }\]</span> Note that <span class="math inline">\(\xi(m,n)\)</span> can be precomputed for all values of <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>.</p>
<ol type="a">
<li>Show that <span class="math inline">\((m,n)\)</span> is an information state for the above problem.</li>
<li>Write down a dynamic program in terms of <span class="math inline">\((m,n)\)</span>.</li>
<li>Find an optimal strategy for <span class="math inline">\(T = 6\)</span>, <span class="math inline">\(\alpha = \beta = 1\)</span>, <span class="math inline">\(s = 0.6\)</span>, <span class="math inline">\(f(p) = 1\)</span> for <span class="math inline">\(0 \le p \le 1\)</span>. (For each time, you need to identify the pairs <span class="math inline">\((m,n)\)</span> where machine <span class="math inline">\(B\)</span> should be playerd.)</li>
</ol></li>
<li><p>Consider the following modification of the sequential hypothesis testing problem considered in class. As in the model discussed in class, there are two hypothesis <span class="math inline">\(h_0\)</span> and <span class="math inline">\(h_1\)</span>. The a priori probability that the hypothesis is <span class="math inline">\(h_0\)</span> is <span class="math inline">\(p\)</span>.</p>
<p>In contrast to the model discussed in class, there are <span class="math inline">\(N\)</span> sensors. If sensor <span class="math inline">\(m\)</span> is used at time <span class="math inline">\(t\)</span> and the underlying hypothesis is <span class="math inline">\(h_i\)</span>, then the observation <span class="math inline">\(Y_t\)</span> is distrubted according to pdf (or pmf) <span class="math inline">\(f^m_j(y)\)</span>. The cost of using sensor <span class="math inline">\(m\)</span> is <span class="math inline">\(c_m\)</span>.</p>
<p>Whenever the decision maker takes a measurement, he picks a sensor <span class="math inline">\(m\)</span> uniformly at random from <span class="math inline">\(\{1, \dots, N\}\)</span> and observes <span class="math inline">\(Y_t\)</span> according to the distribution <span class="math inline">\(f^m_j(\cdot)\)</span> and incurs a cost <span class="math inline">\(c_m\)</span>.</p>
<p>The system continues for a finite time <span class="math inline">\(T\)</span>. At each time <span class="math inline">\(t &lt; T\)</span>, the decision maker has three options: stop and declare <span class="math inline">\(h_0\)</span>, stop and declare <span class="math inline">\(h_1\)</span>, or continue to take another measurement. At time <span class="math inline">\(T\)</span>, the continue alternative is unavailable.</p>
<ol type="a">
<li><p>Formulate the above problem as a POMDP. Identify an information state and write the dynamic programming decomposition for the problem.</p></li>
<li><p>Show that the optimal control law has a threshold property, similar to the threshold propertly for the model considered in class.</p></li>
</ol></li>
</ol>


<p class="categories">
This entry waslast updated on 26 Mar 2018</p>



      <script type="text/javascript">
      
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-6887174-4']);
            _gaq.push(['_trackPageview']);
            (function() {
                var ga   = document.createElement('script');
                ga.type  = 'text/javascript';
                ga.async = true;
                ga.src   = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
              })();
      
      </script>
    </div>
  </body>
</html>


