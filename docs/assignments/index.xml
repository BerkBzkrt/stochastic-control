<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Assignments on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/assignments/</link>
    <description>Recent content in Assignments on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://adityam.github.io/stochastic-control/assignments/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Assignment 1</title>
      <link>https://adityam.github.io/stochastic-control/assignments/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/01/</guid>
      <description>Exercise 1 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.
 Exercise 2 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.
 Exercise 3 from the notes on the newsvendor problem.</description>
    </item>
    
    <item>
      <title>Assignment 2</title>
      <link>https://adityam.github.io/stochastic-control/assignments/02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/02/</guid>
      <description>AM-GM inequality. Consider the problem of maximizing \[ V_t(a) = \max_{(u_1, \dots, u_t) \in \ALPHABET S_t(a)} u_1 ⋅ u_2 \cdots u_t \] where the region \(\ALPHABET S_t(a)\) is determined by the conditions \[ \sum_{i=1}^t u_i = a \quad \text{and} \quad u_i \ge 0. \]
Prove that \(V_t\) satisfies the following recursion \[ V_t(a) = \max_{u \in [0,a]} u ⋅ V_{t- 1}(a - u), \quad t &amp;gt; 1 \] with \(V_1(a) = a\).</description>
    </item>
    
    <item>
      <title>Assignment 3</title>
      <link>https://adityam.github.io/stochastic-control/assignments/03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/03/</guid>
      <description>Service rate control in queueing systems
Consider a queueing system in which jobs arrive according to a i.i.d. Bernoulli process \(\{A_t\}_{t \ge 1}\), where \(\PR(A_t = 1) = q\). The jobs are queued in a buffer of size \(n\). Let \(X_t \in \{0, 1, \dots, n\}\) denote the number of jobs in the queue at time \(t\).
At each time, if there are jobs in the queue, the server works on the job at the head of the queue at service rate \(U_t \in \{0, 1, \dots, m\}\) and finishes the job with probability \(p(u)\).</description>
    </item>
    
    <item>
      <title>Assignment 4</title>
      <link>https://adityam.github.io/stochastic-control/assignments/04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/04/</guid>
      <description>Exercise 1 of the notes on power-delay trade-off in wireless communication.
 Exercise 1 (parts a–c; part d is for reference and you don’t need to submit a solution for part d) of the notes on monotonicity in MDPs.
  </description>
    </item>
    
    <item>
      <title>Assignment 5</title>
      <link>https://adityam.github.io/stochastic-control/assignments/05/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/05/</guid>
      <description>Exercise 2 of the notes on monotonicity of optimal stopping.
 Exercise 3 of the notes on monotonicity of optimal stopping.
  </description>
    </item>
    
    <item>
      <title>Assignment 6</title>
      <link>https://adityam.github.io/stochastic-control/assignments/06/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/06/</guid>
      <description>Exercise 1 of note on discounted mdp
 Consider the infinite horizon version rate control problem described in Assignment 3 with discount factor \(\beta = 0.9\).
Use value iteration to find a policy \(g\) such that \(\NORM{V_g - V^*} \le 10^{-5}\). How many iterations does it take to converge? Plot the value function at the last step of the iteration. Print the optimal policy.
 Use policy iteration to find the optimal policy \(g\).</description>
    </item>
    
    <item>
      <title>Assignment 7</title>
      <link>https://adityam.github.io/stochastic-control/assignments/07/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/07/</guid>
      <description>Exercise 1 of notes on Lipschitz MDPs.  The purpose of this exercise is to justify the truncation of the state space, as we did in in the example of state transmission in Internet of Things considered in Assignment 3. Consider the infinite horizon discounted cost version of that example.
In Exercise 1 of monotone MDPs, we have already established that the optimal policy is even and quasi-convex. Since the action space is binary, this means that the optimal policy is of the form \[ g(e) = \begin{cases} 0, &amp;amp; |e| \le k, \\ 1, &amp;amp; |e| &amp;gt; k.</description>
    </item>
    
    <item>
      <title>Assignment 8</title>
      <link>https://adityam.github.io/stochastic-control/assignments/08/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/08/</guid>
      <description>Exercise 1 of notes on POMDPs
 Exercise 1 of notes on Sequential hypothesis testin
 Consider a Markov chain \(\{X_t, t= 1, 2, \dots \}\) with state space \(\mathcal X = \{0, 1\}\) and transition matrix shown below.
The state \(0\) denotes a safe state while the state \(1\) denotes a faulty state. Assume that \(\PR(X_1 = 0) = q_0\).
A decision makers observes the state with noise as follows \[ Y_t = h(X_t, N_t), \quad t= 1, 2, \dots\] where \(\{N_t\} _ {t=1}^{T}\) is the observation noise process and \(h\) is a known function.</description>
    </item>
    
  </channel>
</rss>
