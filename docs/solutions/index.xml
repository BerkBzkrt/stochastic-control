<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Solutions on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/solutions/</link>
    <description>Recent content in Solutions on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://adityam.github.io/stochastic-control/solutions/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://adityam.github.io/stochastic-control/solutions/03-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/03-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://adityam.github.io/stochastic-control/solutions/05-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/05-2/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://adityam.github.io/stochastic-control/solutions/06-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/06-2/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://adityam.github.io/stochastic-control/solutions/07-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/07-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Assignment 1 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/01/</guid>
      <description>Problem 1 We first observe a simplification that can be done for this model, which makes the calculations a little easier. In principle, we need to compute \[ Q(x,u) = \EXP[ c(x,u,W) | X = x] = \sum_{w \in \ALPHABET W} P(W = w | X = x) c(x,u,w). \]
This means that we need to compute \(P(W|X)\). However, we can avoid that step by observing that \[P(W|X) = \dfrac{ P(X,W) }{ P(X) }.</description>
    </item>
    
    <item>
      <title>Assignment 2 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/02/</guid>
      <description>Problem 1 The result follows by writing the maximization in a nested manner \[\begin{align*} V_t(a) &amp;amp;= \max_{(u_1, \dots u_t) \ge 0 : \sum_{i=1}^t u_i = a} u_1 \cdots u_t \\ &amp;amp;= \max_{u_t \in [0,a]} \Bigl\{ \max_{(u_1, \dots, u_{t-1}) \ge 0 : \sum_{i=1}^{t-1} u_i = a-u_t} u_1 \cdots u_{t-1} \Bigr\} \cdot u_t \\ &amp;amp; = \max_{u_t \in [0,a]} V_t(a-u_t) \cdot u_t \end{align*}\]
 We use induction to show that \(V_t(a) = (a/t)^t\).</description>
    </item>
    
    <item>
      <title>Assignment 3 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/03/</guid>
      <description>Problem 1 The code for computing the optimal policy is available here. The code is written for ease of reading, not efficiency.
# State spaces n, m = 8, 3 X = 1:n+1 U = 1:m+1 # Transition probability q = 0.6 p = [0.0,0.25,0.5,0.8] P = [ zeros(n+1,n+1) for u in U ] # We are using 1-indexed arrays, so need to be careful with the indices for u in U @views Pu = P[u] for x in X if x == 1 Pu[x,x] = 1 - q Pu[x,x+1] = q elseif x == n+1 Pu[x,x-1] = (1-q)*p[u] Pu[x,x ] = 1 - (1-q)*p[u] else Pu[x,x-1] = (1-q)*p[u] Pu[x,x ] = (1-q)*(1-p[u]) + q*p[u] Pu[x,x+1] = q*(1-p[u]) end end end P_concat = vcat(P.</description>
    </item>
    
    <item>
      <title>Assignment 4 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/04/</guid>
      <description>Problem 1 I had a solution to this problem but the TA found a mistake in my solution. Having thought about it a bit, I realize that as stated the problem cannot be solved.
There are urban legends of students solving unsolved problems thinking they were homework questions. That was not my intention! Let me attempt to explain what led to the mistake and how it can be fixed. This requires some background knowledge of communication systems.</description>
    </item>
    
    <item>
      <title>Assignment 5 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/05/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/05/</guid>
      <description>Problem 1 For the reward maximization case, the dynamic program is given as follows:
\[V_T(x) = s_T(x)\] and for \(t \in \{T-1,\dots, 1\}\), we have \[V_t(x) = \max\{ s_t(x), c_t(x) + \EXP[V_{t+1}(X_{t+1} | X_t = x]. \]
We define the benefit funciton as before. i.e., \[B_t(x) = c_t(x) + \EXP[ V_{t+1}(X_{t+1}) | X_t = x] - s_t(x). \] Then, it is optimal to stop whenever \(B_t(x) \le 0\).
Now, we write the value function in terms of the benefit function as follows:</description>
    </item>
    
    <item>
      <title>Assignment 6 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/06/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/06/</guid>
      <description>Problem 1 We start by the right side of the inequality. We want to show that \[V_{k+1} + \bar{\delta}_{k+1} \leq V_k + \bar{\delta}_k.\] We know from the value iteration algorithm proof that \[\mathcal{B} V_k - V_k \leq (1 - \beta) \bar{\delta}_k\] Rewriting this equation, we get \[\begin{equation} V_k + \bar{\delta}_k \geq \mathcal{B} V_k + \beta \bar{\delta}_k \implies V_k + \bar{\delta}_k \geq V_{k+1} + \beta \bar{\delta}_k. \label{eq:one} \end{equation}\]
By definition, we have \[\begin{align} \bar{\delta}_{k+1} &amp;amp;= \frac{\beta}{1-\beta} \max_x \{ V_{k+1}(x) - V_k(x) \} = \frac{\beta}{1-\beta} \| V_{k+1} - V_k \| \notag \\ &amp;amp;= \frac{\beta}{1-\beta} \| \mathcal{B}V_k - \mathcal{B}V_{k-1} \| \notag \\ &amp;amp;\leq \frac{\beta^2}{1-\beta} \| V_{k} - V_{k-1} \| = \beta \bar{\delta}_k.</description>
    </item>
    
    <item>
      <title>Assignment 7 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/07/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/07/</guid>
      <description>Part a. Let \(Y_t \in \{0, 1\}\) denote the observation of the player where \(Y_t = 0\) means that the output was a failure and \(Y_t = 1\) means that the output was a success. Let \(I_t = \{Y_{1:t-1}, U_{1:t-1} \}\) denote all the information available to the player.
From the definition of \((M_t, N_t)\), the evolution of \((M_t, N_t)\) can be written as \[ (M_{t+1}, N_{t+1}) = \begin{cases} (M_t, N_t), &amp;amp; U_t = A \\ (M_t + 1, N_t), &amp;amp; U_t = B, Y_t = 1 \\ (M_t , N_t + 1), &amp;amp; U_t = B, Y_t = 0 \end{cases}\]</description>
    </item>
    
  </channel>
</rss>