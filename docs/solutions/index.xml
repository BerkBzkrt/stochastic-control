<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Solutions on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/solutions/</link>
    <description>Recent content in Solutions on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://adityam.github.io/stochastic-control/solutions/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Assignment 1 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/01/</guid>
      <description>Problem 1 We first observe a simplification that can be done for this model, which makes the calculations a little easier. In principle, we need to compute \[ Q(s,a) = \EXP[ c(s,a,W) | S = s] = \sum_{w \in \ALPHABET W} P(W = w | S = s) c(s,a,w). \]
This means that we need to compute \(P(W|S)\). However, we can avoid that step by observing that \[P(W|S) = \dfrac{ P(S,W) }{ P(S) }.</description>
    </item>
    
    <item>
      <title>Assignment 2 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/02/</guid>
      <description>Problem 1  Problem 2 We prove the result for the case when \(V_{T-1}(x) \le V_T(x)\). The case when \(V_{T-1}(x) \ge V_T(x)\) follows from a similar argument.
As usual, the proof proceeds by backward induction. We know that the result is true for \(t = T-1\). This forms the basis of induction. Assume that the result is true for \(t+1\), i.e., \[\begin{equation} \label{eq:1.1} V_{t+1}(x) \le V_{t+2}(x), \quad \forall x \in \ALPHABET X \end{equation}\]</description>
    </item>
    
    <item>
      <title>Assignment 3 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/03/</guid>
      <description>0.1 Problem 1 The code for computing the optimal policy is available here. The code is written for ease of reading, not efficiency.
using Distributions, OffsetArrays  # Parameters n, m = 8, 3 λ = 0.5  μ = [0.2, 0.5, 0.8] q = [1, 3, 8]  h = 1 R = 10 B = 15  # State spaces S = 0:n A = 1:m W = 0:B Z = 0:1  # Dynamics f(s,z,w) = min( max(s-z, 0) + w, n)  # Arrival probability Pw(w) = pdf(Poisson(λ),w)  # Departure probability Pz(z,s,a) = (s == 0) ?</description>
    </item>
    
    <item>
      <title>Assignment 4 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/04/</guid>
      <description>Problem 1 We first note that when the channel is i.i.d., we can simplify the dynamic program as follows:
\[V_{T+1}(x,s) = 0\]
and for \(t \in \{T, \dots, 1\}\), \[\begin{align} H_t(y) &amp;amp;= λ d(y) + \EXP[ V_{t+1}(y + W_t, S_{t+1}) ] \label{eq:1}\\ V_t(x,s) &amp;amp;= \min_{0 \le a \le x} \bigl\{ p(a) q(s) + H_t(x-a) \bigr\} \end{align}\]
Note that due the i.i.d. nature of the channel, we don’t need to condition on the current state in \eqref{eq:1}.</description>
    </item>
    
    <item>
      <title>Assignment 5 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/05-v1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/05-v1/</guid>
      <description>0.1 Problem 1 For the reward maximization case, the dynamic program is given as follows:
\[V_T(x) = s_T(x)\] and for \(t \in \{T-1,\dots, 1\}\), we have \[V_t(x) = \max\{ s_t(x), c_t(x) + \EXP[V_{t+1}(X_{t+1} | X_t = x]. \]
We define the benefit funciton as before. i.e., \[B_t(x) = c_t(x) + \EXP[ V_{t+1}(X_{t+1}) | X_t = x] - s_t(x). \] Then, it is optimal to stop whenever \(B_t(x) \le 0\).
Now, we write the value function in terms of the benefit function as follows:</description>
    </item>
    
    <item>
      <title>Assignment 5 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/05/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/05/</guid>
      <description>Note that \(c(s,1) = λ\), which is a constant and \(c(\cdot,2) = [0,1,5]\), which is increasing. Thus (C1) is satisfied.
Now we consider stochastic monotonicity of the transition matrices. \(P(1) = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \\ 1 &amp;amp; 0 &amp;amp; 0 \\ 1 &amp;amp; 0 &amp;amp; 0 \end{bmatrix}\). Since all rows of \(P(1)\) are identical, it is trivially, stochsatic monotone.
\(P(2) = \begin{bmatrix} 1-θ &amp;amp; θ &amp;amp; 0 \\ 0 &amp;amp; 1-θ &amp;amp; θ \\ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}\).</description>
    </item>
    
    <item>
      <title>Assignment 6 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/06/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/06/</guid>
      <description>\(\def\ONES{\mathbb{1}}\)
Problem 1 We start by the right side of the inequality. We want to show that \[\begin{equation} \label{eq:zero} V_{k+1} + \bar{\delta}_{k+1}\ONES \leq V_k + \bar{\delta}_k\ONES. \end{equation}\]
From the definition of \(\bar{δ}_k\), we know that for any \(s \in \ALPHABET S\),
\[ V_{k}(s) - V_{k-1}(s) \le \frac{(1-γ)}{γ} \bar δ_k. \] Thus, \[ V_k \le V_{k-1} + \frac{(1-γ)}{γ} \bar δ_k \ONES. \] Applying the Bellman operator on both sides and using the discounting property, we get that \[\begin{equation} V_{k+1} \le V_k + (1-γ) \bar δ_k \ONES \label{eq:one} \end{equation}\]</description>
    </item>
    
    <item>
      <title>Assignment 7 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/07/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/07/</guid>
      <description>0.0.1 Problem 1 We are given Bernoulli measures \(μ = a δ_x + (1-a) δ_y\) and \(ν = b δ_x + (1-b)δ_y\). Now consider \[\begin{align*} K(μ, ν) &amp;amp;= \sup_{ f \colon \| f \|_L \le 1 } \left| \int_X f dμ - \int_X f dν \right| \\ &amp;amp;= \sup_{ f \colon \| f \|_L \le 1 } \left| \int_X f(z)( a δ_x(z) + (1 - a) δ_y(z) ) dz - \int_X f(z)( b δ_x(z) + (1 - b) δ_y(z) ) dz \right| \\ &amp;amp;= \sup_{ f \colon \| f \|_L \le 1 } \left| a f(x) + (1-a) f(y) - \big( b f(x) + (1-b) f(y) \big) \right| \\ &amp;amp;= \sup_{ f \colon \| f \|_L \le 1 } | a-b | | f(x) - f(y) | \\ &amp;amp;= |a - b | \sup_{ f \colon \| f \|_L \le 1 } \left| f(x) - f(y) \right | \\ &amp;amp;= |a -b | d_X(x,y).</description>
    </item>
    
  </channel>
</rss>
