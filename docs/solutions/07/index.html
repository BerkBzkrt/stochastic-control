<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" />
  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/github.css" type="text/css" />
  <script src="https://adityam.github.io/stochastic-control//js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script type="text/javascript"
    src="https://cdn.plot.ly/plotly-1.2.0.min.js">
  </script>
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_SVG">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "HTML-CSS": { 
          fonts: ["TeX"]
      }, 
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
    },
      TeX : {
          Macros : {
            PR: "\\mathbb{P}",
            EXP: "\\mathbb{E}",
            IND: "\\mathbb{I}",
            reals: "\\mathbb{R}",
            integers: "\\mathbb{Z}",
            TRANS: "\\intercal",
            VEC: "\\operatorname{vec}",
            TR:  "\\operatorname{Tr}",
            // mathcal: "\\mathscr",
            ALPHABET: ["\\mathcal{#1}", 1],
            MATRIX: ["\\begin{bmatrix} #1 \\end{bmatrix}", 1],
            NORM: ["\\left\\lVert #1 \\right\\rVert", 1],
            ABS: ["\\left\\lvert #1 \\right\\rvert", 1],
          }
      }
    });
  </script>
</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2018
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<h1>Assignment 7 (solution)</h1>

<ol type="1">
<li><p>A person is offered <span class="math inline">\(N\)</span> free plays to be distributed as he pleases between two slot machines <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. Machine <span class="math inline">\(A\)</span> pays <span class="math inline">\(\alpha\)</span> dollars with known probability <span class="math inline">\(s\)</span> and nothing with probability <span class="math inline">\((1-s)\)</span>. Machine <span class="math inline">\(B\)</span> pays <span class="math inline">\(\beta\)</span> dollars with probability <span class="math inline">\(p\)</span> and nothing with probability <span class="math inline">\((1-p)\)</span>. The person does not know <span class="math inline">\(p\)</span> but has an a priori probability distribution function <span class="math inline">\(f(p)\)</span> for <span class="math inline">\(p\)</span>. The problem is to find a playing policy that maximizes expected profit.</p>
<p>Suppose at the <span class="math inline">\(t\)</span>-th time step, the person has played machine <span class="math inline">\(B\)</span>, <span class="math inline">\(m + n\)</span> times, where <span class="math inline">\(m\)</span> denotes the number of successes and <span class="math inline">\(n\)</span> the number of failures. Let <span class="math inline">\(\xi(m,n)\)</span> denote the probability that the next play of machine <span class="math inline">\(B\)</span> will yield a success, that is, <span class="math display">\[\xi(m,n) = \frac {\int_{0}^1 p^{m+1} (1 - p)^n f(p) dp }
                     {\int_{0}^1 p^m     (1 - p)^n f(p) dp }\]</span> Note that <span class="math inline">\(\xi(m,n)\)</span> can be precomputed for all values of <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>.</p>
<ol type="a">
<li>Show that <span class="math inline">\((m,n)\)</span> is an information state for the above problem.</li>
<li>Write down a dynamic program in terms of <span class="math inline">\((m,n)\)</span>.</li>
<li>Find an optimal strategy for <span class="math inline">\(T = 6\)</span>, <span class="math inline">\(\alpha = \beta = 1\)</span>, <span class="math inline">\(s = 0.6\)</span>, <span class="math inline">\(f(p) = 1\)</span> for <span class="math inline">\(0 \le p \le 1\)</span>. (For each time, you need to identify the pairs <span class="math inline">\((m,n)\)</span> where machine <span class="math inline">\(B\)</span> should be playerd.)</li>
</ol></li>
<li><p><strong>Part a.</strong> Let <span class="math inline">\(Y_t \in \{0, 1\}\)</span> denote the observation of the player where <span class="math inline">\(Y_t = 0\)</span> means that the output was a failure and <span class="math inline">\(Y_t = 1\)</span> means that the output was a success. Let <span class="math inline">\(I_t = \{Y_{1:t-1}, U_{1:t-1} \}\)</span> denote all the information available to the player.</p>
<p>From the definition of <span class="math inline">\((M_t, N_t)\)</span>, the evolution of <span class="math inline">\((M_t, N_t)\)</span> can be written as <span class="math display">\[ (M_{t+1}, N_{t+1}) = 
\begin{cases}
   (M_t, N_t), &amp; U_t = A \\
   (M_t + 1, N_t), &amp; U_t = B, Y_t = 1 \\
   (M_t , N_t + 1), &amp; U_t = B, Y_t = 0 
\end{cases}\]</span></p>
<p>Furthermore, the probability of next observation is given as <span class="math display">\[\PR(Y_t = 1 | I_t, U_t) = 
\begin{cases}
   s, &amp; U_t = A \\
   ξ(M_t, N_t), &amp; U_t = B 
\end{cases}\]</span></p>
<p>Combing the two, we can write <span class="math display">\[\begin{align}
\PR( (M_{t+1}, N_{t+1}) | I_t, U_t) &amp;=
\begin{cases}
   1, &amp; \hbox{if } U_t = A \hbox{ and } (M_{t+1}, N_{t+1}) = (M_t, N_t) \\
   ξ(M_t, N_t), &amp; \hbox{if } U_t = B \hbox{ and } (M_{t+1}, N_{t+1}) = (M_t
   + 1, N_t) \\
   1 - ξ(M_t, N_t), &amp; \hbox{if } U_t = B \hbox{ and } (M_{t+1}, N_{t+1}) =
   (M_t, N_t + 1) 
\end{cases} \\
&amp;= 
\PR( (M_{t+1}, N_{t+1}) | (M_t, N_t), U_t)
\end{align}\]</span> Thus, <span class="math inline">\((M_t, N_t)\)</span> evolves in a controlled Markovian manner.</p>
<p>Now, let’s consider the per-step reward. <span class="math display">\[R_t(Y_t, U_t) = 
\begin{cases}
   α, Y_t = 1, U_t = A \\
   β, Y_t = 1, U_t = B 
\end{cases}\]</span></p>
<p>Thus, <span class="math display">\[\begin{align}
\EXP[ R_t(Y_t, U_t) | I_t, U_t ] &amp;=
\begin{cases}
   α s, &amp; \hbox{if } U_t = A \\
   β ξ(M_t, N_t), &amp; \hbox{if } U_t = B
\end{cases} \\
&amp;= \EXP[ R_t(Y_t, U_t) | (M_t, N_t), U_t ].
\end{align}\]</span> Thus, <span class="math inline">\((M_t, N_t)\)</span> is a sufficient statistic for evaluating current reward.</p>
<p>Therefore, <span class="math inline">\((M_t, N_t)\)</span> satisfies the two properties of information states. Hence, it is an information state.</p>
<p><strong>Part b.</strong> Based on the properties of the information state, we can write the dynamic program as follows.</p>
<p><span class="math display">\[V_{T+1}(m,n) = 0 \]</span> and for <span class="math inline">\(t \in \{T, \dots, 1\}\)</span>, we have <span class="math display">\[\begin{align}
   Q_t(m,n,A) &amp;= α s + V_{t+1}(m,n) \\
   Q_t(m,n,B) &amp;= β ξ(m,n) + ξ(m,n) V_{t+1}(m+1, n) + (1 - ξ(m,n)) 
   V_{t+1}(m, n+1) \\
   V_t(m, n) &amp;= \min\{ Q_t(m,n,A), Q_t(m,n,B) \}.
\end{align}\]</span></p>
<p><strong>Part c.</strong> First note that <span class="math display">\[\int_0^1 p^m (1-p)^n dp = B(m+1, n+1)\]</span> where <span class="math inline">\(B(m,n)\)</span> is the <a href="https://www.statlect.com/mathematical-tools/beta-function">Beta function</a> which is given by <span class="math display">\[B(m,n) = \frac{Γ(m) Γ(n)}{Γ(m+n)}\]</span> where <span class="math inline">\(Γ(\cdot)\)</span> is the <a href="https://www.statlect.com/mathematical-tools/gamma-function">Gamma function</a>. Thus, for integer valued <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>, <span class="math inline">\(Γ(m) = (m-1)!\)</span> and <span class="math inline">\(Γ(n) = (n-1)!\)</span>. Hence, <span class="math display">\[\int_0^1 p^m (1-p)^n dp = \frac{m! n!}{(m+n+1)!}\]</span>. Therefore, we have <span class="math display">\[\begin{align}
  ξ(m,n) &amp;= \frac{ (m+1)! n! }{ (m+n+1)! } 
  \frac{ (m+n)!}{ m! n!} \\
  &amp;= \frac{ m + 1 } { m + n +1 }.
\end{align}\]</span></p>
<p>See <a href="../07-2/">this Jupyter notebook</a> for the code for numerically solving the dynamic program.</p></li>
</ol>
<ol start="2" type="1">
<li><p>Consider the following modification of the sequential hypothesis testing problem considered in class. As in the model discussed in class, there are two hypothesis <span class="math inline">\(h_0\)</span> and <span class="math inline">\(h_1\)</span>. The a priori probability that the hypothesis is <span class="math inline">\(h_0\)</span> is <span class="math inline">\(p\)</span>.</p>
<p>In contrast to the model discussed in class, there are <span class="math inline">\(N\)</span> sensors. If sensor <span class="math inline">\(m\)</span> is used at time <span class="math inline">\(t\)</span> and the underlying hypothesis is <span class="math inline">\(h_i\)</span>, then the observation <span class="math inline">\(Y_t\)</span> is distrubted according to pdf (or pmf) <span class="math inline">\(f^m_j(y)\)</span>. The cost of using sensor <span class="math inline">\(m\)</span> is <span class="math inline">\(c_m\)</span>.</p>
<p>Whenever the decision maker takes a measurement, he picks a sensor <span class="math inline">\(m\)</span> uniformly at random from <span class="math inline">\(\{1, \dots, N\}\)</span> and observes <span class="math inline">\(Y_t\)</span> according to the distribution <span class="math inline">\(f^m_j(\cdot)\)</span> and incurs a cost <span class="math inline">\(c_m\)</span>.</p>
<p>The system continues for a finite time <span class="math inline">\(T\)</span>. At each time <span class="math inline">\(t &lt; T\)</span>, the decision maker has three options: stop and declare <span class="math inline">\(h_0\)</span>, stop and declare <span class="math inline">\(h_1\)</span>, or continue to take another measurement. At time <span class="math inline">\(T\)</span>, the continue alternative is unavailable.</p>
<ol type="a">
<li><p>Formulate the above problem as a POMDP. Identify an information state and write the dynamic programming decomposition for the problem.</p></li>
<li><p>Show that the optimal control law has a threshold property, similar to the threshold propertly for the model considered in class.</p></li>
</ol></li>
</ol>


<p class="categories">
This entry was last updated on
10 Apr 2018

</p>



      <script type="text/javascript">
      
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-6887174-4']);
            _gaq.push(['_trackPageview']);
            (function() {
                var ga   = document.createElement('script');
                ga.type  = 'text/javascript';
                ga.async = true;
                ga.src   = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
              })();
      
      </script>
    </div>
  </body>
</html>


