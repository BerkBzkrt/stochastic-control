<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" />
  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/github.css" type="text/css" />
  <script src="https://adityam.github.io/stochastic-control//js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script type="text/javascript"
    src="https://cdn.plot.ly/plotly-1.2.0.min.js">
  </script>
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_SVG">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "HTML-CSS": { 
          fonts: ["TeX"]
      }, 
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
    },
      TeX : {
          Macros : {
            PR: "\\mathbb{P}",
            EXP: "\\mathbb{E}",
            IND: "\\mathbb{I}",
            reals: "\\mathbb{R}",
            integers: "\\mathbb{Z}",
            TRANS: "\\intercal",
            VEC: "\\operatorname{vec}",
            TR:  "\\operatorname{Tr}",
            // mathcal: "\\mathscr",
            ALPHABET: ["\\mathcal{#1}", 1],
            MATRIX: ["\\begin{bmatrix} #1 \\end{bmatrix}", 1],
            NORM: ["\\left\\lVert #1 \\right\\rVert", 1],
            ABS: ["\\left\\lvert #1 \\right\\rvert", 1],
          }
      }
    });
  </script>
</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2018
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<h1>Assignment 7 (solution)</h1>

<ol type="1">
<li><p><strong>Part a.</strong> Let <span class="math inline">\(Y_t \in \{0, 1\}\)</span> denote the observation of the player where <span class="math inline">\(Y_t = 0\)</span> means that the output was a failure and <span class="math inline">\(Y_t = 1\)</span> means that the output was a success. Let <span class="math inline">\(I_t = \{Y_{1:t-1}, U_{1:t-1} \}\)</span> denote all the information available to the player.</p>
<p>From the definition of <span class="math inline">\((M_t, N_t)\)</span>, the evolution of <span class="math inline">\((M_t, N_t)\)</span> can be written as <span class="math display">\[ (M_{t+1}, N_{t+1}) = 
\begin{cases}
   (M_t, N_t), &amp; U_t = A \\
   (M_t + 1, N_t), &amp; U_t = B, Y_t = 1 \\
   (M_t , N_t + 1), &amp; U_t = B, Y_t = 0 
\end{cases}\]</span></p>
<p>Furthermore, the probability of next observation is given as <span class="math display">\[\PR(Y_t = 1 | I_t, U_t) = 
\begin{cases}
   s, &amp; U_t = A \\
   ξ(M_t, N_t), &amp; U_t = B 
\end{cases}\]</span></p>
<p>Combing the two, we can write <span class="math display">\[\begin{align}
\PR( (M_{t+1}, N_{t+1}) | I_t, U_t) &amp;=
\begin{cases}
   1, &amp; \hbox{if } U_t = A \hbox{ and } (M_{t+1}, N_{t+1}) = (M_t, N_t) \\
   ξ(M_t, N_t), &amp; \hbox{if } U_t = B \hbox{ and } (M_{t+1}, N_{t+1}) = (M_t
   + 1, N_t) \\
   1 - ξ(M_t, N_t), &amp; \hbox{if } U_t = B \hbox{ and } (M_{t+1}, N_{t+1}) =
   (M_t, N_t + 1) 
\end{cases} \\
&amp;= 
\PR( (M_{t+1}, N_{t+1}) | (M_t, N_t), U_t)
\end{align}\]</span> Thus, <span class="math inline">\((M_t, N_t)\)</span> evolves in a controlled Markovian manner.</p>
<p>Now, let’s consider the per-step reward. <span class="math display">\[R_t(Y_t, U_t) = 
\begin{cases}
   α, Y_t = 1, U_t = A \\
   β, Y_t = 1, U_t = B 
\end{cases}\]</span></p>
<p>Thus, <span class="math display">\[\begin{align}
\EXP[ R_t(Y_t, U_t) | I_t, U_t ] &amp;=
\begin{cases}
   α s, &amp; \hbox{if } U_t = A \\
   β ξ(M_t, N_t), &amp; \hbox{if } U_t = B
\end{cases} \\
&amp;= \EXP[ R_t(Y_t, U_t) | (M_t, N_t), U_t ].
\end{align}\]</span> Thus, <span class="math inline">\((M_t, N_t)\)</span> is a sufficient statistic for evaluating current reward.</p>
<p>Therefore, <span class="math inline">\((M_t, N_t)\)</span> satisfies the two properties of information states. Hence, it is an information state.</p>
<p><strong>Part b.</strong> Based on the properties of the information state, we can write the dynamic program as follows.</p>
<p><span class="math display">\[V_{T+1}(m,n) = 0 \]</span> and for <span class="math inline">\(t \in \{T, \dots, 1\}\)</span>, we have <span class="math display">\[\begin{align}
   Q_t(m,n,A) &amp;= α s + V_{t+1}(m,n) \\
   Q_t(m,n,B) &amp;= β ξ(m,n) + ξ(m,n) V_{t+1}(m+1, n) + (1 - ξ(m,n)) 
   V_{t+1}(m, n+1) \\
   V_t(m, n) &amp;= \max\{ Q_t(m,n,A), Q_t(m,n,B) \}.
\end{align}\]</span></p>
<p><strong>Part c.</strong> First note that <span class="math display">\[\int_0^1 p^m (1-p)^n dp = B(m+1, n+1)\]</span> where <span class="math inline">\(B(m,n)\)</span> is the <a href="https://www.statlect.com/mathematical-tools/beta-function">Beta function</a> which is given by <span class="math display">\[B(m,n) = \frac{Γ(m) Γ(n)}{Γ(m+n)}\]</span> where <span class="math inline">\(Γ(\cdot)\)</span> is the <a href="https://www.statlect.com/mathematical-tools/gamma-function">Gamma function</a>. Thus, for integer valued <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>, <span class="math inline">\(Γ(m) = (m-1)!\)</span> and <span class="math inline">\(Γ(n) = (n-1)!\)</span>. Hence, <span class="math display">\[\int_0^1 p^m (1-p)^n dp = \frac{m! n!}{(m+n+1)!}\]</span>. Therefore, we have <span class="math display">\[\begin{align}
  ξ(m,n) &amp;= \frac{ (m+1)! n! }{ (m+n+2)! } 
  \frac{ (m+n+1)!}{ m! n!} \\
  &amp;= \frac{ m + 1 } { m + n +2 }.
\end{align}\]</span></p>
<p>See <a href="../07-2/">this Jupyter notebook</a> for the code for numerically solving the dynamic program.</p></li>
</ol>


<p class="categories">
</p>



      <script type="text/javascript">
      
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-6887174-4']);
            _gaq.push(['_trackPageview']);
            (function() {
                var ga   = document.createElement('script');
                ga.type  = 'text/javascript';
                ga.async = true;
                ga.src   = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
              })();
      
      </script>
    </div>
  </body>
</html>


