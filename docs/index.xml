<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/</link>
    <description>Recent content on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://adityam.github.io/stochastic-control/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Dynamic programming for general models</title>
      <link>https://adityam.github.io/stochastic-control/pomdp/information-state/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/pomdp/information-state/</guid>
      <description>Consider a general controlled input output system with two inputs—control action \(U_t\) and disturbance \(W_t\)—and two outputs—observation \(Y_t\) and cost \(C_t\). We assume that the system has an initial state, which we denote by \(X_1\), and assume to be random. We do not assume a state-space model for the system, but simply assume an input-ouput model, i.e., there exist functions \(\{f_t\}_{t \ge 1}\) and \(\{h_t\}_{t \ge 1}\) such that1 \[\begin{equation}\label{eq:Y} Y_t = f_t(X_1, U_{1:t-1}, W_{1:t-1}) \end{equation}\] and \[\begin{equation}\label{eq:C} C_t = h_t(X_1, U_{1:t}, W_{1:t-1}).</description>
    </item>
    
    <item>
      <title>Infinite horizon discounted MDP</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/discounted-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/discounted-mdp/</guid>
      <description>Note: Throughout this section, we assume that \(\ALPHABET X\) and \(\ALPHABET U\) are finite and \(|\ALPHABET X|= n\) and \(|\ALPHABET U| = m\).
Performance of a time-homogeneous Markov policy For any \(g \colon \ALPHABET X \to \ALPHABET U\), consider the time homogeneous policy \((g, g, \dots)\). For ease of notation, we denote this policy simply by \(g\). The expected discounted cost under this policy is given by \[ V_g(x) = \EXP^g\bigg[ \sum_{t=1}^∞ β^{t-1} c(X_t, U_t) \biggm| X_1 = x \bigg].</description>
    </item>
    
    <item>
      <title>Linear Quadratic Regulation (LQR)</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/lqr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/lqr/</guid>
      <description>Note: To be consistent with the notation used in linear systems, we denote the state and action by lowercase \(x\) and \(u\), even for stochastic systems (unlike the notation used for other models where we use uppercase \(X\) and \(U\) for state and actions to emphasize the fact they are random variables).
We start by considering a determinisitc linear system with state \(x_t \in \reals^n\) and control actions \(u_t \in \reals^m\) and dynamics \[ x_{t+1} = A_t x_t + B_t u_t,\] where \(A_t \in \reals^{n \times n}\) and \(B_t \in \reals^{n \times m}\) are known matrices.</description>
    </item>
    
    <item>
      <title>Prelim: Stochastic optimization</title>
      <link>https://adityam.github.io/stochastic-control/stochastic/stochastic-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/stochastic/stochastic-optimization/</guid>
      <description>Let’s start with the simplest optimization problem. A decision maker has to choose an action \(u \in \ALPHABET U\). Upon choosing the action \(u\), the decision maker incurs a cost \(c(u)\). What action should the decision maker pick to minimize the cost?
Formally, the above optimization problem may be written as \[ \begin{equation} \label{eq:basic} \min_{u \in \ALPHABET U} c(u). \end{equation}\]
When the action space \(\ALPHABET U\) is finite, say \(\ALPHABET U = \{1, \dots, m\}\), solving the optimization problem \eqref{eq:basic} is conceptually straight-forward: enumerate the cost of all possible actions, i.</description>
    </item>
    
    <item>
      <title>Theory: Optimality of threshold policies in optimal stopping</title>
      <link>https://adityam.github.io/stochastic-control/optimal-stopping/monotonicity-optimal-stopping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/optimal-stopping/monotonicity-optimal-stopping/</guid>
      <description>Let \(\{X_t\}_{t \ge 1}\) be a Markov chain. At each time \(t\), a decision maker observes the state \(X_t\) of the Markov chain and decides whether to continue or stop the process. If the decision maker decides to continue, he incurs a continuation cost \(c_t(X_t)\) and the state evolves. If the DM decides to stop, he incurs a stopping cost of \(s_t(X_t)\) and the problem is terminated. The objective is to determine an optimal stopping time \(\tau\) to minimize \[J(\tau) := \EXP\bigg[ \sum_{t=1}^{\tau-1} c_t(X_t) + s_\tau(X_\tau) \bigg].</description>
    </item>
    
    <item>
      <title>Example: Inventory Management (revisited)</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/inventory-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/inventory-management/</guid>
      <description>TL;DR One of the potential benefits of modeling a system as infinite horizon discounted cost MDP is that it can be simpler to identify an optimal policy. We illustrate this using the inventory management example.
Consider the model for inventory management and assume that it runs for an infinite horizon. We assume that the per-step cost is given by \[c(x,u,x_{+}) = p u + \beta h(x), \] where \[ h(x) = \begin{cases} ax, &amp;amp; \text{if $x \ge 0$} \\ -bx, &amp;amp; \text{if $x &amp;lt; 0$}, \end{cases}\] where \(a\) is the per-unit holding cost, \(b\) is the per-unit shortage cost, and \(p\) is the per-unit procurement cost.</description>
    </item>
    
    <item>
      <title>Example: The Newsvendor Problem</title>
      <link>https://adityam.github.io/stochastic-control/stochastic/newsvendor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/stochastic/newsvendor/</guid>
      <description>Image credit: https://americangallery.wordpress.com/category/cafferty-james-h/  TL;DR The newsvendor problem is a simple model of stochastic optimization problem where a decision has to be made when there is uncertainty about the outcome. It also shows that for some stochastic optimization problems it is possible to obtain the qualitative properties of the nature of optimal solution.
Each morning, a newsvendor has to decide how many newspapers to buy before knowing the demand during the day.</description>
    </item>
    
    <item>
      <title>Lipschitz MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/lipschitz-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/lipschitz-mdp/</guid>
      <description>Preliminaries Lipschitz continuous functions Given two metric spaces \((\ALPHABET X, d_X)\) and \((\ALPHABET Y, d_Y)\), the Lipschitz constant of function \(f \colon \ALPHABET X \to \ALPHABET Y\) is defined by \[ \| f\|_{L} = \sup_{x_1 \neq x_2} \left\{ \frac{ d_Y(f(x_1), f(x_2)) } { d_X(x_1, x_2) } : x_1, x_2 \in \ALPHABET X \right\} \in [0, ∞]. \] The function is called Lipschitz continuous if its Lipschitz constant is finite.
Intuitively, a Lipschitz continuous function is limited by how fast it can change.</description>
    </item>
    
    <item>
      <title>State aggregation in MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/state-aggregation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/state-aggregation/</guid>
      <description>State quantization Consider an MDP with continuous state space \(\ALPHABET X\) and finite action space \(\ALPHABET U\). We denote this MDP by \(M = (\ALPHABET X, \ALPHABET U, c, p)\), where for simplicity we assume that the \(p\) is the density of the transition kernel.
Since the state space is continuous, in general, we cannot compute the value functions exactly. The simplest way to proceed is to discretize the state space \(\ALPHABET X\).</description>
    </item>
    
    <item>
      <title>Approximate dynamic programming</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/adp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/adp/</guid>
      <description>The value and policy iteration algorithms for discounted cost MDPs rely on exact computation of the Bellman update \(W = \mathcal B V\) and the corresponding optimal policy \(g\) such that \(\mathcal B V = \mathcal B_g V\). Suppose we cannot compute these updates exactly, but can find approximate solutions \(W\) and \(g\) such that \[ \NORM{W - \mathcal B V} \le δ \quad\text{and}\quad \NORM{\mathcal B_g V - \mathcal B V} \le ε\] where \(δ\) and \(ε\) are positive constants.</description>
    </item>
    
    <item>
      <title>Example: Optimal choice of the best alternative</title>
      <link>https://adityam.github.io/stochastic-control/optimal-stopping/optimal-choice/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/optimal-stopping/optimal-choice/</guid>
      <description>A decision maker (DM) wants to select the best alternative from a set of \(T\) alternatives. The DM evaluates the alternatives sequentially. After evaluating alternative \(t\), the DM knows whether alternative \(t\) was the best alternative so far or not. Based on this information, the DM must decide whether to choose alternative \(t\) and stop the search, or to permanently reject alternative \(t\) and evaluate remaining alternatives. The DM may reject the last alternative and not make a choice at all.</description>
    </item>
    
    <item>
      <title>Theory: Basic model of an MDP</title>
      <link>https://adityam.github.io/stochastic-control/mdp/mdp-functional/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/mdp-functional/</guid>
      <description>Markov decision processes (MDP) are the simplest model of a stochastic control system. The dynamic behavior of an MDP is modeled by an equation of the form \[ X_{t+1} = f_t(X_t, U_t, W_t) \] where \(X_t \in \ALPHABET X\) is the state, \(U_t \in \ALPHABET U\) is the control input, and \(W_t \in \ALPHABET W\) is the noise. An agent/controller observes the state and chooses the control input \(U_t\).
The controller can be as sophisticated as we want.</description>
    </item>
    
    <item>
      <title>Example: Call options</title>
      <link>https://adityam.github.io/stochastic-control/optimal-stopping/call-options/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/optimal-stopping/call-options/</guid>
      <description>An investor has a call option to buy one share of a stock at a fixed price \(p\) dollars and has \(T\) days to exercise this option. For simplicity, we assume that the investor makes a decision at the beginning of each day.
The investory may decide not to exercise the option but if he does exercise the option when the stock price is \(x\), he effectively gets \((x-p)\).
Assume that the price of the stoch varies with independent increments, i.</description>
    </item>
    
    <item>
      <title>Example: Optimal Gambling</title>
      <link>https://adityam.github.io/stochastic-control/mdp/optimal-gambling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/optimal-gambling/</guid>
      <description>Image credit: http://commons.wikimedia.org/wiki/File:Gambling-ca-1800.jpg  TL;DR This stylized model of optimal gambling was introduced by Kelly (1956) to highlight a relationship between channel capacity (which had been proposed recently by Shannon), and gambling. Our motivation for studying this model is to use it as an illustrative example to show that sometimes it is possible to identify the optimal strategy and value function of MDPs in closed form.
Imagine a gambler who goes to a casino with an initial fortune of \(x_1\) dollars and places bets over time and must leave after \(T\) bets.</description>
    </item>
    
    <item>
      <title>Example: Inventory Management</title>
      <link>https://adityam.github.io/stochastic-control/mdp/inventory-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/inventory-management/</guid>
      <description>Image credit: https://hbr.org/2015/06/inventory-management-in-the-age-of-big-data  TL;DR The inventory management example illustrates that a dynamic programming formulation is useful even when a closed form solution does not exist. This model also introduces the idea of post-decision state, which is useful in many contexts.
Imagine a retail store that stockpiles products in its warehouse to meet random demand. Suppose the store procures new stocks at the end of each day (and that there is no lead time and stocks are available next morning).</description>
    </item>
    
    <item>
      <title>Numerics: Matrix formulation of Markov decision processes</title>
      <link>https://adityam.github.io/stochastic-control/mdp/mdp-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/mdp-matrix/</guid>
      <description>Image credit: https://pixabay.com/photos/accountant-accounting-adviser-1238598/  In this section, we present a matrix formulation for finite state finite action MDPs, which is useful for computing the solutions numerically. Let’s start with the function model described earlier and assume that \(\ALPHABET X\) and \(\ALPHABET U\) are finite sets and that the cost function and the probability distribution of \(\{W_t\}_{t \ge 1}\) are time-homogeneous. Then, the following is a fundamental property of MDPs:</description>
    </item>
    
    <item>
      <title>Prelim: Stochastic dominance</title>
      <link>https://adityam.github.io/stochastic-control/mdp/stochastic-dominance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/stochastic-dominance/</guid>
      <description>Stochastic dominance is a partial order on random variables. Let \(\ALPHABET X\) be a totally ordered finite set, say \(\{1, \dots, n\}\) and let \(\Delta(\ALPHABET X)\) denote the state of pmfs on \(\ALPHABET X\).
 Definition Suppose \(X\) and \(Y\) are \(\ALPHABET X\) valued random variables where \(X \sim \pi\) and \(Y \sim \mu\). We say \(X\) stochastically dominates \(Y\) if for any \(x \in \ALPHABET X\), \[\begin{equation}\label{eq:inc-prob} \PR(X \ge x) \ge \PR(Y \ge x).</description>
    </item>
    
    <item>
      <title>Theory: Monotone value functions and policies</title>
      <link>https://adityam.github.io/stochastic-control/mdp/monotonicity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/monotonicity/</guid>
      <description>Consider the matrix formulation of MDPs and suppose the state space \(\ALPHABET X\) is totally ordered. In many applications, it is useful to know if the value function is increasing (or decreasing) in state.
 Theorem Consider an MDP where the state space \(\ALPHABET X\) is totally ordered. Suppose the following conditions are satisfied.
C1. For every \(u \in \ALPHABET U\), the per-step cost \(c_t(x,u)\) is weakly inceasing in \(x\).</description>
    </item>
    
    <item>
      <title>Example: Power-delay trade-off in wireless communication</title>
      <link>https://adityam.github.io/stochastic-control/mdp/power-delay-tradeoff/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/power-delay-tradeoff/</guid>
      <description>TL;DR This stylized example of power-delay trade-off in wireless communications illustrates that a dynamic programming formulation can be used to identify qualitative properties of the value function and optimal policies.
In a cell phone, higher layer applications such as voicecall, email, browsers, etc. generate data packets. These packets are buffered in a queue and the transmission protocol decides how many packets to transmit at each time depending the number of packets in the queue and the quality of the wireless channel.</description>
    </item>
    
    <item>
      <title>Example: Power-delay trade-off in wireless communication (2nd version)</title>
      <link>https://adityam.github.io/stochastic-control/mdp/power-delay-tradeoff-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/power-delay-tradeoff-2/</guid>
      <description>TL;DR This stylized example of power-delay trade-off in wireless communications illustrates that a dynamic programming formulation can be used to identify qualitative properties of the value function and optimal policies.
In a cell phone, higher layer applications such as voicecall, email, browsers, etc. generate data packets. These packets are buffered in a queue and the transmission protocol decides how many packets to transmit at each time depending the number of packets in the queue and the quality of the wireless channel.</description>
    </item>
    
    <item>
      <title>Theory: Reward Shaping</title>
      <link>https://adityam.github.io/stochastic-control/mdp/reward-shaping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/reward-shaping/</guid>
      <description>What are the conditions under which two MDPs which have the same dynamics but different cost functions have the same optimal policy? This is an important question in reinforcement learning (where one often shapes the reward function to speed up learning) and inverse reinforcement learning (where one learns the reward function from the behavior of an expert). The following result provides a complete answer to this question.
Let \(M^1\) and \(M^2\) denote two MDPs on the same state space \(\ALPHABET X\) and action space \(\ALPHABET U\).</description>
    </item>
    
    <item>
      <title>Vectorization</title>
      <link>https://adityam.github.io/stochastic-control/appendix/vectorization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/vectorization/</guid>
      <description>Vectorization is a linear transformation that converts a matrix to a column vector. For example, \[\VEC\left(\MATRIX{a &amp;amp; b \\ c &amp;amp; d }\right) = \MATRIX{a \\ c \\ b \\ d}.\]
Vectorization is often used to express matrix multiplication as a linear transformation on matrices. In particular, we have the following three properties:
\(\VEC(ABC) = (C^\TRANS \otimes A) \VEC(B).\) \(\VEC(ABC) = (I \otimes AB)\VEC(C).\) \(\VEC(ABC) = (C^\TRANS B^\TRANS \otimes I)\VEC(A).\)  Another useful formulation is the following</description>
    </item>
    
    <item>
      <title>Positive definite matrices</title>
      <link>https://adityam.github.io/stochastic-control/appendix/positive-definite-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/positive-definite-matrix/</guid>
      <description>Definition A \(n \times n\) symmetric matrix \(M\) is called
 positive definite (written as \(M &amp;gt; 0\)) if for all \(x \in \reals^n\), \(x \neq 0\), we have \[x^\TRANS M x &amp;gt; 0.\]
 positive semi definite (written as \(M \ge 0\)) if for all \(x \in \reals^n\), \(x \neq 0\), we have \[x^\TRANS M x \ge 0.\]
    A symmetric matrix is positive definite (respt.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://adityam.github.io/stochastic-control/coursework/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/coursework/</guid>
      <description>Project  Project report due on April 13, 2018. To be done either individually, or in teams of two.
 In class 15-minute project presentation on April 11 and 13, 2018.
 Grading scheme for the presentation
 Background and presentation of the problem setting: 20% Summary of the main results: 60% Clarity of the presentation: 10% Time management: 10%  Grading scheme for the report (10–15 pages)
 Background, presentation of the problem setting, and literature overview: 20% Summary of the results, proof outlines, and examples to illustrate the main results: 50% Critical evaluation of the contributions: 20% Clarity of the presentation: 10%     Project topics</description>
    </item>
    
    <item>
      <title></title>
      <link>https://adityam.github.io/stochastic-control/solutions/03-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/03-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://adityam.github.io/stochastic-control/solutions/05-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/05-2/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://adityam.github.io/stochastic-control/solutions/06-2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/06-2/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://adityam.github.io/stochastic-control/solutions/07-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/07-1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Assignment 1</title>
      <link>https://adityam.github.io/stochastic-control/assignments/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/01/</guid>
      <description>Exercise 1 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.
 Exercise 2 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.
 Selling random wind. The amount \(W\) of power generated by the wind turbine is a positive real-valued random variable with probability density function \(f\).</description>
    </item>
    
    <item>
      <title>Assignment 1 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/01/</guid>
      <description>Problem 1 We first observe a simplification that can be done for this model, which makes the calculations a little easier. In principle, we need to compute \[ Q(x,u) = \EXP[ c(x,u,W) | X = x] = \sum_{w \in \ALPHABET W} P(W = w | X = x) c(x,u,w). \]
This means that we need to compute \(P(W|X)\). However, we can avoid that step by observing that \[P(W|X) = \dfrac{ P(X,W) }{ P(X) }.</description>
    </item>
    
    <item>
      <title>Assignment 2</title>
      <link>https://adityam.github.io/stochastic-control/assignments/02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/02/</guid>
      <description>AM-GM inequality. Consider the problem of maximizing \[ V_t(a) = \max_{(u_1, \dots, u_t) \in \ALPHABET S_t(a)} u_1 ⋅ u_2 \cdots u_t \] where the region \(\ALPHABET S_t(a)\) is determined by the conditions \[ \sum_{i=1}^t u_i = a \quad \text{and} \quad u_i \ge 0. \]
Prove that \(V_t\) satisfies the following recursion \[ V_t(a) = \max_{u \in [0,a]} u ⋅ V_{t- 1}(a - u), \quad t &amp;gt; 1 \] with \(V_1(a) = a\).</description>
    </item>
    
    <item>
      <title>Assignment 2 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/02/</guid>
      <description>Problem 1 The result follows by writing the maximization in a nested manner \[\begin{align*} V_t(a) &amp;amp;= \max_{(u_1, \dots u_t) \ge 0 : \sum_{i=1}^t u_i = a} u_1 \cdots u_t \\ &amp;amp;= \max_{u_t \in [0,a]} \Bigl\{ \max_{(u_1, \dots, u_{t-1}) \ge 0 : \sum_{i=1}^{t-1} u_i = a-u_t} u_1 \cdots u_{t-1} \Bigr\} \cdot u_t \\ &amp;amp; = \max_{u_t \in [0,a]} V_t(a-u_t) \cdot u_t \end{align*}\]
 We use induction to show that \(V_t(a) = (a/t)^t\).</description>
    </item>
    
    <item>
      <title>Assignment 3</title>
      <link>https://adityam.github.io/stochastic-control/assignments/03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/03/</guid>
      <description>Service rate control in queueing systems
Consider a queueing system in which jobs arrive according to a i.i.d. Bernoulli process \(\{A_t\}_{t \ge 1}\), where \(\PR(A_t = 1) = q\). The jobs are queued in a buffer of size \(n\). Let \(X_t \in \{0, 1, \dots, n\}\) denote the number of jobs in the queue at time \(t\).
At each time, if there are jobs in the queue, the server works on the job at the head of the queue at service rate \(U_t \in \{0, 1, \dots, m\}\) and finishes the job with probability \(p(u)\).</description>
    </item>
    
    <item>
      <title>Assignment 3 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/03/</guid>
      <description>Problem 1 The code for computing the optimal policy is available here. The code is written for ease of reading, not efficiency.
# State spaces n, m = 8, 3 X = 1:n+1 U = 1:m+1 # Transition probability q = 0.6 p = [0.0,0.25,0.5,0.8] P = [ zeros(n+1,n+1) for u in U ] # We are using 1-indexed arrays, so need to be careful with the indices for u in U @views Pu = P[u] for x in X if x == 1 Pu[x,x] = 1 - q Pu[x,x+1] = q elseif x == n+1 Pu[x,x-1] = (1-q)*p[u] Pu[x,x ] = 1 - (1-q)*p[u] else Pu[x,x-1] = (1-q)*p[u] Pu[x,x ] = (1-q)*(1-p[u]) + q*p[u] Pu[x,x+1] = q*(1-p[u]) end end end P_concat = vcat(P.</description>
    </item>
    
    <item>
      <title>Assignment 4</title>
      <link>https://adityam.github.io/stochastic-control/assignments/04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/04/</guid>
      <description>Exercise 1 of the notes on power-delay trade-off in wireless communication.
 Exercise 1 (parts a–c; part d is for reference and you don’t need to submit a solution for part d) of the notes on monotonicity in MDPs.
  </description>
    </item>
    
    <item>
      <title>Assignment 4 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/04/</guid>
      <description>Problem 1 I had a solution to this problem but the TA found a mistake in my solution. Having thought about it a bit, I realize that as stated the problem cannot be solved.
There are urban legends of students solving unsolved problems thinking they were homework questions. That was not my intention! Let me attempt to explain what led to the mistake and how it can be fixed. This requires some background knowledge of communication systems.</description>
    </item>
    
    <item>
      <title>Assignment 5</title>
      <link>https://adityam.github.io/stochastic-control/assignments/05/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/05/</guid>
      <description>Exercise 2 of the notes on monotonicity of optimal stopping.
 Exercise 3 of the notes on monotonicity of optimal stopping.
  </description>
    </item>
    
    <item>
      <title>Assignment 5 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/05/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/05/</guid>
      <description>Problem 1 For the reward maximization case, the dynamic program is given as follows:
\[V_T(x) = s_T(x)\] and for \(t \in \{T-1,\dots, 1\}\), we have \[V_t(x) = \max\{ s_t(x), c_t(x) + \EXP[V_{t+1}(X_{t+1} | X_t = x]. \]
We define the benefit funciton as before. i.e., \[B_t(x) = c_t(x) + \EXP[ V_{t+1}(X_{t+1}) | X_t = x] - s_t(x). \] Then, it is optimal to stop whenever \(B_t(x) \le 0\).
Now, we write the value function in terms of the benefit function as follows:</description>
    </item>
    
    <item>
      <title>Assignment 6</title>
      <link>https://adityam.github.io/stochastic-control/assignments/06/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/06/</guid>
      <description>Exercise 1 of note on discounted mdp
 Consider the infinite horizon version rate control problem described in Assignment 3 with discount factor \(\beta = 0.9\).
Use value iteration to find a policy \(g\) such that \(\NORM{V_g - V^*} \le 10^{-5}\). How many iterations does it take to converge? Plot the value function at the last step of the iteration. Print the optimal policy.
 Use policy iteration to find the optimal policy \(g\).</description>
    </item>
    
    <item>
      <title>Assignment 7</title>
      <link>https://adityam.github.io/stochastic-control/assignments/07/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/07/</guid>
      <description>Exercise 1 of notes on Lipschitz MDPs.  The purpose of this exercise is to justify the truncation of the state space, as we did in in the example of state transmission in Internet of Things considered in Assignment 3. Consider the infinite horizon discounted cost version of that example.
In Exercise 1 of monotone MDPs, we have already established that the optimal policy is even and quasi-convex. Since the action space is binary, this means that the optimal policy is of the form \[ g(e) = \begin{cases} 0, &amp;amp; |e| \le k, \\ 1, &amp;amp; |e| &amp;gt; k.</description>
    </item>
    
    <item>
      <title>Assignment 7 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/07/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/07/</guid>
      <description>Part a. Let \(Y_t \in \{0, 1\}\) denote the observation of the player where \(Y_t = 0\) means that the output was a failure and \(Y_t = 1\) means that the output was a success. Let \(I_t = \{Y_{1:t-1}, U_{1:t-1} \}\) denote all the information available to the player.
From the definition of \((M_t, N_t)\), the evolution of \((M_t, N_t)\) can be written as \[ (M_{t+1}, N_{t+1}) = \begin{cases} (M_t, N_t), &amp;amp; U_t = A \\ (M_t + 1, N_t), &amp;amp; U_t = B, Y_t = 1 \\ (M_t , N_t + 1), &amp;amp; U_t = B, Y_t = 0 \end{cases}\]</description>
    </item>
    
    <item>
      <title>Course Notes</title>
      <link>https://adityam.github.io/stochastic-control/notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/notes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lectures</title>
      <link>https://adityam.github.io/stochastic-control/lectures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/lectures/</guid>
      <description>Whenever possible, I will post notes on some of the material covered in class, but that is not guaranteed. This is a graduate class and you are responsible for taking notes in class and reading the appropriate chapters of the textbooks.
The notes will be updated as we move along in the course. Please check the dates on the first page to keep track. If you find any typos/mistakes in the notes, please let me know.</description>
    </item>
    
    <item>
      <title>Prelim: Change of Measure</title>
      <link>https://adityam.github.io/stochastic-control/risk-sensitive/change-of-measure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/risk-sensitive/change-of-measure/</guid>
      <description>Change of measure of a single random variable.  Theorem 1 Let \((\Omega, \mathcal F, P)\) be a probability space and \(\Lambda\) be an almost surely non-negative random variable such that \(\EXP[\Lambda] = 1\). For any \(A \in \mathcal F\), define \[ P^\dagger(A) = \int_A \Lambda(\omega) dP(\omega). \] Then,
 \(P^\dagger\) is a probability measure. For any random variable \(X\), \[ \EXP^\dagger[X] = \EXP[ \Lambda X]. \] If \(\Lambda\) is almost surely positive, then \[ \EXP[X] = \EXP^\dagger \left[ \frac{X}{\Lambda} \right].</description>
    </item>
    
  </channel>
</rss>