<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/</link>
    <description>Recent content on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://adityam.github.io/stochastic-control/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Optimal Gambling</title>
      <link>https://adityam.github.io/stochastic-control/examples/optimal-gambling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/examples/optimal-gambling/</guid>
      <description>TL;DR This stylized model of optimal gambling was introduced by Kelly (1956) to highlight a relationship between channel capacity (which had been proposed recently by Shannon), and gambling. Our motivation for studying this model is to use it as an illustrative example to show that sometimes it is possible to identify the optimal strategy and value function of MDPs in closed form.
Imagine a gambler who goes to a casino with an initial fortune of \(x_1\) dollars and places bets over time and must leave after \(T\) bets.</description>
    </item>
    
    <item>
      <title>Stochastic optimization</title>
      <link>https://adityam.github.io/stochastic-control/theory/stochastic-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/stochastic-optimization/</guid>
      <description>Let’s start with a simple stochastic optimization problem. Suppose \(\ALPHABET X\), \(\ALPHABET U\), \(\ALPHABET W\) are finite sets and let \(X \in \ALPHABET X\) and \(W \in \ALPHABET W\) be random variables defined on a common probability space. A decision maker observes the realization of \(X\) and chooses and action \(U \in \ALPHABET U\) according to a decison rule \(g\), i.e., \[ U = g(X) \] and incurs a cost \[J(g) = \EXP[ c(X, g(X), W) ],\] where the expectation is taken with respect to the joint probability distribution of \((X,W)\).</description>
    </item>
    
    <item>
      <title>Blackwell&#39;s principle of irrelevant information</title>
      <link>https://adityam.github.io/stochastic-control/theory/principle-of-irrelevant-information/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/principle-of-irrelevant-information/</guid>
      <description>Theorem Let \(\ALPHABET X\), \(\ALPHABET Y\), and \(\ALPHABET U\) be standard Borel spaces and \(X \in \ALPHABET X\) and \(Y \in \ALPHABET Y\) be random variables defined on a common probability space.
A decision maker observes \((X,Y)\) and chooses \(U\) to minimize \(\EXP[c(X,U)]\), where \(c \colon \ALPHABET X \times \ALPHABET U \to \reals\) is a measurable function.
Then, there is no loss of optimality in choosing \(U\) only as a function of \(X\).</description>
    </item>
    
    <item>
      <title>Inventory Management</title>
      <link>https://adityam.github.io/stochastic-control/examples/inventory-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/examples/inventory-management/</guid>
      <description>TL;DR The inventory management example illustrates that a dynamic programming formulation is useful even when a closed form solution does not exist. This model also introduces the idea of post-decision state, which is useful in many contexts.
Imagine a retail store that stockpiles products in its warehouse to meet random demand. Suppose the store procures new stocks at the end of each day (and that there is no lead time and stocks are available next morning).</description>
    </item>
    
    <item>
      <title>Markov decision processes</title>
      <link>https://adityam.github.io/stochastic-control/theory/mdp-functional/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/mdp-functional/</guid>
      <description>Markov decision processes (MDP) model model the simplest stochastic control architecture. The dynamic behavior of an MDP is modeled by an equation of the form \[ X_{t+1} = f_t(X_t, U_t, W_t) \] where \(X_t \in \ALPHABET X\) is the state, \(U_t \in \ALPHABET U\) is the control input, and \(W_t \in \ALPHABET W\) is the noise. An agent/controller observes the state and chooses the control input \(U_t\).
The controller can be as sophisticated as we want.</description>
    </item>
    
    <item>
      <title>Matrix formulation of Markov decision processes</title>
      <link>https://adityam.github.io/stochastic-control/theory/mdp-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/mdp-matrix/</guid>
      <description>In this section, we present a matrix formulation for finite state finite action MDPs, which is useful for computing the solutions numerically. Let’s start with the function model described earlier and assume that \(\ALPHABET X\) and \(\ALPHABET U\) are finite sets and that the cost function and the probability distribution of \(\{W_t\}_{t \ge 1}\) are time-homogeneous. Then, the following is a fundamental property of MDPs:
 Lemma For any \(x_1, x_2, \dots, x_T \in \ALPHABET X\) and \(u_1, \dots, u_T \in \ALPHABET U\), we have \[ \PR(X_{t+1} = x_{t+1} | X_{1:t} = x_{1:t}, U_{1:t} = u_{1:t}) = \PR(X_{t+1} = x_{t+1} | X_{t} = x_t , U_t = u_t).</description>
    </item>
    
    <item>
      <title>Linear Quadratic Regulation (LQR)</title>
      <link>https://adityam.github.io/stochastic-control/theory/lqr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/lqr/</guid>
      <description>Note: To be consistent with the notation used in linear systems, we denote the state and action by lowercase \(x\) and \(u\), even for stochastic systems (unlike the notation used for other models where we use uppercase \(X\) and \(U\) for state and actions to emphasize the fact they are random variables.
We start by considering a determinisitc linear system with state \(x_t \in \reals^n\) and control actions \(u_t \in \reals^m\) and dynamics \[ x_{t+1} = A_t x_t + B_t u_t,\] where \(A_t \in \reals^{n \times n}\) and \(B_t \in \reals^{n \times m}\) are known matrices.</description>
    </item>
    
    <item>
      <title>Stochastic dominance</title>
      <link>https://adityam.github.io/stochastic-control/theory/stochastic-dominance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/stochastic-dominance/</guid>
      <description>Stochastic dominance is a partial order on random variables. Let \(\ALPHABET X\) be a totally ordered finite set, say \(\{1, \dots, n\}\) and let \(\Delta(\ALPHABET X)\) denote the state of pmfs on \(\ALPHABET X\).
 Definition Suppose \(X\) and \(Y\) are \(\ALPHABET X\) valued random variables where \(X \sim \pi\) and \(Y \sim \mu\). We say \(X\) stochastically dominates \(Y\) if for any \(x \in \ALPHABET X\), \[ \PR(X \ge x) \ge \PR(Y \ge x).</description>
    </item>
    
    <item>
      <title></title>
      <link>https://adityam.github.io/stochastic-control/coursework/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/coursework/</guid>
      <description> Project  To be announced    </description>
    </item>
    
    <item>
      <title>Assignment 1</title>
      <link>https://adityam.github.io/stochastic-control/assignments/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/01/</guid>
      <description>Execrise 1 from the notes on stochastic optimization. You may either do the caculations by hand or write a computer program in any language of your choice to find the optimal policy. Show the intermediate steps in both cases. If you solve the problem by writing a computer program, you must submit your code along with your solution.
 Exercises 3 from the notes on optimal gambling. You may assume the results of Exercises 1 and 2.</description>
    </item>
    
    <item>
      <title>Assignment 2</title>
      <link>https://adityam.github.io/stochastic-control/assignments/02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/02/</guid>
      <description>Monotonicity in time. Suppose that the per-step cost \(c_t(\cdot)\) is time-homogeneous (except, possibly at the terminal time \(t=T\)). Suppose we have that \(J_{T-1}(x) \le J_T(x)\) for all \(x \in \ALPHABET S\). Then, show that \[ J_{t}(x) \le J_{t+1}(x), \quad \text{for all $x \in \ALPHABET X$ and $t$}.\]
Similarly, if we have that \(J_{T-1}(x) \ge J_T(x)\) for all \(x \in \ALPHABET X\), then \[ J_{t}(x) \ge J_{t+1}(x), \quad \text{for all $x \in \ALPHABET X$ and $t$}.</description>
    </item>
    
    <item>
      <title>Course Notes</title>
      <link>https://adityam.github.io/stochastic-control/notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/notes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lectures</title>
      <link>https://adityam.github.io/stochastic-control/lectures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/lectures/</guid>
      <description>Whenever possible, I will post notes on some of the material covered in class, but that is not guaranteed. This is a graduate class and you are responsible for taking notes in class and reading the appropriate chapters of the textbooks.
The notes will be updated as we move along in the course. Please check the dates on the first page to keep track. If you find any typos/mistakes in the notes, please let me know.</description>
    </item>
    
    <item>
      <title>Positive definite matrices</title>
      <link>https://adityam.github.io/stochastic-control/appendix/positive-definite-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/positive-definite-matrix/</guid>
      <description>Definition A \(n \times n\) symmetric matrix \(M\) is called
 positive definite (written as \(M &amp;gt; 0\)) if for all \(x \in \reals^n\), \(x \neq 0\), we have \[x^\TRANS M x &amp;gt; 0.\]
 positive semi definite (written as \(M \ge 0\)) if for all \(x \in \reals^n\), \(x \neq 0\), we have \[x^\TRANS M x \ge 0.\]
    A symmetric matrix is positive definite (respt.</description>
    </item>
    
    <item>
      <title>Vectorization</title>
      <link>https://adityam.github.io/stochastic-control/appendix/vectorization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/vectorization/</guid>
      <description>Vectorization is a linear transformation that converts a matrix to a column vector. For example, \[\VEC\left(\MATRIX{a &amp;amp; b \\ c &amp;amp; d }\right) = \MATRIX{a \\ c \\ b \\ d}.\]
Vectorization is often used to express matrix multiplication as a linear transformation on matrices. In particular, we have the following three properties:
\(\VEC(ABC) = (C^\TRANS \otimes A) \VEC(B).\) \(\VEC(ABC) = (I \otimes AB)\VEC(C).\) \(\VEC(ABC) = (C^\TRANS B^\TRANS \otimes I)\VEC(A).\)  Another useful formulation is the following</description>
    </item>
    
  </channel>
</rss>