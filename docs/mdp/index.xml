<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MDP on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/mdp/</link>
    <description>Recent content in MDP on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://adityam.github.io/stochastic-control/mdp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Prelim: Stochastic optimization</title>
      <link>https://adityam.github.io/stochastic-control/mdp/stochastic-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/stochastic-optimization/</guid>
      <description>Let’s start with a simple stochastic optimization problem. Suppose \(\ALPHABET X\), \(\ALPHABET U\), \(\ALPHABET W\) are finite sets and let \(X \in \ALPHABET X\) and \(W \in \ALPHABET W\) be random variables defined on a common probability space. A decision maker observes the realization of \(X\) and chooses and action \(U \in \ALPHABET U\) according to a decison rule \(g\), i.e., \[ U = g(X) \] and incurs a cost \[J(g) = \EXP[ c(X, g(X), W) ],\] where the expectation is taken with respect to the joint probability distribution of \((X,W)\).</description>
    </item>
    
    <item>
      <title>Prelim: Blackwell&#39;s principle of irrelevant information</title>
      <link>https://adityam.github.io/stochastic-control/mdp/principle-of-irrelevant-information/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/principle-of-irrelevant-information/</guid>
      <description>Theorem Let \(\ALPHABET X\), \(\ALPHABET Y\), and \(\ALPHABET U\) be standard Borel spaces and \(X \in \ALPHABET X\) and \(Y \in \ALPHABET Y\) be random variables defined on a common probability space.
A decision maker observes \((X,Y)\) and chooses \(U\) to minimize \(\EXP[c(X,U)]\), where \(c \colon \ALPHABET X \times \ALPHABET U \to \reals\) is a measurable function.
Then, there is no loss of optimality in choosing \(U\) only as a function of \(X\).</description>
    </item>
    
    <item>
      <title>Theory: Basic model of an MDP</title>
      <link>https://adityam.github.io/stochastic-control/mdp/mdp-functional/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/mdp-functional/</guid>
      <description>Markov decision processes (MDP) are the simplest model of a stochastic control system. The dynamic behavior of an MDP is modeled by an equation of the form \[ X_{t+1} = f_t(X_t, U_t, W_t) \] where \(X_t \in \ALPHABET X\) is the state, \(U_t \in \ALPHABET U\) is the control input, and \(W_t \in \ALPHABET W\) is the noise. An agent/controller observes the state and chooses the control input \(U_t\).
The controller can be as sophisticated as we want.</description>
    </item>
    
    <item>
      <title>Example: Optimal Gambling</title>
      <link>https://adityam.github.io/stochastic-control/mdp/optimal-gambling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/optimal-gambling/</guid>
      <description>TL;DR This stylized model of optimal gambling was introduced by Kelly (1956) to highlight a relationship between channel capacity (which had been proposed recently by Shannon), and gambling. Our motivation for studying this model is to use it as an illustrative example to show that sometimes it is possible to identify the optimal strategy and value function of MDPs in closed form.
Imagine a gambler who goes to a casino with an initial fortune of \(x_1\) dollars and places bets over time and must leave after \(T\) bets.</description>
    </item>
    
    <item>
      <title>Example: Inventory Management</title>
      <link>https://adityam.github.io/stochastic-control/mdp/inventory-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/inventory-management/</guid>
      <description>TL;DR The inventory management example illustrates that a dynamic programming formulation is useful even when a closed form solution does not exist. This model also introduces the idea of post-decision state, which is useful in many contexts.
Imagine a retail store that stockpiles products in its warehouse to meet random demand. Suppose the store procures new stocks at the end of each day (and that there is no lead time and stocks are available next morning).</description>
    </item>
    
    <item>
      <title>Numerics: Matrix formulation of Markov decision processes</title>
      <link>https://adityam.github.io/stochastic-control/mdp/mdp-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/mdp-matrix/</guid>
      <description>In this section, we present a matrix formulation for finite state finite action MDPs, which is useful for computing the solutions numerically. Let’s start with the function model described earlier and assume that \(\ALPHABET X\) and \(\ALPHABET U\) are finite sets and that the cost function and the probability distribution of \(\{W_t\}_{t \ge 1}\) are time-homogeneous. Then, the following is a fundamental property of MDPs:
 Lemma For any \(x_1, x_2, \dots, x_T \in \ALPHABET X\) and \(u_1, \dots, u_T \in \ALPHABET U\), we have \[ \PR(X_{t+1} = x_{t+1} | X_{1:t} = x_{1:t}, U_{1:t} = u_{1:t}) = \PR(X_{t+1} = x_{t+1} | X_{t} = x_t , U_t = u_t).</description>
    </item>
    
    <item>
      <title>Prelim: Stochastic dominance</title>
      <link>https://adityam.github.io/stochastic-control/mdp/stochastic-dominance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/stochastic-dominance/</guid>
      <description>Stochastic dominance is a partial order on random variables. Let \(\ALPHABET X\) be a totally ordered finite set, say \(\{1, \dots, n\}\) and let \(\Delta(\ALPHABET X)\) denote the state of pmfs on \(\ALPHABET X\).
 Definition Suppose \(X\) and \(Y\) are \(\ALPHABET X\) valued random variables where \(X \sim \pi\) and \(Y \sim \mu\). We say \(X\) stochastically dominates \(Y\) if for any \(x \in \ALPHABET X\), \[\begin{equation}\label{eq:inc-prob} \PR(X \ge x) \ge \PR(Y \ge x).</description>
    </item>
    
    <item>
      <title>Theory: Monotone value functions and policies</title>
      <link>https://adityam.github.io/stochastic-control/mdp/monotonicity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/monotonicity/</guid>
      <description>Consider the matrix formulation of MDPs and suppose the state space \(\ALPHABET X\) is totally ordered. In many applications, it is useful to know if the value function is increasing (or decreasing) in state.
 Theorem Consider an MDP where the state space \(\ALPHABET X\) is totally ordered. Suppose the following conditions are satisfied.
C1. For every \(u \in \ALPHABET U\), the per-step cost \(c_t(x,u)\) is weakly inceasing in \(x\).</description>
    </item>
    
    <item>
      <title>Example: Power-delay trade-off in wireless communication</title>
      <link>https://adityam.github.io/stochastic-control/mdp/power-delay-tradeoff/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/power-delay-tradeoff/</guid>
      <description>TL;DR This stylized example of power-delay trade-off in wireless communications illustrates that a dynamic programming formulation can be used to identify qualitative properties of the value function and optimal policies.
In a cell phone, higher layer applications such as voicecall, email, browsers, etc. generate data packets. These packets are buffered in a queue and the transmission protocol decides how many packets to transmit at each time depending the number of packets in the queue and the quality of the wireless channel.</description>
    </item>
    
  </channel>
</rss>