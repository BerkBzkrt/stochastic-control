<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="Markov strategies,dynamic programming,comparison principle,principle of irrelevant information" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2022
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Theory: Basic model of an MDP</div>

<p>Markov decision processes (MDP) are the simplest model of a stochastic control system. The dynamic behavior of an MDP is modeled by an equation of the form <span class="math display">\[ \begin{equation} 
  S_{t+1} = f_t(S_t, A_t, W_t) \label{eq:state}
\end{equation}\]</span> where <span class="math inline">\(S_t \in \ALPHABET S\)</span> is the state, <span class="math inline">\(A_t \in \ALPHABET A\)</span> is the control input, and <span class="math inline">\(W_t \in \ALPHABET W\)</span> is the noise. An agent/controller observes the state and chooses the control input <span class="math inline">\(A_t\)</span>.</p>
<p>Eq. \eqref{eq:state} is a <em>non-linear</em> <em>stochastic</em> state-space model—<em>non-linear</em> because <span class="math inline">\(f_t\)</span> can be any nonlinear function; <em>stochastic</em> because the system is driven by stochastic noise <span class="math inline">\(\{W_t\}_{t \ge 1}\)</span>.</p>
<p>The controller can be as sophisticated as we want. In principle, it can analyze the entire history of observations and control actions to choose the current control action. Thus, the control action can be written as <span class="math display">\[ A_t = π_t(S_{1:t}, A_{1:t-1}),\]</span> where <span class="math inline">\(S_{1:t}\)</span> is a shorthand for <span class="math inline">\((S_1, \dots, S_t)\)</span> and a similar interpretation holds for <span class="math inline">\(A_{1:t-1})\)</span>. The function <span class="math inline">\(π_t\)</span> is called the <em>control law</em> (or <em>policy</em> or <em>strategy</em>) at time <span class="math inline">\(t\)</span>.</p>
<p>At each time, the system incurs a cost that may depend on the current state and control action. This cost is denoted by <span class="math inline">\(c_t(S_t, A_t)\)</span>. The system operates for a time horizon <span class="math inline">\(T\)</span>. During this time, it incurs a total cost <span class="math display">\[ \sum_{t=1}^T c_t(S_t, A_t). \]</span></p>
<p>The initial state <span class="math inline">\(S_1\)</span> and the noise process <span class="math inline">\(\{W_t\}_{t \ge 1}\)</span> are random variables defined on a common probability space (these are called <em>primitive random variables</em>) and are mutually independent. This seemingly benign assumption is critical for the theory that we present to go through.</p>
<p>Suppose we have to design such a controller. We are told the probability distribution of the initial state and the noise. We are also told the system update functions <span class="math inline">\((f_1, \dots, f_T)\)</span> and the cost functions <span class="math inline">\((c_1, \dots, c_T)\)</span>. We are asked to choose a <em>control strategy</em> <span class="math inline">\(π = (π_1, \dots, π_T)\)</span> to minimize the expected total cost <span class="math display">\[ J(π) := \EXP\bigg[ \sum_{t=1}^T c_t(S_t, A_t) \bigg]. \]</span> How should we proceed?</p>
<p>At first glance, the problem looks intimidating. It appears that we have to design a very sophisticated controller: one that analyzes all past data to choose a control input. However, this is not the case. A remarkable result is that the optimal control station can discard all past data and choose the control input based only on the current state of the system. Formally, we have the following:</p>
<div class="proclaim">
<p><strong>Optimality of Markov strategies.</strong> For the system model described above, there is no loss of optimality in chosing the control action according to <span class="math display">\[ A_t = π_t(S_t), \quad t=1, \dots, T.\]</span> Such a control strategy is called a <em>Markov strategy</em>.</p>
</div>
<details>
<summary>
<h4 class="unnumbered" id="proof-of-optimality-of-markov-strategies">Proof of optimality of Markov strategies</h4>
</summary>
<div>
<p>The above result claims that the cost incurred by the best Markovian strategy is the same as the cost incurred by the best history dependent strategy. This appears to be a tall claim, so lets see how we can prove it. The main idea of the proof is to repeatedly apply <a href="../principle-of-irrelevant-information">Blackwell’s principle of irrelevant information</a> <span class="citation" data-cites="Blackwell1964">(<a href="#ref-Blackwell1964" role="doc-biblioref">1964</a>)</span></p>
<div class="proclaim">
<p><strong>Two-Step Lemma.</strong> Consider an MDP that operates for two steps (<span class="math inline">\(T=2\)</span>). Then there is no loss of optimality in restricting attention to a Markov control strategy at time <span class="math inline">\(t=2\)</span>.</p>
</div>
<p>Note that <span class="math inline">\(π_1\)</span> is Markov because it can only depend <span class="math inline">\(S_1\)</span>.</p>
<h4 class="unnumbered" id="proof">Proof</h4>
<p>Fix <span class="math inline">\(π_1\)</span> and look at the problem of optimizing <span class="math inline">\(π_2\)</span>. The total cost is <span class="math display">\[ \EXP[ c_1(S_1, π_1(S_1)) + c_2(S_2, π_2(S_{1:2}, A_1)) ]\]</span> The choice of <span class="math inline">\(π_2\)</span> does not influence the first term. So, for a fixed <span class="math inline">\(π_1\)</span>, minimizing the total cost is the equivalent to minimizing the second term. Now, from Blackwell’s principle of irrelevant information, there exists a <span class="math inline">\(π_2^* \colon S_2 \mapsto A_2\)</span> such that for any <span class="math inline">\(π_2\)</span> <span class="math display">\[\EXP[c_2(S_2, π_2^*(S_2) ] \le \EXP[c_2(S_2, π_2(S_{1:2}, A_2) ].\]</span></p>
<div class="proclaim">
<p><strong>Three-Step Lemma.</strong> Consider an MDP that operates for three steps (<span class="math inline">\(T=3\)</span>). Assume that the control law <span class="math inline">\(π_3\)</span> at time <span class="math inline">\(t=3\)</span> is Markov, i.e., <span class="math inline">\(A_3 = π_3(S_3)\)</span>. Then, there is no loss of optimality in restricting attention to Markov control law at time <span class="math inline">\(t=2\)</span>.</p>
</div>
<h4 class="unnumbered" id="proof-1">Proof</h4>
<p>Fix <span class="math inline">\(π_1\)</span> and <span class="math inline">\(π_3\)</span> and look at optimizing <span class="math inline">\(π_2\)</span>. The total cost is <span class="math display">\[ \EXP[ c_1(S_1, π_1(S_1)) + c_2(S_2, π_2(S_{1:2}, A_1)) + c_3(S_3, π_3(S_3)].\]</span></p>
<p>The choice of <span class="math inline">\(π_2\)</span> does not affect the first term. So, for a fixed <span class="math inline">\(π_1\)</span> and <span class="math inline">\(π_3\)</span>, minimizing the total cost is the same as minimizing the last two terms. Let us look at the last term carefully. Bu the law of iterated expectations, we have <span class="math display">\[ \EXP[ c_3(S_3, π_3(S_3) ] = \EXP[ \EXP[ c_3(S_3, π_3(S_3)) | S_2, A_2 ] ]. \]</span> Now, <span class="math display">\[\begin{align*}
  \EXP[ c_3(S_3, π_3(S_3)) | S_2 = s_2, A_2 = u_2 ] &amp;= 
  \sum_{s_3 \in \ALPHABET S} c_3(s_3, π_3(s_3)) \\
  &amp;= \PR( w_2 \in \ALPHABET W : f_2(s_2, u_2, w_2) = s_3 )
  \\
  &amp;=: h_2(s_2, u_2).
\end{align*}\]</span> The key point is that <span class="math inline">\(h_2(s_2, u_2)\)</span> does not depend on <span class="math inline">\(π_1\)</span> or <span class="math inline">\(π_2\)</span>.</p>
<p>Thus, the total expected cost affected by the choice of <span class="math inline">\(π_2\)</span> can be written as <span class="math display">\[\begin{align*}
  \EXP[ c_2(S_2, A_2) + c_3(S_3, A_3) ] &amp;= \EXP[ c_2(S_2, A_2) + h_2(S_2, A_2)
  ] \\
  &amp;=: \EXP[ \tilde c_2(S_2, A_2) ].
\end{align*}\]</span> Now, by Blackwell’s principle of irrelevant information, there exists a <span class="math inline">\(π_2^* : S_2 \mapsto A_2\)</span> such that for any <span class="math inline">\(π_2\)</span>, we have <span class="math display">\[ \EXP[ \tilde c_2(S_2, π_2^*(S_2))] \le  \EXP[ \tilde c_2(S_2, π_2(S_{1:2},
A_1) ].\]</span></p>
<hr />
<p>Now we have enough background to present the proof of optimality of Markov strategies.</p>
<h4 class="unnumbered" id="proof-of-optimality-of-markov-strategies-1">Proof of optimality of Markov strategies</h4>
<p>The main idea is that any system can be thought of as a two- or three-step system by aggregating time. Suppose that the system operates for <span class="math inline">\(T\)</span> steps. It can be thought of as a two-step system where <span class="math inline">\(t \in \{1, \dots, T - 1\}\)</span> corresponds to step 1 and <span class="math inline">\(t = T\)</span> corresponds to step 2. From the two-step lemma, there is no loss of optimality in restricting attention to Markov control law at step 2 (i.e., at time <span class="math inline">\(t=T\)</span>), i.e., <span class="math display">\[ A_T = π_T(S_T). \]</span></p>
<p>Now consider a system where we are using a Markov strategy at time <span class="math inline">\(t=T\)</span>. This system can be thought of as a three-step system where <span class="math inline">\(t \in \{1, \dots, T-2\}\)</span> corresponds to step 1, <span class="math inline">\(t = T-1\)</span> corresponds to step 2, and <span class="math inline">\(t=T\)</span> corresponds to step 3. Since the controller at time <span class="math inline">\(T\)</span> is Markov, the assumption of the three step lemma is satisfied. Thus, by that lemma, there is no loss of optimality in restricting attention to Markov controllers at step 2 (i.e., at time <span class="math inline">\(t=T-1\)</span>), i.e., <span class="math display">\[A_{T-1} = π_{T-1}(S_{T-1}).\]</span></p>
<p>Now consider a system where we are using a Markov strategy at time <span class="math inline">\(t \in \{T-1, T\}\)</span>. This can be thought of as a three-step system where <span class="math inline">\(t \in \{1, \dots, T - 3\}\)</span> correspond to step 1, <span class="math inline">\(t = T-2\)</span> correspond to step 2, and <span class="math inline">\(t \in \{T-1, T\}\)</span> correspond to step 3. Since the controllers at time <span class="math inline">\(t \in \{T-1, T\}\)</span> are Markov, the assumption of the three-step lemma is satisfied. Thus, by that lemma, there is no loss of optimality in restricting attention to Markov controllers at step 2 (i.e., at time <span class="math inline">\(t=T-2\)</span>), i.e., <span class="math display">\[A_{T-2} = π_{T-2}(S_{T-2}).\]</span></p>
<p>Proceeding this way, we continue to think of the system as a three step system by different relabeling of time. Once we have shown that the controllers at times <span class="math inline">\(t \in \{s+1, s+2, \dots, T\}\)</span> are Markov, we relabel time as follows: <span class="math inline">\(t=\{1, \dots, s-1\}\)</span> corresponds to step 1, <span class="math inline">\(t = s\)</span> corresponds to step 2, and <span class="math inline">\(t \in \{s+1, \dots, T\}\)</span> corresponds to step 3. Since the controllers at time <span class="math inline">\(t \in \{s+1, \dots, T\}\)</span> are Markov, the assumption of the three-step lemma is satisfied. Thus, by that lemma, there is no loss of optimality in restricting attention to Markov controllers at stage 2 (i.e. at time <span class="math inline">\(s\)</span>), i.e., <span class="math display">\[A_s = π_s(S_s).\]</span></p>
<p>Proceeding until <span class="math inline">\(s=2\)</span>, completes the proof.</p>
</div>
</details>
<h1 data-number="1" id="performance"><span class="header-section-number">1</span> Performance of Markov strategies</h1>
<p>We have shown that there is no loss of optimality to restrict attention to Markov strategies. One of the advantages of Markov strategies is that it is easy to recursively compute their performance. In particular, given any Markov strategy <span class="math inline">\(π = (π_1, \dots, π_T)\)</span>, define <em>the cost-to-go functions</em> as follows: <span class="math display">\[J_t(s; π) = \EXP^π \bigg[ \sum_{s = t}^{T} c_s(S_s, π_s(S_s)) \biggm| S_t =
s\bigg]. \]</span> Note that <span class="math inline">\(J_t(s; π)\)</span> only depends on the future strategy <span class="math inline">\((π_t, \dots, π_T)\)</span>. These functions can be computed recursively as follows: <span class="math display">\[\begin{align*}
  J_t(s; π) &amp;= \EXP^π \bigg[ \sum_{s = t}^{T} c_s(S_s, π_s(S_s)) \biggm| S_t =
  s \bigg] \\
  &amp;= \EXP^π \bigg[ c_t(s, π_t(s)) + \EXP^π \bigg[ \sum_{s = t+1}^T
    c_s(S_s, π_s(S_s)) \biggm| S_{t+1} \bigg] \biggm| S_t = s \bigg]
  \\
  &amp;= \EXP^π\big[ c_t(s, π_t(s)) + J_{t+1}(S_{t+1}; π) \big| S_t = s \big].
\end{align*}\]</span></p>
<h1 data-number="2" id="DP"><span class="header-section-number">2</span> Dynamic Programming Decomposition</h1>
<p>Now we are ready to state the main result of MDPs</p>
<div class="proclaim">
<p><strong>Theorem (Dynamic program)</strong> Recursive define <em>value functions</em> <span class="math inline">\(\{V_t\}_{t = 1}^{T+1} \colon \ALPHABET S \to \reals\)</span> as follows: <span class="math display">\[ \begin{equation} \label{eq:DP-1}
  V_{T+1}(s) = 0 
\end{equation} \]</span> and for <span class="math inline">\(t \in \{T, \dots, 1\}\)</span>: <span class="math display">\[\begin{align}
   Q_t(s,a) &amp;= c(s,a) + \EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a] 
   \nonumber \\
   &amp;= c(s,a) + \EXP[ V_{t+1}(f_t(s,a,W_t)) ], \label{eq:DP-2}
\end{align}\]</span> and define <span class="math display">\[ \begin{equation} \label{eq:DP-3}
  V_t(s) = \min_{a \in \ALPHABET A} Q_t(s,a).
\end{equation} \]</span> Then, a Markov policy is optimal if and only if it satisfies <span class="math display">\[ \begin{equation} \label{eq:verification} 
  π_t^*(s) = \arg \min_{a \in \ALPHABET A} Q_t(s,a).
\end{equation} \]</span></p>
</div>
<p>Instead of proving the above result, we prove a related result</p>
<div class="proclaim">
<p><strong>Theorem (The comparison principle)</strong> For any Markov strategy <span class="math inline">\(π\)</span> <span class="math display">\[ J_t(s; π) \ge V_t(s) \]</span> with equality at <span class="math inline">\(t\)</span> if and only if the <em>future strategy</em> <span class="math inline">\(π_{t:T}\)</span> satisfies the verification step \eqref{eq:verification}.</p>
</div>
<p>Note that the comparison principle immediately implies that the strategy obtained using dynamic programming is optimal.</p>
<p>The comparison principle also allows us to interpret the value functions. The value function at time <span class="math inline">\(t\)</span> is the minimum of all the cost-to-go functions over all future strategies. The comparison principle also allows us to interpret the optimal policy (the interpretation is due to Bellman and is colloquially called Bellman’s principle of optimality).</p>
<div class="proclaim">
<p><strong>Bellman’s principle of optimality.</strong> An optimal policy has the property that whatever the initial state and the initial decisions are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.</p>
</div>
<details>
<summary>
<h4 class="unnumbered" id="proof-of-the-comparison-principle">Proof of the comparison principle</h4>
</summary>
<div>
<p>The proof proceeds by backward induction. Consider any Markov strategy <span class="math inline">\(π = (π_1, \dots, π_T)\)</span>. For <span class="math inline">\(t = T\)</span>, <span class="math display">\[ \begin{align*}
  V_T(s) &amp;= \min_{a \in \ALPHABET A} Q_T(s,a) \\
  &amp;\stackrel{(a)}= \min_{a \in \ALPHABET A} c_T(s,a) \\
  &amp;\stackrel{(b)}\le c_T(s, π_T(s)) \\
  &amp;\stackrel{(c)}= J_T(s; π),
\end{align*} \]</span> where <span class="math inline">\((a)\)</span> follows from the definition of <span class="math inline">\(Q_T\)</span>, <span class="math inline">\((b)\)</span> follows from the definition of minimization, and <span class="math inline">\((c)\)</span> follows from the definition of <span class="math inline">\(J_T\)</span>. Equality holds in <span class="math inline">\((b)\)</span> iff the policy <span class="math inline">\(π_T\)</span> is optimal. This result forms the basis of induction.</p>
<p>Now assume that the statement of the theorem is true for <span class="math inline">\(t+1\)</span>. Then, for <span class="math inline">\(t\)</span> <span class="math display">\[ \begin{align*}
  V_t(s) &amp;= \min_{a \in \ALPHABET A} Q_t(s,a) \\
  &amp;\stackrel{(a)}= \min_{a \in \ALPHABET A} \Big\{
  c_t(s,a) + \EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a] 
  \Big\}
  \\
  &amp;\stackrel{(b)}\le  \Big\{
  c_t(s,π_t(s)) + \EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = π_t(s)] 
  \Big\} \\
  &amp;\stackrel{(c)}\le  \Big\{
  c_t(s,π_t(s)) + \EXP[ J_{t+1}(S_{t+1}; π) | S_t = s, A_t = π_t(s)] 
  \Big\} \\
  &amp;\stackrel{(d)}= J_t(s; π),
\end{align*} \]</span> where <span class="math inline">\((a)\)</span> follows from the definition of <span class="math inline">\(Q_t\)</span>, <span class="math inline">\((b)\)</span> follows from the definition of minimization, <span class="math inline">\((c)\)</span> follows from the induction hypothesis, and <span class="math inline">\((d)\)</span> follows from the definition of <span class="math inline">\(J_t\)</span>. We have equality in step <span class="math inline">\((b)\)</span> iff <span class="math inline">\(π_t\)</span> satisfies the verification step \eqref{eq:verification} and have equality in step <span class="math inline">\((c)\)</span> iff <span class="math inline">\(π_{t+1:T}\)</span> is optimal (this is part of the induction hypothesis). Thus, the result is true for time <span class="math inline">\(t\)</span> and, by the principle of induction, is true for all time. <span class="math inline">\(\Box\)</span></p>
</div>
</details>
<h1 data-number="3" id="variations-of-a-theme"><span class="header-section-number">3</span> Variations of a theme</h1>
<h2 data-number="3.1" id="cost-depends-on-next-state"><span class="header-section-number">3.1</span> Cost depends on next state</h2>
<p>In the basic model that we have considered above, we assumed that the per-step cost depends only on the current state and current actions. In some applications, such as the <a href="../inventory-management">inventory management</a> model considered in class, it is more natural to have a cost function where the cost depends on the current state, current action, and the next state. Conceptually, such problems can be treated in the same way as the standard model.</p>
<p>In particular, suppose we have a per-step cost given by <span class="math inline">\(c_t(S_t,A_t,S_{t+1})\)</span>, where the objective is to minimize <span class="math display">\[ J(π) = \EXP\Bigl[ \sum_{t=1}^T c_t(S_t, A_t, S_{t+1}) \Bigr]. \]</span></p>
<p>Define <span class="math display">\[ \tilde c_t(s, a) = \EXP[ c_t(s, a, S_{t+1}) | S_t = s, A_t = a ] 
= \EXP[ c_t(s,a, f_t(s,a, W_t) ]. \]</span> Then, by the towering property of conditional expectation, we can write</p>
<p><span class="math display">\[ \begin{align*} 
 J(π) &amp;= \EXP\Bigl[ \sum_{t=1}^T \EXP[ c_t(S_t, A_t, S_{t+1}) | S_t, A_t] \Bigr] \\
 &amp;= \EXP\Bigl[ \sum_{t=1}^T \tilde c_t(S_t, A_t) \Bigr].
\end{align*} \]</span></p>
<p>Thus, we can equivalently consider this as our standard model with the per-step cost given by <span class="math inline">\(\tilde c_t(S_t, A_t)\)</span>. We can write the recursive step of the dynamic program as follows: <span class="math display">\[ Q_t(s,a) = \EXP[ c_t(s,a, S_{t+1}) + V_{t+1}(S_{t+1}) | S_t = s, A_t = a ].\]</span></p>
<p>For numerically solving the dynamic program when the cost is time-homogeneous (i.e., does not depend on <span class="math inline">\(t\)</span>), it is more efficient to compute <span class="math inline">\(\tilde c\)</span> once and recuse that in the dynamic program recursion.</p>
<h2 data-number="3.2" id="discounted-cost"><span class="header-section-number">3.2</span> Discounted cost</h2>
<p>In some applications, it is common to consider a discounted expected cost given by <span class="math display">\[ J(π) = \EXP\Bigl[ \sum_{t=1}^T γ^{t-1} c_t(S_t, A_t) \Bigr] \]</span> where <span class="math inline">\(γ \in (0,1)\)</span> is called the discount factor.</p>
<p>There are two interpretations of the discount factor <span class="math inline">\(γ\)</span>. The first interpretation is an economic interpretation to determine the <em>present value</em> of a utility that will be received in the future. For example, suppose a decision maker is indifferent between receiving 1 dollar today or <span class="math inline">\(s\)</span> dollars tomorrow. This means that the decision maker discounts the future at a rate <span class="math inline">\(1/s\)</span>, so <span class="math inline">\(γ = 1/s\)</span>.</p>
<p>The second interpretation is that of an absorbing state. Suppose we are operating a machine that generates a value of $1 each day. However, there is a probability <span class="math inline">\(p\)</span> that the machine will break down at the end of the day. Thus, the expected return for today is $1 while the expected return for tomorrow is <span class="math inline">\((1-p)\)</span> (which is the probability that the machine is still working tomorrow). In this case, the discount factor is defined as <span class="math inline">\((1-p)\)</span>. See <span class="citation" data-cites="Shwartz2001"><a href="#ref-Shwartz2001" role="doc-biblioref">Shwartz</a> (<a href="#ref-Shwartz2001" role="doc-biblioref">2001</a>)</span> for a detailed discussion of this alternative.</p>
<p>The recursive step of the dynamic program for such models can be written as <span class="math display">\[ Q_t(s,a) = c_t(s,a) + γ \, \EXP[ V_{t+1}( S_{t+1}) | S_t = s, A_t = a ].\]</span></p>
<h2 data-number="3.3" id="multiplicative-cost"><span class="header-section-number">3.3</span> Multiplicative cost</h2>
<p>So far, we have assumed that the cost is additive. The dynamic proramming decomposition also works for models with multiplicative cost. In particular, suppose that the performance of any policy is given by <span class="math display">\[ J(π) = \EXP\Bigl[ \prod_{t=1}^T c_t(S_t, A_t) \Bigr] \]</span> where the per-step cost function is positive. Then, it can be shown that the optimal policy is given by the following dynamic program.</p>
<div class="highlight">
<p><strong>Dynamic Program for multiplicative cost</strong> Initialize <span class="math inline">\(V_{T+1}(s) = 1\)</span> and recursively compute <span class="math display">\[ \begin{align*}
 Q_t(s,a) &amp;= c_t(s,a) \EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a ], \\
 V_t(s) &amp;= \min_{a \in \ALPHABET A} Q_t(s,a).
\end{align*} \]</span></p>
</div>
<h2 data-number="3.4" id="exponential-cost"><span class="header-section-number">3.4</span> Exponential cost function</h2>
<p>A special class of multiplicative cost function is exponential of sum: <span class="math display">\[J(π) = \EXP\Bigl[ \exp\Bigl( \theta \sum_{t=1}^T c_t(S_t, A_t) \Bigr) \Bigr]. \]</span></p>
<p>When <span class="math inline">\(\theta &gt; 0\)</span>, the above captures risk-averse preferences and when <span class="math inline">\(\theta &lt; 0\)</span>, it corresponds to risk-seeking preferences. This is equivalent to a multiplicative cost <span class="math display">\[J(π) = \EXP\Bigl[ \prod_{t=1}^T \exp( \theta c_t(S_t, A_t)) \Bigr]. \]</span> Therefore, the dynamic program for multiplicative cost is also applicable for this model.</p>
<h2 data-number="3.5" id="optimal-stopping"><span class="header-section-number">3.5</span> Optimal stopping</h2>
<p>Let <span class="math inline">\(\{S_t\}_{t \ge 1}\)</span> be a Markov chain. At each time <span class="math inline">\(t\)</span>, a decision maker observes the state <span class="math inline">\(S_t\)</span> of the Markov chain and decides whether to continue or stop the process. If the decision maker decides to continue, he incurs a <em>continuation cost</em> <span class="math inline">\(c_t(S_t)\)</span> and the state evolves. If the DM decides to stop, he incurs a <em>stopping cost</em> of <span class="math inline">\(d_t(S_t)\)</span> and the problem is terminated. The objective is to determine an optimal <em>stopping time</em> <span class="math inline">\(\tau\)</span> to minimize <span class="math display">\[J(\tau) := \EXP\bigg[ \sum_{t=1}^{\tau-1} c_t(S_t) + d_\tau(S_\tau)
\bigg].\]</span></p>
<p>Such problems are called <em>Optimal stopping problems</em>.</p>
<p>Define the <em>cost-to-go function</em> of any stopping rule as <span class="math display">\[J_t(s; \tau) = \EXP\bigg[ \sum_{s = t}^{\tau - 1} c_{\tau}(S_t) +
d_\tau(S_\tau) \,\bigg|\, \tau &gt; t \bigg]\]</span> and the <em>value function</em> as <span class="math display">\[V_t(s) = \inf_{\tau} J_t(s; \tau). \]</span> Then, it can be shown that the value functions satisfy the following recursion:</p>
<div class="highlight">
<p><strong>Dynamic Program for optimal stopping</strong> <span class="math display">\[ \begin{align*}
V_T(s) &amp;= s_T(s) \\
V_t(s) &amp;= \min\{ s_t(s), c_t(s) + \EXP[ V_{t+1}(S_{t+1}) | S_t = s].
\end{align*}\]</span></p>
</div>
<p>For more details on the optimal stopping problems, see <span class="citation" data-cites="Ferguson:book"><a href="#ref-Ferguson:book" role="doc-biblioref">Ferguson</a> (<a href="#ref-Ferguson:book" role="doc-biblioref">2008</a>)</span>.</p>
<h2 data-number="3.6" id="minimax-setup"><span class="header-section-number">3.6</span> Minimax setup</h2>
<p><em>To be written</em></p>
<h1 class="unnumbered" id="references">References</h1>
<p>The proof idea for the optimality of Markov strategies is based on a proof by <span class="citation" data-cites="Witsenhausen1979"><a href="#ref-Witsenhausen1979" role="doc-biblioref">Witsenhausen</a> (<a href="#ref-Witsenhausen1979" role="doc-biblioref">1979</a>)</span> on the structure of optimal coding strategies for real-time communication. Note that the proof does not require us to find a dynamic programming decomposition of the problem. This is in contrast with the standard textbook proof where the optimality of Markov strategies is proved as part of the dynamic programming decomposition.</p>
<hr />
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Blackwell1964" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Blackwell, D.</span> 1964. Memoryless strategies in finite-stage dynamic programming. <em>The Annals of Mathematical Statistics</em> <em>35</em>, 2, 863–865. DOI: <a href="https://doi.org/10.1214/aoms/1177703586">10.1214/aoms/1177703586</a>.</p>
</div>
<div id="ref-Ferguson:book" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Ferguson, T.S.</span> 2008. Optimal stopping and applications. Available at: <a href="http://www.math.ucla.edu/~tom/Stopping/Contents.html">http://www.math.ucla.edu/~tom/Stopping/Contents.html</a>.</p>
</div>
<div id="ref-Shwartz2001" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Shwartz, A.</span> 2001. Death and discounting. <em><span>IEEE</span> Transactions on Automatic Control</em> <em>46</em>, 4, 644–647. DOI: <a href="https://doi.org/10.1109/9.917668">10.1109/9.917668</a>.</p>
</div>
<div id="ref-Witsenhausen1979" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Witsenhausen, H.S.</span> 1979. On the structure of real-time source coders. <em>Bell System Technical Journal</em> <em>58</em>, 6, 1437–1451.</p>
</div>
</div>


<p class="categories">
This entry 

 was last updated on 17 Dec 2021
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/mdp">
  MDP</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/markov-strategies">markov strategies</a>,
<a href="https://adityam.github.io/stochastic-control/tags/dynamic-programming">dynamic programming</a>,
<a href="https://adityam.github.io/stochastic-control/tags/comparison-principle">comparison principle</a>,
<a href="https://adityam.github.io/stochastic-control/tags/principle-of-irrelevant-information">principle of irrelevant information</a>.</p>



    </div>
  </body>
</html>


