<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="reward shaping" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2022
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Theory: Reward Shaping</div>

<p>What are the conditions under which two MDPs which have the same dynamics but different cost functions have the same optimal policy? This is an important question in reinforcement learning (where one often <em>shapes</em> the reward function to speed up learning) and inverse reinforcement learning (where one learns the reward function from the behavior of an expert). The following result provides a complete answer to this question.</p>
<p>Let <span class="math inline">\(M^1\)</span> and <span class="math inline">\(M^2\)</span> denote two MDPs on the same state space <span class="math inline">\(\ALPHABET S\)</span> and action space <span class="math inline">\(\ALPHABET A\)</span>. Both MDPs have the same dynamics <span class="math inline">\(f = (f^1, \dots, f_T)\)</span>, same distribution on the noise <span class="math inline">\(P_W\)</span>, but different cost functions <span class="math inline">\(c^1 = (c^1_1, \dots, c^1_T)\)</span> and <span class="math inline">\(c^2 = (c^2_1, \dots, c^2_T)\)</span>. We assume that for <span class="math inline">\(t \in \{1, \dots, T-1\}\)</span>, the per-step cost is a function of the current state, current action, and next state (see <a href="../mdp-functional#cost-depends-on-next-state">cost depending on next state</a>) and for <span class="math inline">\(t = T\)</span>, the per-step cost function is just a function of the current state. Let <span class="math inline">\(π^1 = (π^1_1, \dots, π^1_T)\)</span> and <span class="math inline">\(π^2 = (π^2_1, \dots, π^2_T)\)</span> denote the optimal policy corresponding to <span class="math inline">\(M^1\)</span> and <span class="math inline">\(M^2\)</span>, respectively.</p>
<div class="highlight">
<dl>
<dt><span id="thm:1"></span><span id="thm:reward-shaping" class="pandoc-numbering-text thm"><strong>Theorem 1</strong></span></dt>
<dd>
<p>The policy <span class="math inline">\(π^1\)</span> is equal to the policy <span class="math inline">\(π^2\)</span> for every choice of transition function <span class="math inline">\(f\)</span> and probability distribution <span class="math inline">\(P_W\)</span> if and only if there exists a sequence of functions <span class="math inline">\(\{Φ_t \colon \ALPHABET S \to \reals\}_{t \ge 1}\)</span>, which are called <em>potential functions</em>, such that:</p>
<ol type="1">
<li><p>For <span class="math inline">\(t = T\)</span>, <span class="math display">\[ c^2_T(s) = c^1_T(s) - Φ_T(s).  \]</span></p></li>
<li><p>For <span class="math inline">\(t \in \{1, \dots, T-1\}\)</span>, <span class="math display">\[ c^2_t(s,a,s_{+}) = c^1_t(s,a,s_{+}) +  Φ_{t+1}(s_{+}) - Φ_t(s). \]</span></p></li>
</ol>
</dd>
</dl>
</div>
<dl>
<dt>Remark</dt>
<dd>
<ol type="1">
<li><p>The sign of the potential function is irrelevant. So, we could also have written <span class="math display">\[ c^2_t(s,a,s_{+}) = c^1_t(s,a,s_{+}) +  Φ_t(s) - Φ_{t+1}(s_{+}). \]</span></p></li>
<li><p>The result naturally extends to infinite horizon expected cost model (and is typically stated for such a model). In the infinite horizon case, the potential function must be time-invariant and condition 2 needs to be replaced by <span class="math display">\[ c^2(s,a,s_{+}) = c^1(s,a,s_{+}) + γ Φ(s_{+}) - Φ(s). \]</span></p></li>
</ol>
</dd>
</dl>
<h4 class="unnumbered" id="proof-sufficiency">Proof (sufficiency)</h4>
<p>Suppose conditions 1 and 2 in the statement of the theorem hold. That is, <span class="math display">\[c^2_t(s,a,s_{+}) = c^1_t(s,a,s_{+}) - Φ_t(s) + Φ_{t+1}(s_{+})
\quad\text{and}\quad
c^2_T(s) = c^1_T(s) - Φ_T(s). \]</span></p>
<p>Then, we claim that <span class="math display">\[\begin{equation} \label{eq:claim}
Q^2_t(s,a) = Q^1_t(s,a) - Φ_t(s)
\quad
V^2_t(s) = V^1_t(s) - Φ_t(s).
\end{equation}\]</span></p>
<p>We prove the result by backward induction. First note that <span class="math display">\[
  V^2_T(s) = c^2_T(s) = c^1_T(s) - Φ_T(s) = V^1_T(s) - Φ_T(s).
\]</span> This forms the basis of induction. Now suppose that \eqref{eq:claim} holds for time <span class="math inline">\(t+1\)</span>. Now consider <span class="math display">\[\begin{align*}
Q^2_t(s,a) &amp;= \EXP[ c^2_t(s,a,S_{t+1}) + V^2_{t+1}(S_{t+1}) \mid S_t = s, A_t = a ]
\\
&amp;\stackrel{(a)}= \EXP[ c^1_t(s,a,S_{t+1}) - Φ_t(s) + Φ_{t+1}(S_{t+1}) \\
&amp;\qquad + V^1_{t+1}(S_{t+1}) - Φ_{t+1}(S_{t+1}) \mid S_t = s, A_t = a ] \\
&amp;= \EXP[ c^1_t(s,a,S_{t+1}) - Φ_t(s) + V^1_{t+1}(S_{t+1}) \mid
S_t = s, A_t = a] \\
&amp;= Q^1_t(s,a) - Φ_t(s),
\end{align*}\]</span> where <span class="math inline">\((a)\)</span> follows from property 2 and the induction hypothesis.</p>
<p>Now, mimimizing both sides over <span class="math inline">\(a\)</span> gives <span class="math display">\[ V^2_t(s) = V^1_t(s) - Φ_t(s). \]</span></p>
<p>This proves the induction step.  <span class="math inline">\(\Box\)</span></p>
<h4 class="unnumbered" id="proof-necessity">Proof (necessity)</h4>
<p>See <span class="citation" data-cites="Ng1999">Ng et al. (<a href="#ref-Ng1999" role="doc-biblioref">1999</a>)</span>.</p>
<dl>
<dt>Additional Remarks</dt>
<dd>
<ol type="1">
<li><p>The advantage (or benefit) function given by <span class="math display">\[ B_t(s,a) := Q_t(s,a) - V_t(s) \]</span> measures the relative cost of choosing action <span class="math inline">\(a\)</span> over the optimal action. An implication of the claim \eqref{eq:claim} used in the above proof is that reward shaping does not change the advantage function!</p></li>
<li><p>In reinforcement learning in infinite horizon discounted cost setup, reward shaping refers to choosing a potential function <span class="math inline">\(Φ\)</span> to change the cost function from <span class="math inline">\(c(s,a,s_{+})\)</span> to <span class="math inline">\(\tilde c(s,a,s_{+}) = c(s,a,s_{+}) + γ Φ(s_{+}) - Φ(s)\)</span> to speed up learning. One often chooses <span class="math inline">\(Φ(s) = V(s)\)</span>. If we extend claim \eqref{eq:claim} to infinite horizon, then an implication of choosing <span class="math inline">\(Φ(s) = V(s)\)</span> is that the value function of the modified cost <span class="math inline">\(\tilde c(s,a,s_{+})\)</span> is zero!</p></li>
</ol>
</dd>
</dl>
<h1 data-number="1" id="generalization-to-discounted-models"><span class="header-section-number">1</span> Generalization to discounted models</h1>
<p>Now consider a finite horizon discounted cost problem, where the performance of a policy <span class="math inline">\(π\)</span> is given by <span class="math display">\[ 
J(π) = \EXP\Bigl[ \sum_{t=1}^{T-1} γ^{t-1} c_t(S_t, A_t) + γ^T c_T(S_T)
       \Bigr]. 
\]</span></p>
<p>As argued in <a href="../mdp-functional#discounted-cost">the introduction to discounted models</a>, the dynamic prgram for this case is given by</p>
<p><span class="math display">\[ V_{T}(s) = c_T(s) \]</span> and for <span class="math inline">\(t \in \{T-1, \dots, 1\}\)</span>: <span class="math display">\[ \begin{align*}
  Q_t(s,a) &amp;= c(s,a) + γ \EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a ], \\
  V_t(s) &amp;= \min_{a \in \ALPHABET A} Q_t(s,a).
\end{align*} \]</span></p>
<p>For such models, we have the following.</p>
<div class="highlight">
<dl>
<dt><span id="cor:1"></span><span id="cor:discounted" class="pandoc-numbering-text cor"><strong>Corollary 1</strong></span></dt>
<dd>
<p>For discounted cost models, the result of <a href="#thm:reward-shaping">Theorem 1</a> hold if condition 2 is replaced by</p>
<ol start="2" type="1">
<li><p>For <span class="math inline">\(t \in \{1, \dots, T-1\}\)</span>,</p>
<p><span class="math display">\[ c^2_t(s,a,s_{+}) = c^1_t(s,a,s_{+}) + γ Φ_{t+1}(s_{+}) - Φ_t(s). \]</span></p></li>
</ol>
</dd>
</dl>
</div>
<dl>
<dt>Remark</dt>
<dd>
<ol type="1">
<li><p>If the cost function is time homogeneous, <a href="#cor:discounted">Corollary 1</a> extends naturally to infinite horizon models with a time-homogeneous potential function.</p></li>
<li><p>See the notes on <a href="../../inf-mdp/martingale-approach">martingale approach to stochastic control</a> for an iteresting relationship between reward shaping and martingales.</p></li>
<li><p>As an example of reward shaping, see the notes on <a href="../../inf-mdp/inventory-management">inventory management</a>.</p></li>
</ol>
</dd>
</dl>
<h1 class="unnumbered" id="exercises">Exercises</h1>
<ol type="1">
<li><p>Suppose <span class="math inline">\(π = (π_1, \dots, π_T)\)</span> is any <em>Markov</em> policy. Let <span class="math inline">\(J^i_t(s;π)\)</span>, <span class="math inline">\(i \in \{1, 2\}\)</span> denote the performance of policy <span class="math inline">\(π\)</span> in model <span class="math inline">\(M^i\)</span> starting in state <span class="math inline">\(s\)</span> at time <span class="math inline">\(t\)</span> (see <a href="../mdp-functional/#performance">Performance of Markov Strategies</a>). Show that</p>
<p><span class="math display">\[ J^2_t(s; π) = J^1_t(s; π) - Φ(s). \]</span></p>
<p>Using this relationship, show that reward shaping is <em>robust</em> in the sense that near-optimal policies are preserved. If <span class="math inline">\(π\)</span> is near optimal in <span class="math inline">\(M^2\)</span> in the sense that <span class="math inline">\(| V^2_t(s) - J^2_t(s; π)| \le ε_t\)</span> using potential based shaping, then, <span class="math inline">\(π\)</span> will also be near optimal in the original model, i.e., <span class="math inline">\(|V^1_t(s) - J^1_t(s;π) | \le ε_t\)</span>.</p></li>
</ol>
<h1 class="unnumbered" id="references">References</h1>
<p>The above result is due to <span class="citation" data-cites="Ng1999">Ng et al. (<a href="#ref-Ng1999" role="doc-biblioref">1999</a>)</span> who provided the result for infinite horizon models. However, in my opinion, it is conceptually simpler to start with the finite horizon model. For a discussion on practical considerations in using reward shaping in reinforcement learning, see <span class="citation" data-cites="Grzes2009">Grzes and Kudenko (<a href="#ref-Grzes2009" role="doc-biblioref">2009</a>)</span> and <span class="citation" data-cites="Devlin2014">Devlin (<a href="#ref-Devlin2014" role="doc-biblioref">2014</a>)</span>.</p>
<hr />
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Devlin2014" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Devlin, S.</span> 2014. Potential based reward shaping tutorial. Available at: <a href="http://www-users.cs.york.ac.uk/~devlin/presentations/pbrs-tut.pdf">http://www-users.cs.york.ac.uk/~devlin/presentations/pbrs-tut.pdf</a>.
</div>
<div id="ref-Grzes2009" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Grzes, M. and Kudenko, D.</span> 2009. Theoretical and empirical analysis of reward shaping in reinforcement learning. <em>International conference on machine learning and applications</em>, 337–344. DOI: <a href="https://doi.org/10.1109/ICMLA.2009.33">10.1109/ICMLA.2009.33</a>.
</div>
<div id="ref-Ng1999" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Ng, A.Y., Harada, D., and Russell, S.</span> 1999. Policy invariance under reward transformations: Theory and application to reward shaping. <em>ICML</em>, 278–287. Available at: <a href="http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf">http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf</a>.
</div>
</div>


<p class="categories">
This entry 

 was last updated on 16 Dec 2021
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/mdp">
  MDP</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/reward-shaping">reward shaping</a>.</p>



    </div>
  </body>
</html>


