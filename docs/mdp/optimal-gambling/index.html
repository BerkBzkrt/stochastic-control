<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="gambling,closed form solution,structural result,information theory" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" />
  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/github.css" type="text/css" />
  <script src="https://adityam.github.io/stochastic-control//js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script type="text/javascript"
    src="https://cdn.plot.ly/plotly-basic-latest.min.js">
  </script>
  <script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_SVG,https://adityam.github.io/stochastic-control/js/mathjax-local.js">
  </script>
</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2018
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<h1>Example: Optimal Gambling</h1>

<p>TL;DR <em>This stylized model of optimal gambling was introduced by <span class="citation" data-cites="Kelly1956">Kelly (<a href="#ref-Kelly1956">1956</a>)</span> to highlight a relationship between channel capacity (which had been proposed recently by Shannon), and gambling. Our motivation for studying this model is to use it as an illustrative example to show that sometimes it is possible to identify the optimal strategy and value function of MDPs in closed form.</em></p>
<hr />
<p>Imagine a gambler who goes to a casino with an initial fortune of <span class="math inline">\(x_1\)</span> dollars and places bets over time and must leave after <span class="math inline">\(T\)</span> bets. Let <span class="math inline">\(X_t\)</span> denote the gambler’s fortune after <span class="math inline">\(t\)</span> bets. In this example, time denotes the number of times that the gambler has bet.</p>
<p>At time <span class="math inline">\(t\)</span>, the gambler may place a bet for any amount <span class="math inline">\(U_t\)</span> less than or equal to his current fortune <span class="math inline">\(X_t\)</span>. If he wins the bet (denoted by the even <span class="math inline">\(W_t = 1\)</span>), the casino gives him the amount that he had bet. If he loses the bet (denoted by the event <span class="math inline">\(W_t = -1\)</span>), he pays the casino the amount that he had bet. Thus, the dynamics can be written as</p>
<p><span class="math display">\[ X_{t+1} = X_t + W_t U_t. \]</span></p>
<p>The outcomes of the bets <span class="math inline">\(\{W_t\}_{t \ge 1}\)</span> are <em>primitive random variables</em>, i.e., they are independent of each other, the gambler’s initial fortune, and his betting strategy.</p>
<p>The gambler’s utility is <span class="math inline">\(\log X_T\)</span>, the logarithm of his final fortune. Thus, the reward function may be written as</p>
<p><span class="math display">\[ r_t(x, u) = 0 
   \quad \text{and} \quad
   r_T(x) = \log x.
\]</span></p>
<p>Find the strategy that maximizes the gambler’s utility, <span class="math inline">\(\EXP[\log X_T]\)</span>.</p>
<h2 id="optimal-gambling-strategy-and-value-functions">Optimal gambling strategy and value functions</h2>
<p>The above model of optimal gambling is a Markov decision process. Therefore, the optimal solution is given by dynamic programming.</p>
<div class="highlight">
<dl>
<dt>Dynamic program</dt>
<dd><p>Define the following value function <span class="math inline">\(V_t \colon \reals_{\ge 0} \to \reals\)</span> <span class="math display">\[ V_T(x) = \log x \]</span> and for <span class="math inline">\(t \in \{T-1, \dots, 1\}\)</span>: <span class="math display">\[ \begin{align}
  Q_t(x,u) &amp;= \EXP[ r_t(x,u) + V_{t+1}(X_{t+1}) \,|\, X_t = x, U_t = u] \\
  &amp;= p V_{t+1}(x+u) + (1-p) V_{t+1}(x-u),
  \end{align}
\]</span> and <span class="math display">\[ \begin{align}
  V_t(x) &amp;=  \max_{u \in [0, x]} Q_t(x,u), \\
  g_t(x) &amp;= \arg \max_{u \in [0, x]} Q_t(x,u). \\
  \end{align}
\]</span></p>
<p>Then the strategy <span class="math inline">\(g = (g_1, \dots, g_{T-1})\)</span> is optimal.</p>
</dd>
</dl>
</div>
<p>The above model is one of the rare instances when the optimal strategy and the optimal strategy and value function of an MDP can be identified in closed form.</p>
<div class="highlight">
<dl>
<dt>Theorem</dt>
<dd><p>When <span class="math inline">\(p \le 0.5\)</span>:</p>
<ul>
<li>the optimal strategy is to <em>not gamble</em>, specifically <span class="math inline">\(g_t(x) = 0\)</span>;</li>
<li>the value function is <span class="math inline">\(V_t(x) = \log x\)</span>.</li>
</ul>
<p>When <span class="math inline">\(p &gt; 0.5\)</span>:</p>
<ul>
<li>the optimal strategy is <em>to bet a fraction of the current fortune</em>, specifically <span class="math inline">\(g_t(x) = (2p - 1)x\)</span>;</li>
<li>the value function is <span class="math inline">\(V_t(x) = \log x + (T - t) C\)</span>, where<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> <span class="math display">\[ C = \log 2 + p \log p + (1-p) \log (1-p).\]</span></li>
</ul>
</dd>
</dl>
</div>
<p>We prove the two cases separately.</p>
<h4 id="proof-when-p-le-0.5">Proof when <span class="math inline">\(p \le 0.5\)</span></h4>
<p>Let <span class="math inline">\(p = \PR(W_t = 1)\)</span> and <span class="math inline">\(q = \PR(W_t = -1)\)</span>. Then <span class="math inline">\(p \le 0.5\)</span> implies that <span class="math inline">\(p \le 1 - p = q\)</span>.</p>
<p>We proceed by backward induction. For <span class="math inline">\(t = T\)</span>, we have that <span class="math inline">\(V_T(x) = \log x\)</span>. This forms the basis of induction. Now assume that for <span class="math inline">\(t+1\)</span>, <span class="math inline">\(V_{t+1}(x) = \log x\)</span>. Now consider</p>
<p><span class="math display">\[ Q_t(x,u) = p V_{t+1}(x+u) + qV_{t+1}(x-u). \]</span></p>
<p>Differentiating both sides w.r.t. <span class="math inline">\(u\)</span>, we get <span class="math display">\[ \begin{align} 
  \frac { \partial Q_t(x,u) } {\partial u} &amp;= 
   \frac p { x + u} - \frac q { x - u } 
   \\
   &amp; = \frac { (p - q) x - (p + q) u } { x^2 - u^2 } 
   \\
   &amp; =
   \frac { - (q - p) x - u } {x^2 - u^2 } 
   \\
   &amp;&lt; 0.
  \end{align}   
\]</span></p>
<p>This implies that <span class="math inline">\(Q_t(x,u)\)</span> is decreasing in <span class="math inline">\(u\)</span>. Therefore,</p>
<p><span class="math display">\[ g_t(x) = \arg\max_{u \in [0, x]} Q_t(x,u) = 0. \]</span></p>
<p>Moreover, <span class="math display">\[ V_t(x) = Q_t(x, g_t(x)) = \log x.\]</span></p>
<p>This completes the induction step.</p>
<h4 id="proof-when-p-0.5">Proof when <span class="math inline">\(p &gt; 0.5\)</span></h4>
<p>As in the previous case, let <span class="math inline">\(p = \PR(W_t = 1)\)</span> and <span class="math inline">\(q = \PR(W_t = -1)\)</span>. Then <span class="math inline">\(p &gt; 0.5\)</span> implies that <span class="math inline">\(p &gt; 1 - p = q\)</span>.</p>
<p>We proceed by backward induction. For <span class="math inline">\(t = T\)</span>, we have that <span class="math inline">\(V_T(x) = \log x\)</span>. This forms the basis of induction. Now assume that for <span class="math inline">\(t+1\)</span>, <span class="math inline">\(V_{t+1}(x) = \log x + (T -t - 1)C\)</span>. Now consider</p>
<p><span class="math display">\[ Q_t(x,u) = p V_{t+1}(x+u) + qV_{t+1}(x-u). \]</span></p>
<p>Differentiating both sides w.r.t. <span class="math inline">\(u\)</span>, we get <span class="math display">\[ \begin{align} 
  \frac { \partial Q_t(x,u) } {\partial u} &amp;= 
   \frac p { x + u} - \frac q { x - u } 
   \\
   &amp; = \frac { (p - q) x - (p + q) u } { x^2 - u^2 } 
   \\
   &amp; =
   \frac { (p - q) x - u } {x^2 - u^2 } 
  \end{align}   
\]</span></p>
<p>Setting <span class="math inline">\(\partial Q_t(x,u)/\partial u = 0\)</span>, we get that the optimal action is</p>
<p><span class="math display">\[ g_t(x) = (p-q) x. \]</span></p>
<p>Note that <span class="math inline">\((p-q) \in (0,1]\)</span></p>
<p><span class="math display">\[ 
  \frac { \partial^2 Q_t(x,u) } {\partial u^2} = 
   - \frac p { (x + u)^2 } - \frac q { (x - u)^2 } 
  &lt; 0;
\]</span> hence the above action is indeed the maximizer. Moreover, <span class="math display">\[ \begin{align} 
  V_t(x) &amp;= Q_t(x, g_t(x))  \\
  &amp;= p V_{t+1}(x + g_t(x)) + q V_{t+1}( x - g_t(x) )\\
  &amp;= \log x + p \log (1 + (p-q)) + q \log (1 - (p-q)) + (T - t -1)C \\
  &amp;= \log x + p \log 2p + q \log 2q + (T - t + 1)C \\
  &amp;= \log x + (T - t) C
  \end{align}   
\]</span></p>
<p>This completes the induction step.</p>
<h2 id="generalized-model">Generalized model</h2>
<p>Suppose that the terminal reward <span class="math inline">\(r_T(x)\)</span> is monotone increasing<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> in <span class="math inline">\(x\)</span>.</p>
<div class="highlight">
<dl>
<dt>Theorem</dt>
<dd><p>For the generalized optimal gambling problem:</p>
<ul>
<li>For each <span class="math inline">\(t\)</span>, the value function <span class="math inline">\(V_t(x)\)</span> is monotone increasing in <span class="math inline">\(x\)</span>.</li>
<li>For each <span class="math inline">\(x\)</span>, the value function <span class="math inline">\(V_t(x)\)</span> is monotone decreasing in <span class="math inline">\(t\)</span>.</li>
</ul>
</dd>
</dl>
</div>
<h4 id="proof-of-monotonicity-in-x">Proof of monotonicity in <span class="math inline">\(x\)</span></h4>
<p>We proceed by backward induction. <span class="math inline">\(V_T(x) = r_T(x)\)</span> which is monotone increasing in <span class="math inline">\(x\)</span>. Assume that <span class="math inline">\(V_{t+1}(x)\)</span> is increasing in <span class="math inline">\(x\)</span>. Now, consider <span class="math inline">\(V_t(x)\)</span>. Consider <span class="math inline">\(x_1, x_2 \in \reals_{\ge 0}\)</span> such that <span class="math inline">\(x_1 \le x_2\)</span>. Then for any <span class="math inline">\(u \le x_1\)</span>, we have that</p>
<p><span class="math display">\[ \begin{align}
    Q_t(x_1, u) &amp;= p V_{t+1}(x_1+u) + q V_{t+1}(x_1-u) \\
    &amp; \stackrel{(a)}{\le} p V_{t+1}(x_2 + u) + q V_{t+1}(x_2  - u) \\
    &amp; = Q_t(x_2, u),
  \end{align}
\]</span> where <span class="math inline">\((a)\)</span> uses the induction hypothesis. Now consider</p>
<p><span class="math display">\[ \begin{align}
  V_t(x_1) &amp;= \max_{u \in [0, x_1]} Q_t(x_1, u) \\
  &amp; \stackrel{(b)}{\le} \max_{u \in [0, x_1]} Q_t(x_2, u) \\
  &amp; \le \max_{u \in [0, x_2]} Q_t(x_2, u) \\
  &amp;= V_t(x_2),
  \end{align}
\]</span> where <span class="math inline">\((b)\)</span> uses monotonicity of <span class="math inline">\(Q_t\)</span> in <span class="math inline">\(x\)</span>. This completes the induction step.</p>
<h4 id="proof-of-monotonicity-in-t">Proof of monotonicity in <span class="math inline">\(t\)</span></h4>
<p>This is a simple consequence of the following:</p>
<p><span class="math display">\[V_t(x) = \max_{u \in [0, x]} Q_t(x,u) \ge Q_t(x,0) = V_{t+1}(x).\]</span></p>
<h2 id="exercises">Exercises</h2>
<p>The purpose of these series of exercises is to generalize the basic result to a model where the gambler can bet on many mutually exclusive outcomes (think of betting on multiple horses in a horse race).</p>
<ol type="1">
<li><p>Given positive numbers <span class="math inline">\((p_1, \dots, p_n)\)</span>, consider the following constraint optimization problem: <span class="math display">\[\max \sum_{i=1}^n p_i \log w_i\]</span> subject to:</p>
<ul>
<li><span class="math inline">\(w_i \ge 0\)</span></li>
<li><span class="math inline">\(\sum_{i=1}^n w_i \le x\)</span>.</li>
</ul>
<p>Show that the optimal solution is given by <span class="math display">\[ w_i = \frac{p_i}{p} x\]</span> where <span class="math inline">\(p = \sum_{i=1}^n p_i\)</span>.</p></li>
<li><p>Given positive numbers <span class="math inline">\((p_1, \dots, p_n)\)</span>, consider the following constraint optimization problem: <span class="math display">\[\max \sum_{i=1}^n p_i \log (x - u + nu_i)\]</span> subject to:</p>
<ul>
<li><span class="math inline">\(u_i \ge 0\)</span></li>
<li><span class="math inline">\(u = \sum_{i=1}^n u_i \le x\)</span>.</li>
</ul>
<p>Show that the optimal solution is given by <span class="math display">\[ u_i = \frac{p_i}{p} x\]</span> where <span class="math inline">\(p = \sum_{i=1}^n p_i\)</span>.</p></li>
<li><p>Consider an alternative of the optimal gambling problem where, at each time, the gambler can place bets on many mutually exclusive outcomes. Suppose there are <span class="math inline">\(n\)</span> outcomes, with success probabilities <span class="math inline">\((p_1, \dots, p_n)\)</span>. Let <span class="math inline">\((U_{1,t}, \dots, U_{n,t})\)</span> denote the amount that the gambler bet’s one each outcome. The total amount <span class="math inline">\(U_t := \sum_{i=1}^n U_{i,t}\)</span> must be less than the gambler’s fortune <span class="math inline">\(X_t\)</span>. If <span class="math inline">\(W_t\)</span> denotes the winning outcome, then the gambler’s wealth evolves according to <span class="math display">\[ X_{t+1} = X_t - U_t + nU_{W_t, t}.\]</span> For example, if there are three outcomes, gambler’s current wealth is <span class="math inline">\(x\)</span>, the gambler bets <span class="math inline">\((u_1, u_2, u_3)\)</span>, and outcome 2 wins, then the gambler wins <span class="math inline">\(3 u_2\)</span> and his fortune at the next time is <span class="math display">\[ x - (u_1 + u_2 + u_3) + 3 u_2. \]</span></p>
<p>The gambler’s utility is <span class="math inline">\(\log X_T\)</span>, the logarithm of his final wealth. Find the strategy that maximizes the gambler’s expected utility.</p>
<p><em>Hint:</em> Argue that the value function is of the form <span class="math inline">\(V_t(x) = \log x +  (T -t)C\)</span>, where <span class="math display">\[C = \log n - H(p_1, \dots, p_n)\]</span> where <span class="math inline">\(H(p_1, \dots, p_n) = - \sum_{i=1}^n p_i \log p_i\)</span> is the entropy of a random variable with pmf <span class="math inline">\((p_1, \dots, p_n)\)</span>.</p>
<dl>
<dt>Side Remark</dt>
<dd><p>The constant <span class="math inline">\(C\)</span> is the capacity of a symmetric discrete memoryless with <span class="math inline">\(n\)</span> outputs and for every input, the output probabilities are a permutation of <span class="math inline">\((p_1, \dots, p_n)\)</span>.</p>
</dd>
</dl></li>
</ol>
<h2 id="references">References</h2>
<p>The above model (including the model described in the exercise) was introduced by <span class="citation" data-cites="Kelly1956">Kelly (<a href="#ref-Kelly1956">1956</a>)</span>. However, Kelly restricted attention to “bet a constant fraction of your fortune” betting strategy and found the optimal fraction. This strategy is sometimes referred to as <a href="https://en.wikipedia.org/wiki/Kelly_criterion">Kelly criteria</a>. As far as I know, the dynamic programming treatment of the problem is due to <span class="citation" data-cites="Ross1974">Ross (<a href="#ref-Ross1974">1974</a>)</span>. Ross also considered variations where the objective was to maximize the probability of reaching a preassigned fortune or maximizing the time until becoming broke.</p>
<hr />
<div id="refs" class="references">
<div id="ref-Kelly1956">
<p><span class="smallcaps">Kelly, J.L., Jr.</span> 1956. A new interpretation of information rate. <em>Bell System Technical Journal</em> <em>35</em>, 4, 917–927. DOI: <a href="https://doi.org/10.1002/j.1538-7305.1956.tb03809.x">10.1002/j.1538-7305.1956.tb03809.x</a>.</p>
</div>
<div id="ref-Ross1974">
<p><span class="smallcaps">Ross, S.M.</span> 1974. Dynamic programming and gambling models. <em>Advances in Applied Probability</em> <em>6</em>, 3, 593–606. DOI: <a href="https://doi.org/10.2307/1426236">10.2307/1426236</a>.</p>
</div>
</div>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The constant <span class="math inline">\(C\)</span> defined above is equal to the capacity of a binary symmetric channel! In fact, the above model was introduced by <span class="citation" data-cites="Kelly1956">Kelly (<a href="#ref-Kelly1956">1956</a>)</span> to show a gambling interpretation of information rates.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>I use the convention that <em>increasing</em> means <em>weakly increasing</em>. The alternative term <em>non-decreasing</em> implicitly assumes that we are talking about a totally ordered set.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</section>


<p class="categories">
This entry waslast updated on 12 Sep 2018
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/mdp">
  MDP</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/gambling">gambling</a>,
<a href="https://adityam.github.io/stochastic-control/tags/closed-form-solution">closed form solution</a>,
<a href="https://adityam.github.io/stochastic-control/tags/structural-result">structural result</a>,
<a href="https://adityam.github.io/stochastic-control/tags/information-theory">information theory</a>.</p>



      <script type="text/javascript">
      
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-6887174-4']);
            _gaq.push(['_trackPageview']);
            (function() {
                var ga   = document.createElement('script');
                ga.type  = 'text/javascript';
                ga.async = true;
                ga.src   = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
              })();
      
      </script>
    </div>
  </body>
</html>


