<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="gambling,closed form solution,structural result,information theory" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2022
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Example: Optimal Gambling</div>

<figure>
<img src="/stochastic-control/banners/gambling.jpg" title="How to gamble if you must" style="width:100.0%" alt="Image credit: http://commons.wikimedia.org/wiki/File:Gambling-ca-1800.jpg" /><figcaption aria-hidden="true">Image credit: http://commons.wikimedia.org/wiki/File:Gambling-ca-1800.jpg</figcaption>
</figure>
<p>TL;DR <em>This stylized model of optimal gambling was introduced by <span class="citation" data-cites="Kelly1956">Kelly (<a href="#ref-Kelly1956" role="doc-biblioref">1956</a>)</span> to highlight a relationship between channel capacity (which had been proposed recently by Shannon), and gambling. Our motivation for studying this model is to use it as an illustrative example to show that sometimes it is possible to identify the optimal strategy and value function of MDPs in closed form.</em></p>
<hr />
<p>Imagine a gambler who goes to a casino with an initial fortune of <span class="math inline">\(s_1\)</span> dollars and places bets over time and must leave after <span class="math inline">\(T\)</span> bets. Let <span class="math inline">\(S_t\)</span> denote the gambler’s fortune after <span class="math inline">\(t\)</span> bets. In this example, time denotes the number of times that the gambler has bet.</p>
<p>At time <span class="math inline">\(t\)</span>, the gambler may place a bet for any amount <span class="math inline">\(A_t\)</span> less than or equal to his current fortune <span class="math inline">\(S_t\)</span>. If he wins the bet (denoted by the even <span class="math inline">\(W_t = 1\)</span>), the casino gives him the amount that he had bet. If he loses the bet (denoted by the event <span class="math inline">\(W_t = -1\)</span>), he pays the casino the amount that he had bet. Thus, the dynamics can be written as</p>
<p><span class="math display">\[ S_{t+1} = S_t + W_t A_t. \]</span></p>
<p>The outcomes of the bets <span class="math inline">\(\{W_t\}_{t \ge 1}\)</span> are <em>primitive random variables</em>, i.e., they are independent of each other, the gambler’s initial fortune, and his betting strategy.</p>
<p>The gambler’s utility is <span class="math inline">\(\log S_T\)</span>, the logarithm of his final fortune.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Thus, the reward function may be written as</p>
<p><span class="math display">\[ r_t(s, a) = 0 
   \quad \text{and} \quad
   r_T(s) = \log s.
\]</span></p>
<p>Find the strategy that maximizes the gambler’s utility, <span class="math inline">\(\EXP[\log S_T]\)</span>.</p>
<hr />
<p>To fix ideas, let’s try to find the optimal policy on our own. An example strategy is given below.</p>
<div id="observablehq-viewof-code" style="font-size:80%; font-family:monospace">

</div>
<p>Assuming that <span class="math inline">\(S_1 = 100\)</span>, we plot the performance of this policy below.</p>
<figure style="max-width:30em;">
<table style="border:none; width:60px;">
<tr style="width:60px">
<td id="observablehq-viewof-p" style="width:40px">
</td>
<td id="observablehq-viewof-run" style="width:20px">
</td>
</tr>
<tr>
<td style="text-align:left">
<div id="observablehq-viewof-strategy">

</div>
</td>
</tr>
</table>
<div id="observablehq-reward_plot">

</div>
<figcaption>
<b>Figure 1</b> Plot of the performance of the strategy for a horizon of <span class="math inline">\(T=100\)</span>. The curves in gray show the performance over <span class="math inline">\(n=25\)</span> difference sample paths and the red curve shows its mean. For ease of visualization, we are plotting the utility at each stage (i.e., <span class="math inline">\(\log s_t\)</span>), even though the reward is only received at the terminal time step. The red line shows the mean performance over the <span class="math inline">\(n\)</span> sample paths. The final mean value of the reward is shown in red. You can toggle the select strategy button to see how the optimal strategy performs (and how close you came to it).
</figcaption>
</figure>
<script type="module">
import {Runtime, Inspector} from "https://cdn.jsdelivr.net/npm/@observablehq/runtime@4/dist/runtime.js";
import define from "https://api.observablehq.com/d/5f730d266006f947.js?v=3";
new Runtime().module(define, name => {
  if (name === "reward_plot") return new Inspector(document.querySelector("#observablehq-reward_plot"));
  if (name === "viewof code") return new Inspector(document.querySelector("#observablehq-viewof-code"));
  if (name === "viewof strategy") return new Inspector(document.querySelector("#observablehq-viewof-strategy"));
  if (name === "viewof p") return new Inspector(document.querySelector("#observablehq-viewof-p"));
  if (name === "viewof run") return new Inspector(document.querySelector("#observablehq-viewof-run"));
  return ["user_strategy","bet","data","optimal_strategy"].includes(name);
});
</script>
<p>As we can see, most intuitive policies do not do so well. We will now see how to compute the optimal policy using dynamic programming.</p>
<hr />
<h1 data-number="1" id="optimal-gambling-strategy-and-value-functions"><span class="header-section-number">1</span> Optimal gambling strategy and value functions</h1>
<p>The above model of optimal gambling is a Markov decision process. Therefore, the optimal solution is given by dynamic programming.</p>
<div class="highlight">
<dl>
<dt>Dynamic program</dt>
<dd><p>Define the following value function <span class="math inline">\(V_t \colon \reals_{\ge 0} \to \reals\)</span> <span class="math display">\[ V_T(s) = \log s \]</span> and for <span class="math inline">\(t \in \{T-1, \dots, 1\}\)</span>: <span class="math display">\[ \begin{align*}
  Q_t(s,a) &amp;= \EXP[ r_t(s,a) + V_{t+1}(S_{t+1}) \,|\, S_t = s, A_t = a] \\
  &amp;= p V_{t+1}(s+a) + (1-p) V_{t+1}(s-a),
  \end{align*}
\]</span> and <span class="math display">\[ \begin{align*}
  V_t(s) &amp;=  \max_{a \in [0, s]} Q_t(s,a), \\
  π_t(s) &amp;= \arg \max_{a \in [0, s]} Q_t(s,a). \\
  \end{align*}
\]</span></p>
<p>Then the strategy <span class="math inline">\(π = (π_1, \dots, π_{T-1})\)</span> is optimal.</p>
</dd>
</dl>
</div>
<p>The above model is one of the rare instances when the optimal strategy and the optimal strategy and value function of an MDP can be identified in closed form.</p>
<div class="highlight">
<dl>
<dt>Theorem</dt>
<dd><p>When <span class="math inline">\(p \le 0.5\)</span>:</p>
<ul>
<li>the optimal strategy is to <em>not gamble</em>, specifically <span class="math inline">\(π_t(s) = 0\)</span>;</li>
<li>the value function is <span class="math inline">\(V_t(s) = \log s\)</span>.</li>
</ul>
<p>When <span class="math inline">\(p &gt; 0.5\)</span>:</p>
<ul>
<li>the optimal strategy is <em>to bet a fraction of the current fortune</em>, specifically <span class="math inline">\(π_t(s) = (2p - 1)s\)</span>;</li>
<li>the value function is <span class="math inline">\(V_t(s) = \log s + (T - t) C\)</span>, where<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <span class="math display">\[ C = \log 2 + p \log p + (1-p) \log (1-p).\]</span></li>
</ul>
</dd>
</dl>
</div>
<p>We prove the two cases separately.</p>
<details>
<summary>
<h4 class="unnumbered" id="proof-when-p-le-0.5">Proof when <span class="math inline">\(p \le 0.5\)</span></h4>
</summary>
<div>
<p>Let <span class="math inline">\(p = \PR(W_t = 1)\)</span> and <span class="math inline">\(q = \PR(W_t = -1)\)</span>. Then <span class="math inline">\(p \le 0.5\)</span> implies that <span class="math inline">\(p \le 1 - p = q\)</span>.</p>
<p>We proceed by backward induction. For <span class="math inline">\(t = T\)</span>, we have that <span class="math inline">\(V_T(s) = \log s\)</span>. This forms the basis of induction. Now assume that for <span class="math inline">\(t+1\)</span>, <span class="math inline">\(V_{t+1}(s) = \log s\)</span>. Now consider</p>
<p><span class="math display">\[ Q_t(s,a) = p V_{t+1}(s+a) + qV_{t+1}(s-a). \]</span></p>
<p>Differentiating both sides w.r.t. <span class="math inline">\(a\)</span>, we get <span class="math display">\[ \begin{align*} 
  \frac { \partial Q_t(s,a) } {\partial a} &amp;= 
   \frac p { s + a} - \frac q { s - a } 
   \\
   &amp; = \frac { (p - q) s - (p + q) a } { s^2 - a^2 } 
   \\
   &amp; =
   \frac { - (q - p) s - a } {s^2 - a^2 } 
   \\
   &amp;&lt; 0.
  \end{align*}   
\]</span></p>
<p>This implies that <span class="math inline">\(Q_t(s,a)\)</span> is decreasing in <span class="math inline">\(a\)</span>. Therefore,</p>
<p><span class="math display">\[ π_t(s) = \arg\max_{a \in [0, s]} Q_t(s,a) = 0. \]</span></p>
<p>Moreover, <span class="math display">\[ V_t(s) = Q_t(s, π_t(s)) = \log s.\]</span></p>
<p>This completes the induction step.</p>
</div>
</details>
<details>
<summary>
<h4 class="unnumbered" id="proof-when-p-0.5">Proof when <span class="math inline">\(p &gt; 0.5\)</span></h4>
</summary>
<div>
<p>As in the previous case, let <span class="math inline">\(p = \PR(W_t = 1)\)</span> and <span class="math inline">\(q = \PR(W_t = -1)\)</span>. Then <span class="math inline">\(p &gt; 0.5\)</span> implies that <span class="math inline">\(p &gt; 1 - p = q\)</span>.</p>
<p>We proceed by backward induction. For <span class="math inline">\(t = T\)</span>, we have that <span class="math inline">\(V_T(s) = \log s\)</span>. This forms the basis of induction. Now assume that for <span class="math inline">\(t+1\)</span>, <span class="math inline">\(V_{t+1}(s) = \log s + (T -t - 1)C\)</span>. Now consider</p>
<p><span class="math display">\[ Q_t(s,a) = p V_{t+1}(s+a) + qV_{t+1}(s-a). \]</span></p>
<p>Differentiating both sides w.r.t. <span class="math inline">\(a\)</span>, we get <span class="math display">\[ \begin{align*} 
  \frac { \partial Q_t(s,a) } {\partial a} &amp;= 
   \frac p { s + a} - \frac q { s - a } 
   \\
   &amp; = \frac { (p - q) s - (p + q) a } { s^2 - a^2 } 
   \\
   &amp; =
   \frac { (p - q) s - a } {s^2 - a^2 } 
  \end{align*}   
\]</span></p>
<p>Setting <span class="math inline">\(\partial Q_t(s,a)/\partial a = 0\)</span>, we get that the optimal action is</p>
<p><span class="math display">\[ π_t(s) = (p-q) s. \]</span></p>
<p>Note that <span class="math inline">\((p-q) \in (0,1]\)</span></p>
<p><span class="math display">\[ 
  \frac { \partial^2 Q_t(s,a) } {\partial a^2} = 
   - \frac p { (s + a)^2 } - \frac q { (s - a)^2 } 
  &lt; 0;
\]</span> hence the above action is indeed the maximizer. Moreover, <span class="math display">\[ \begin{align*} 
  V_t(s) &amp;= Q_t(s, π_t(s))  \\
  &amp;= p V_{t+1}(s + π_t(s)) + q V_{t+1}( s - π_t(s) )\\
  &amp;= \log s + p \log (1 + (p-q)) + q \log (1 - (p-q)) + (T - t -1)C \\
  &amp;= \log s + p \log 2p + q \log 2q + (T - t + 1)C \\
  &amp;= \log s + (T - t) C
  \end{align*}   
\]</span></p>
<p>This completes the induction step.</p>
</div>
</details>
<h1 data-number="2" id="generalized-model"><span class="header-section-number">2</span> Generalized model</h1>
<p>Suppose that the terminal reward <span class="math inline">\(r_T(s)\)</span> is monotone increasing<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> in <span class="math inline">\(s\)</span>.</p>
<div class="highlight">
<dl>
<dt>Theorem</dt>
<dd><p>For the generalized optimal gambling problem:</p>
<ul>
<li>For each <span class="math inline">\(t\)</span>, the value function <span class="math inline">\(V_t(s)\)</span> is monotone increasing in <span class="math inline">\(s\)</span>.</li>
<li>For each <span class="math inline">\(s\)</span>, the value function <span class="math inline">\(V_t(s)\)</span> is monotone decreasing in <span class="math inline">\(t\)</span>.</li>
</ul>
</dd>
</dl>
</div>
<details>
<summary>
<h4 class="unnumbered" id="proof-of-monotonicity-in-s">Proof of monotonicity in <span class="math inline">\(s\)</span></h4>
</summary>
<div>
<p>We proceed by backward induction. <span class="math inline">\(V_T(s) = r_T(s)\)</span> which is monotone increasing in <span class="math inline">\(s\)</span>. Assume that <span class="math inline">\(V_{t+1}(s)\)</span> is increasing in <span class="math inline">\(s\)</span>. Now, consider <span class="math inline">\(V_t(s)\)</span>. Consider <span class="math inline">\(s_1, s_2 \in \reals_{\ge 0}\)</span> such that <span class="math inline">\(s_1 \le s_2\)</span>. Then for any <span class="math inline">\(a \le s_1\)</span>, we have that</p>
<p><span class="math display">\[ \begin{align*}
    Q_t(s_1, a) &amp;= p V_{t+1}(s_1+a) + q V_{t+1}(s_1-a) \\
    &amp; \stackrel{(a)}{\le} p V_{t+1}(s_2 + a) + q V_{t+1}(s_2  - a) \\
    &amp; = Q_t(s_2, a),
  \end{align*}
\]</span> where <span class="math inline">\((a)\)</span> uses the induction hypothesis. Now consider</p>
<p><span class="math display">\[ \begin{align*}
  V_t(s_1) &amp;= \max_{a \in [0, s_1]} Q_t(s_1, a) \\
  &amp; \stackrel{(b)}{\le} \max_{a \in [0, s_1]} Q_t(s_2, a) \\
  &amp; \le \max_{a \in [0, s_2]} Q_t(s_2, a) \\
  &amp;= V_t(s_2),
  \end{align*}
\]</span> where <span class="math inline">\((b)\)</span> uses monotonicity of <span class="math inline">\(Q_t\)</span> in <span class="math inline">\(s\)</span>. This completes the induction step.</p>
</div>
</details>
<details>
<summary>
<h4 class="unnumbered" id="proof-of-monotonicity-in-t">Proof of monotonicity in <span class="math inline">\(t\)</span></h4>
</summary>
<div>
<p>This is a simple consequence of the following:</p>
<p><span class="math display">\[V_t(s) = \max_{a \in [0, s]} Q_t(s,a) \ge Q_t(s,0) = V_{t+1}(s).\]</span></p>
</div>
</details>
<h1 class="unnumbered" id="exercises">Exercises</h1>
<p>The purpose of these series of exercises is to generalize the basic result to a model where the gambler can bet on many mutually exclusive outcomes (think of betting on multiple horses in a horse race).</p>
<ol type="1">
<li><p>Given positive numbers <span class="math inline">\((p_1, \dots, p_n)\)</span>, consider the following constraint optimization problem: <span class="math display">\[\max \sum_{i=1}^n p_i \log w_i\]</span> subject to:</p>
<ul>
<li><span class="math inline">\(w_i \ge 0\)</span></li>
<li><span class="math inline">\(\sum_{i=1}^n w_i \le s\)</span>.</li>
</ul>
<p>Show that the optimal solution is given by <span class="math display">\[ w_i = \frac{p_i}{p} s\]</span> where <span class="math inline">\(p = \sum_{i=1}^n p_i\)</span>.</p></li>
<li><p>Given positive numbers <span class="math inline">\((p_1, \dots, p_n)\)</span>, consider the following constraint optimization problem: <span class="math display">\[\max \sum_{i=1}^n p_i \log (s - a + na_i)\]</span> subject to:</p>
<ul>
<li><span class="math inline">\(a_i \ge 0\)</span></li>
<li><span class="math inline">\(a = \sum_{i=1}^n a_i \le s\)</span>.</li>
</ul>
<p>Show that the optimal solution is given by <span class="math display">\[ a_i = \frac{p_i}{p} s\]</span> where <span class="math inline">\(p = \sum_{i=1}^n p_i\)</span>.</p></li>
<li><p>Consider an alternative of the optimal gambling problem where, at each time, the gambler can place bets on many mutually exclusive outcomes. Suppose there are <span class="math inline">\(n\)</span> outcomes, with success probabilities <span class="math inline">\((p_1, \dots, p_n)\)</span>. Let <span class="math inline">\((A_{1,t}, \dots, A_{n,t})\)</span> denote the amount that the gambler bets on each outcome. The total amount <span class="math inline">\(A_t := \sum_{i=1}^n A_{i,t}\)</span> must be less than the gambler’s fortune <span class="math inline">\(S_t\)</span>. If <span class="math inline">\(W_t\)</span> denotes the winning outcome, then the gambler’s wealth evolves according to <span class="math display">\[ S_{t+1} = S_t - A_t + nU_{W_t, t}.\]</span> For example, if there are three outcomes, gambler’s current wealth is <span class="math inline">\(s\)</span>, the gambler bets <span class="math inline">\((a_1, a_2, a_3)\)</span>, and outcome 2 wins, then the gambler wins <span class="math inline">\(3 a_2\)</span> and his fortune at the next time is <span class="math display">\[ s - (a_1 + a_2 + a_3) + 3 a_2. \]</span></p>
<p>The gambler’s utility is <span class="math inline">\(\log S_T\)</span>, the logarithm of his final wealth. Find the strategy that maximizes the gambler’s expected utility.</p>
<p><em>Hint:</em> Argue that the value function is of the form <span class="math inline">\(V_t(s) = \log s +  (T -t)C\)</span>, where <span class="math display">\[C = \log n - H(p_1, \dots, p_n)\]</span> where <span class="math inline">\(H(p_1, \dots, p_n) = - \sum_{i=1}^n p_i \log p_i\)</span> is the entropy of a random variable with pmf <span class="math inline">\((p_1, \dots, p_n)\)</span>.</p>
<dl>
<dt>Side Remark</dt>
<dd><p>The constant <span class="math inline">\(C\)</span> is the capacity of a symmetric discrete memoryless with <span class="math inline">\(n\)</span> outputs and for every input, the output probabilities are a permutation of <span class="math inline">\((p_1, \dots, p_n)\)</span>.</p>
</dd>
</dl></li>
</ol>
<h1 class="unnumbered" id="references">References</h1>
<p>The above model (including the model described in the exercise) was introduced by <span class="citation" data-cites="Kelly1956">Kelly (<a href="#ref-Kelly1956" role="doc-biblioref">1956</a>)</span>. However, Kelly restricted attention to “bet a constant fraction of your fortune” betting strategy and found the optimal fraction. This strategy is sometimes referred to as <a href="https://en.wikipedia.org/wiki/Kelly_criterion">Kelly criteria</a>. As far as I know, the dynamic programming treatment of the problem is due to <span class="citation" data-cites="Ross1974">Ross (<a href="#ref-Ross1974" role="doc-biblioref">1974</a>)</span>. Ross also considered variations where the objective was to maximize the probability of reaching a preassigned fortune or maximizing the time until becoming broke.</p>
<p>A generalization of the above model to general logarithmic and exponential utilities is presented in <span class="citation" data-cites="Fergurson2004">Ferguson and Gilstein (<a href="#ref-Fergurson2004" role="doc-biblioref">2004</a>)</span>.</p>
<hr />
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Fergurson2004" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Ferguson, T.S. and Gilstein, C.Z.</span> 2004. Optimal investment policies for the horse race model". Available at: <a href="https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf">https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf</a>.
</div>
<div id="ref-Kelly1956" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Kelly, J.L., Jr.</span> 1956. A new interpretation of information rate. <em>Bell System Technical Journal</em> <em>35</em>, 4, 917–926. DOI: <a href="https://doi.org/10.1002/j.1538-7305.1956.tb03809.x">10.1002/j.1538-7305.1956.tb03809.x</a>.
</div>
<div id="ref-Ross1974" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Ross, S.M.</span> 1974. Dynamic programming and gambling models. <em>Advances in Applied Probability</em> <em>6</em>, 3, 593–606. DOI: <a href="https://doi.org/10.2307/1426236">10.2307/1426236</a>.
</div>
</div>
<section class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>See the notes on <a href="../../risk-sensitive/risk-sensitive-utility">risk sensitive utility</a> for a discussion on the choice of the utility function.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>The constant <span class="math inline">\(C\)</span> defined above is equal to the capacity of a binary symmetric channel! In fact, the above model was introduced by <span class="citation" data-cites="Kelly1956">Kelly (<a href="#ref-Kelly1956" role="doc-biblioref">1956</a>)</span> to show a gambling interpretation of information rates.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>I use the convention that <em>increasing</em> means <em>weakly increasing</em>. The alternative term <em>non-decreasing</em> implicitly assumes that we are talking about a totally ordered set.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>


<p class="categories">
This entry 

 was last updated on 20 Jan 2022
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/mdp">
  MDP</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/gambling">gambling</a>,
<a href="https://adityam.github.io/stochastic-control/tags/closed-form-solution">closed form solution</a>,
<a href="https://adityam.github.io/stochastic-control/tags/structural-result">structural result</a>,
<a href="https://adityam.github.io/stochastic-control/tags/information-theory">information theory</a>.</p>



    </div>
  </body>
</html>


