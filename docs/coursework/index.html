<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_SVG,https://adityam.github.io/stochastic-control/js/mathjax-local.js">
  </script>
</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2020
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>






<dl>
   <dt>Assignments</dt>
   <dd>
   <ul>

    <li>
      <a href="https://adityam.github.io/stochastic-control/assignments/01/">Assignment 1</a>.
      Due date: 14 Jan 2018
      (10 marks)
    </li>

    <li>
      <a href="https://adityam.github.io/stochastic-control/assignments/02/">Assignment 2</a>.
      Due date: 21 Jan, 2020
      (10 marks)
    </li>

    <li>
      <a href="https://adityam.github.io/stochastic-control/assignments/03/">Assignment 3</a>.
      Due date: 28 Jan, 2020
      (10 marks)
    </li>

    <li>
      <a href="https://adityam.github.io/stochastic-control/assignments/04/">Assignment 4</a>.
      Due date: 6 Feb, 2020
      (10 marks)
    </li>

    <li>
      <a href="https://adityam.github.io/stochastic-control/assignments/05/">Assignment 5</a>.
      Due date: 13 Feb 2020
      (10 marks)
    </li>

    <li>
      <a href="https://adityam.github.io/stochastic-control/assignments/06/">Assignment 6</a>.
      Due date: 20 Feb, 2020
      (10 marks)
    </li>

    <li>
      <a href="https://adityam.github.io/stochastic-control/assignments/07/">Assignment 7</a>.
      Due date: 5 March, 2018
      (10 marks)
    </li>

</ul>


   </dd>
</dl>


<dl>
<dt>Project</dt>
<dd><ul>
<li><p>In the revised syllabus, the course project must be done individually. Each student must select a project topic by April 10, and the project report due on April 26, 2020.</p></li>
<li><p>The report must not be any longer than 10-15 pages. The grade distribution for different parts of the report are as follows.</p>
<ul>
<li>Background, presentation of the problem setting, and literature overview: 20%</li>
<li>Summary of the results, proof outlines, and examples to illustrate the main results: 50%</li>
<li>Critical evaluation of the contributions: 20%</li>
<li>Clarity of the presentation: 10%</li>
</ul></li>
<li><p>The potential project topics are listed below. You can also pick up on the discussion of the literature in the reference section of the notes. You are also free to pick topics outside of these topics. Please send me an email with the topic that you want to work on, and we can finalize which papers should be covered in the term project.</p></li>
</ul>
</dd>
</dl>
<hr />
<h1 data-number="1" id="selected-projects"><span class="header-section-number">1</span> Selected Projects</h1>
<ul>
<li>Alex: ??</li>
<li>Alper: “Thrify” and “Equalizing” policies: <span class="citation" data-cites="Karatzas2010">Karatzas and Sudderth (<a href="#ref-Karatzas2010" role="doc-biblioref">2010</a>)</span>.</li>
<li>Armo: Adaptive control</li>
<li>Erfan: ??</li>
<li>Fatih: ??</li>
<li>Feras: ??</li>
<li>Imen: Average cost MDPs based on <span class="citation" data-cites="Bertsekas2013">Bertsekas (<a href="#ref-Bertsekas2013" role="doc-biblioref">2013</a>)</span>; <span class="citation" data-cites="Puterman2014">Puterman (<a href="#ref-Puterman2014" role="doc-biblioref">2014</a>)</span></li>
<li>Imene: ??</li>
<li>Jilan: Numerical algorithms for solving POMDPs</li>
<li>Mohammad: Remote estimation Lipsa Martins and Charkavorty Mahajan</li>
<li>Sadia: RL for radio resource management (exact papers to be selected)</li>
<li>Yacine: Multi-armed bandit</li>
</ul>
<hr />
<h1 data-number="2" id="potential-project-topics"><span class="header-section-number">2</span> Potential Project topics</h1>
<p>This is a partial list of topics. If you have a specific topic or direction in mind, please feel free to discuss that with me.</p>
<h2 data-number="2.1" id="interesting-papers"><span class="header-section-number">2.1</span> Interesting papers</h2>
<p>These are a biased collection of interesting papers and could form the basis of a term project. For some of these, you might have to combine them with other follow up work to have substantial material for a term project.</p>
<ul>
<li><p>An interesting characterization of optimal policies (for general decision problems) is that they must be “thrifty” and “equalizing”. An interpretation of this property for MDPs is presented in <span class="citation" data-cites="Karatzas2010">Karatzas and Sudderth (<a href="#ref-Karatzas2010" role="doc-biblioref">2010</a>)</span>.</p></li>
<li><p>In the class, we primarily focussed on structural properties for finite horizon models. An obvious next question is—when do these properties hold for infinite horizon models. An answer to this question in broad generality is provided in <span class="citation" data-cites="Smith2002">Smith and McCardle (<a href="#ref-Smith2002" role="doc-biblioref">2002</a>)</span>. They show that many of these results can be viewed as properties of closed convex cones.</p></li>
<li></li>
</ul>
<h2 data-number="2.2" id="average-cost-mdps"><span class="header-section-number">2.2</span> Average cost MDPs</h2>
<p>This is a topic which I usually cover in the course, but could not cover it this term because of the lost time. A basic treatment of average cost MDPs with finite state and finite action is covered in all standard books: <span class="citation" data-cites="KumarVaraiya1986">Kumar and Varaiya (<a href="#ref-KumarVaraiya1986" role="doc-biblioref">1986</a>)</span>, <span class="citation" data-cites="Puterman2014">Puterman (<a href="#ref-Puterman2014" role="doc-biblioref">2014</a>)</span>, <span class="citation" data-cites="Bertsekas:book">Bertsekas (<a href="#ref-Bertsekas:book" role="doc-biblioref">2011</a>)</span>. An accessible treatment for countable or uncountable state spaces is presented in <span class="citation" data-cites="Sennott:book">Sennott (<a href="#ref-Sennott:book" role="doc-biblioref">1999</a>)</span>. A detailed survey of many of the technical results for average cost models over general state spaces is in <span class="citation" data-cites="Arapostathis1993">Arapostathis et al. (<a href="#ref-Arapostathis1993" role="doc-biblioref">1993</a>)</span>.</p>
<ul>
<li><p>A potential term project on this topic could cover the derivation of the average cost dynamic program under the weak accessibility conditions and a discussion of the value iteration, policy iteration, and linear programming algorithms to solve average cost MDPs.</p></li>
<li><p>An interesting connection with discounted cost models is the vanishing discount approach. This is not deep enough to be a term project on its own, but it can either be part of a broader project or connect with Tauberian theorems in analysis.</p></li>
<li><p>Another potential topic for a term project related to this topic is approximation bounds for average cost problems. There is some material on approximate dynamic programming for average cost MDPs in <span class="citation" data-cites="Bertsekas2013">Bertsekas (<a href="#ref-Bertsekas2013" role="doc-biblioref">2013</a>)</span>. Some recent results on error bounds for discretization of average cost MDPs are in Saldi, Linder, Yuksel (add citation).</p></li>
</ul>
<h2 data-number="2.3" id="multi-armed-bandits"><span class="header-section-number">2.3</span> Multi-armed bandits</h2>
<p>Multi-armed bandits<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> are a special class of resouce allocation problems which have a low complexity solution. In general, scheduling problems can be modeled as MDPs, but the state space of such problems in exponential in the number of alternatives. This is sometimes referred to as the <em>curse of dimensionality</em>. A conceptual breakthrough in such problem was achieved when <span class="citation" data-cites="Gittins1974">Gittins and Jones (<a href="#ref-Gittins1974" role="doc-biblioref">1974</a>)</span>; <span class="citation" data-cites="Gittins1979">Gittins (<a href="#ref-Gittins1979" role="doc-biblioref">1979</a>)</span> showed that the a low complexity solution exists.</p>
<ul>
<li><p>A potential term project on this topic is to review the basic result of <span class="citation" data-cites="Gittins1979">Gittins (<a href="#ref-Gittins1979" role="doc-biblioref">1979</a>)</span>. The proof in <span class="citation" data-cites="Gittins1979">Gittins (<a href="#ref-Gittins1979" role="doc-biblioref">1979</a>)</span> uses interchange argument, which we did not cover in the course. One option is to overview the method of interchange arguments, and then explain Gittins result using that. Another alternative is to present the dynamic programming proof of Gittins result, which was given by <span class="citation" data-cites="Whittle1980">Whittle (<a href="#ref-Whittle1980" role="doc-biblioref">1980</a>)</span>. Since then multiple proofs of Gittins result have been presented in the literature. A survey of the four most popular results is presented in <span class="citation" data-cites="Frostig2016">Frostig and Weiss (<a href="#ref-Frostig2016" role="doc-biblioref">2016</a>)</span>.</p></li>
<li><p>Another potential term project is to present the restart-in-state formulation of the multi-armed problem presented in <span class="citation" data-cites="Katehakis1987">Katehakis and Veinott (<a href="#ref-Katehakis1987" role="doc-biblioref">1987</a>)</span>. An interesting generalization of this idea for multi-armed bandits with commitments is presented in <span class="citation" data-cites="Cowan2015">Cowan and Katehakis (<a href="#ref-Cowan2015" role="doc-biblioref">2015</a>)</span>.</p></li>
<li><p>The result of <span class="citation" data-cites="Gittins1979">Gittins (<a href="#ref-Gittins1979" role="doc-biblioref">1979</a>)</span> is derived under the assumption that the alternatives that are not selected do not evolve. Models where this assumption is violated are called restless bandits <span class="citation" data-cites="Whittle1988">(Whittle <a href="#ref-Whittle1988" role="doc-biblioref">1988</a>)</span>. Another potential term project is to present an overview of <span class="citation" data-cites="Gittins1979">Gittins (<a href="#ref-Gittins1979" role="doc-biblioref">1979</a>)</span>, <span class="citation" data-cites="Whittle1988">Whittle (<a href="#ref-Whittle1988" role="doc-biblioref">1988</a>)</span>, and other generalizations of multi-armed bandits. See <span class="citation" data-cites="Mahajan2008">Mahajan and Teneketzis (<a href="#ref-Mahajan2008" role="doc-biblioref">2008</a>)</span> for an overview.</p></li>
</ul>
<h2 data-number="2.4" id="monotonicity-of-optimal-policies"><span class="header-section-number">2.4</span> Monotonicity of optimal policies</h2>
<p>When we studied monotonicity of optimal policies in class, we assumed that the state and action space were totally ordered. Some of these results generalize to models where the state and action spaces are not totally ordered. An interesting example of such a result is presented in <span class="citation" data-cites="Papadaki2007">Papadaki and Powell (<a href="#ref-Papadaki2007" role="doc-biblioref">2007</a>)</span>. General conditions for the value function and optimal policy to be monotone are derived in <span class="citation" data-cites="Topkis1998">Topkis (<a href="#ref-Topkis1998" role="doc-biblioref">1998</a>)</span>.</p>
<ul>
<li><p>One potential term project on this topic is to review the the lattice structure of partially ordered sets and explain how the monotonicity results derived in class can be extended to such sets following the exposition in <span class="citation" data-cites="Topkis1998">Topkis (<a href="#ref-Topkis1998" role="doc-biblioref">1998</a>)</span>.</p></li>
<li><p>Another potential term project is to review monotonicity results for queueing models. There is a rich history on this topic but perhaps the most comprehensive survey is by <span class="citation" data-cites="Koole2006">Koole (<a href="#ref-Koole2006" role="doc-biblioref">2006</a>)</span>.</p></li>
<li><p>Another potential term project is to present the generalized monotonicity results for optimal stopping. For example <span class="citation" data-cites="Oh2016">Oh and Özer (<a href="#ref-Oh2016" role="doc-biblioref">2016</a>)</span> presents sufficient conditions under which the optimal policy has a control-band type structure. Oh’s PhD thesis also has interesting results and examples on this topic.</p></li>
</ul>
<h2 data-number="2.5" id="sensitivity-based-analysis-and-event-based-optimization-of-mdps"><span class="header-section-number">2.5</span> Sensitivity-based analysis and event-based optimization of MDPs</h2>
<p>For infinite horizon MDPs, an alternative approach to value iteration and policy iteration methods studied in class is to investigate the sensitivity of the average performance to the parameters of the Markov policy. Such an approach is called perturbation analysis in the MDP literature and policy-gradient approach in the RL literature. The sensitivity based view of MDPs can be used to derive an event-based approach to MDPs. The idea of the event based approach is to optimize based on “events” rather than on states.</p>
<ul>
<li><p>One potential term project is to review the results on event-based optimization following the suvery paper <span class="citation" data-cites="Cao2005">Cao (<a href="#ref-Cao2005" role="doc-biblioref">2005</a>)</span> or specific chapters from <span class="citation" data-cites="Cao2007">Cao (<a href="#ref-Cao2007" role="doc-biblioref">2007</a>)</span>.</p></li>
<li><p>Another potential term project is to present a specific applications of event-based optimization. Look at the references in <span class="citation" data-cites="Cao2005">Cao (<a href="#ref-Cao2005" role="doc-biblioref">2005</a>)</span>; <span class="citation" data-cites="Cao2007">Cao (<a href="#ref-Cao2007" role="doc-biblioref">2007</a>)</span> for examples.</p></li>
<li><p>Another option is to review these ideas as used in the RL literature. For senstivity-based analysis (which is known as policy-gradient methods), see the recent survey by <span class="citation" data-cites="Agarwal2019">Agarwal et al. (<a href="#ref-Agarwal2019" role="doc-biblioref">2019</a>)</span>.</p></li>
</ul>
<h2 data-number="2.6" id="constrained-mdps."><span class="header-section-number">2.6</span> Constrained MDPs.</h2>
<p>We briefly talked about constrained MDPs in the notes on <a href="../inf-mdp/linear-programming">linear programming formulation</a>. Potential term projects on this topic include:</p>
<ul>
<li><p>A detailed discussion of the linear programming formulation of constrained MDPs, following the presentation of <span class="citation" data-cites="Altman1999">Altman (<a href="#ref-Altman1999" role="doc-biblioref">1999</a>)</span>.</p></li>
<li><p>A discussion of the convex analytic approach of <span class="citation" data-cites="Borkar1988">Borkar (<a href="#ref-Borkar1988" role="doc-biblioref">1988</a>)</span>. An accessible introduction to this approach is presented in <span class="citation" data-cites="Borkar2002">Borkar (<a href="#ref-Borkar2002" role="doc-biblioref">2002</a>)</span>. The main idea of this approach is to think in terms of the occupancy measure of MDPs and cast the optimization problem as a convex programming problem in the space of probability measures.</p></li>
</ul>
<h2 data-number="2.7" id="stochastic-dual-dynamic-programming-sddp"><span class="header-section-number">2.7</span> Stochastic dual dynamic programming (SDDP)</h2>
<p>SDDP uses ideas from convex optimization to efficiently solve MDPs over Euclean state space. The method originated in <span class="citation" data-cites="Pereira1991">Pereira and Pinto (<a href="#ref-Pereira1991" role="doc-biblioref">1991</a>)</span> and has been extensively used in the energy optimization sector. A potential term project on this topic is to review the results of <span class="citation" data-cites="Shapiro2011">Shapiro (<a href="#ref-Shapiro2011" role="doc-biblioref">2011</a>)</span>.</p>
<h2 data-number="2.8" id="robust-mdps"><span class="header-section-number">2.8</span> Robust MDPs</h2>
<p>Distributionally robust MDPs.</p>
<h2 data-number="2.9" id="numerical-methods-to-efficiently-solve-pomdps"><span class="header-section-number">2.9</span> Numerical methods to efficiently solve POMDPs</h2>
<p>See the remark at the end of the notes on <a href="../pomdp/pomdp">POMDPs</a></p>
<h2 data-number="2.10" id="risk-sensitive-pomdps"><span class="header-section-number">2.10</span> Risk sensitive POMDPs</h2>
<p>Baras et al., Fernandez et al. </p>
<h2 data-number="2.11" id="adaptive-control-of-linear-systems"><span class="header-section-number">2.11</span> Adaptive control of linear systems</h2>
<p>See the chapter in <span class="citation" data-cites="KumarVaraiya1986">Kumar and Varaiya (<a href="#ref-KumarVaraiya1986" role="doc-biblioref">1986</a>)</span> for references.</p>
<h2 data-number="2.12" id="adaptive-control-of-markov-chains."><span class="header-section-number">2.12</span> Adaptive control of Markov chains.</h2>
<p>See the chapter in <span class="citation" data-cites="KumarVaraiya1986">Kumar and Varaiya (<a href="#ref-KumarVaraiya1986" role="doc-biblioref">1986</a>)</span> for references.</p>
<h2 data-number="2.13" id="reinforcement-learning"><span class="header-section-number">2.13</span> Reinforcement learning</h2>
<p>Value based methods (Q-learning, linear function approximation), policy-based method (policy gradient methods, discussed above). Options (related to event-based optimization).</p>
<h2 data-number="2.14" id="applications-of-mdps"><span class="header-section-number">2.14</span> Applications of MDPs</h2>
<p>You could pick any topic on application of MDPs. Here I list some of the references which survey applications in different areas:</p>
<ul>
<li>Communication: <span class="citation" data-cites="Altman2002">Altman (<a href="#ref-Altman2002" role="doc-biblioref">2002</a>)</span></li>
<li>Water Reservoir Management: <span class="citation" data-cites="Lamond2002">Lamond and Boukhtouta (<a href="#ref-Lamond2002" role="doc-biblioref">2002</a>)</span></li>
</ul>
<h2 data-number="2.15" id="applications-of-pomdps"><span class="header-section-number">2.15</span> Applications of POMDPs</h2>
<h2 data-number="2.16" id="applications-of-decentralized-control-systems."><span class="header-section-number">2.16</span> Applications of decentralized control systems.</h2>
<ul>
<li><p>Real-time communication: Witsenhausen, Walrand Varaiya, Teneketzis, Mahajan and Teneketzis.</p></li>
<li><p>Networked control systems: Walrand Varaiya, Mahajan and Teneketzis</p></li>
<li><p>Remote estimation: Lipsa and Martins, Charkavorty and Mahajan, Nayyar et al.</p></li>
<li><p>Mean-field teams: Arabneydi and Mahajan.</p></li>
</ul>
<hr />
<h1 class="unnumbered" data-number="" id="references">References</h1>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Agarwal2019">
<p><span class="smallcaps">Agarwal, A., Kakade, S.M., Lee, J.D., and Mahajan, G.</span> 2019. Optimality and approximation with policy gradient methods in markov decision processes.. Available at: <a href="http://arxiv.org/abs/1908.00261v2">http://arxiv.org/abs/1908.00261v2</a>.</p>
</div>
<div id="ref-Altman1999">
<p><span class="smallcaps">Altman, E.</span> 1999. <em>Constrained markov decision processes</em>. CRC Press. Available at: <a href="http://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf">http://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf</a>.</p>
</div>
<div id="ref-Altman2002">
<p><span class="smallcaps">Altman, E.</span> 2002. Applications of markov decision processes in communication networks. In: <em>International series in operations research &amp; management science</em>. Springer US, 489–536. DOI: <a href="https://doi.org/10.1007/978-1-4615-0805-2_16">10.1007/978-1-4615-0805-2_16</a>.</p>
</div>
<div id="ref-Arapostathis1993">
<p><span class="smallcaps">Arapostathis, A., Borkar, V.S., Fernandez-Gaucherand, E., Ghosh, M.K., and Marcus, S.I.</span> 1993. Discrete-time controlled Markov processes with average cost criterion - a survey. <em>SIAM Journal of Control and Optimization</em> <em>31</em>, 2, 282–344.</p>
</div>
<div id="ref-Bertsekas:book">
<p><span class="smallcaps">Bertsekas, D.P.</span> 2011. <em>Dynamic programming and optimal control</em>. Athena Scientific. Available at: <a href="http://www.athenasc.com/dpbook.html">http://www.athenasc.com/dpbook.html</a>.</p>
</div>
<div id="ref-Bertsekas2013">
<p><span class="smallcaps">Bertsekas, D.P.</span> 2013. <em>Abstract dynamic programming</em>. Athena Scientific Belmont. Available at: <a href="https://web.mit.edu/dimitrib/www/abstractdp_MIT.html">https://web.mit.edu/dimitrib/www/abstractdp_MIT.html</a>.</p>
</div>
<div id="ref-Borkar2002">
<p><span class="smallcaps">Borkar, V.S.</span> 2002. Convex analytic methods in markov decision processes. In: <em>International series in operations research &amp; management science</em>. Springer US, 347–375. DOI: <a href="https://doi.org/10.1007/978-1-4615-0805-2_11">10.1007/978-1-4615-0805-2_11</a>.</p>
</div>
<div id="ref-Borkar1988">
<p><span class="smallcaps">Borkar, V.S.</span> 1988. A convex analytic approach to markov decision processes. <em>Probability Theory and Related Fields</em> <em>78</em>, 4, 583–602. DOI: <a href="https://doi.org/10.1007/bf00353877">10.1007/bf00353877</a>.</p>
</div>
<div id="ref-Cao2007">
<p><span class="smallcaps">Cao, X.-R.</span> 2007. <em>Stochastic learning and optimization</em>. Springer. DOI: <a href="https://doi.org/10.1007/978-0-387-69082-7">10.1007/978-0-387-69082-7</a>.</p>
</div>
<div id="ref-Cao2005">
<p><span class="smallcaps">Cao, X.-R.</span> 2005. Basic ideas for event-based optimization of markov systems. <em>Discrete Event Dynamic Systems</em> <em>15</em>, 2, 169–197. DOI: <a href="https://doi.org/10.1007/s10626-004-6211-4">10.1007/s10626-004-6211-4</a>.</p>
</div>
<div id="ref-Cowan2015">
<p><span class="smallcaps">Cowan, W. and Katehakis, M.N.</span> 2015. Multi-armed bandits under general deprecation and commitment. <em>Probability in the Engineering and Informational Sciences</em> <em>29</em>, 1, 51–76. DOI: <a href="https://doi.org/10.1017/s0269964814000217">10.1017/s0269964814000217</a>.</p>
</div>
<div id="ref-Frostig2016">
<p><span class="smallcaps">Frostig, E. and Weiss, G.</span> 2016. Four proofs of Gittins’ multiarmed bandit theorem. <em>Annals of Operations Research</em> <em>241</em>, 1, 127–165. DOI: <a href="https://doi.org/10.1007/s10479-013-1523-0">10.1007/s10479-013-1523-0</a>.</p>
</div>
<div id="ref-Gittins1979">
<p><span class="smallcaps">Gittins, J.C.</span> 1979. Bandit processes and dynamic allocation indices. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> <em>41</em>, 2, 148–177.</p>
</div>
<div id="ref-Gittins1974">
<p><span class="smallcaps">Gittins, J.C. and Jones, D.M.</span> 1974. A dynamic allocation index for the discounted multiarmed bandit problem. In: <em>Progress in statistics</em>. North-Holland, Amsterdam, Netherlands, 241–266.</p>
</div>
<div id="ref-Karatzas2010">
<p><span class="smallcaps">Karatzas, I. and Sudderth, W.D.</span> 2010. Two characterizations of optimality in dynamic programming. <em>Applied Mathematics and Optimization</em> <em>61</em>, 3, 421–434. DOI: <a href="https://doi.org/10.1007/s00245-009-9093-x">10.1007/s00245-009-9093-x</a>.</p>
</div>
<div id="ref-Katehakis1987">
<p><span class="smallcaps">Katehakis, M.N. and Veinott, A.F.</span> 1987. The multi-armed bandit problem: Decomposition and computation. <em>Mathematics of Operations Research</em> <em>12</em>, 2, 262–268.</p>
</div>
<div id="ref-Koole2006">
<p><span class="smallcaps">Koole, G.</span> 2006. Monotonicity in markov reward and decision chains: Theory and applications. <em>Foundations and Trends in Stochastic Systems</em> <em>1</em>, 1, 1–76. DOI: <a href="https://doi.org/10.1561/0900000002">10.1561/0900000002</a>.</p>
</div>
<div id="ref-KumarVaraiya1986">
<p><span class="smallcaps">Kumar, P.R. and Varaiya, P.</span> 1986. <em>Stochastic systems: Estimation identification and adaptive control</em>. Prentice Hall.</p>
</div>
<div id="ref-LaiRobbins1985">
<p><span class="smallcaps">Lai, T.L. and Robbins, H.</span> 1985. Asymptotically efficient adaptive allocation rules. <em>Advances in Applied Mathematics</em> <em>6</em>, 1, 4–22. DOI: <a href="https://doi.org/http://dx.doi.org/10.1016/0196-8858(85)90002-8">http://dx.doi.org/10.1016/0196-8858(85)90002-8</a>.</p>
</div>
<div id="ref-Lamond2002">
<p><span class="smallcaps">Lamond, B.F. and Boukhtouta, A.</span> 2002. Water reservoir applications of markov decision processes. In: <em>International series in operations research &amp; management science</em>. Springer US, 537–558. DOI: <a href="https://doi.org/10.1007/978-1-4615-0805-2_17">10.1007/978-1-4615-0805-2_17</a>.</p>
</div>
<div id="ref-Mahajan2008">
<p><span class="smallcaps">Mahajan, A. and Teneketzis, D.</span> 2008. Foundations and applications of sensor management. In: Springer-Verlag, 121–151.</p>
</div>
<div id="ref-Oh2016">
<p><span class="smallcaps">Oh, S. and Özer, Ö.</span> 2016. Characterizing the structure of optimal stopping policies. <em>Production and Operations Management</em> <em>25</em>, 11, 1820–1838. DOI: <a href="https://doi.org/10.1111/poms.12579">10.1111/poms.12579</a>.</p>
</div>
<div id="ref-Papadaki2007">
<p><span class="smallcaps">Papadaki, K. and Powell, W.B.</span> 2007. Monotonicity in multidimensional markov decision processes for the batch dispatch problem. <em>Operations Research Letters</em> <em>35</em>, 2, 267–272. DOI: <a href="https://doi.org/10.1016/j.orl.2006.03.013">10.1016/j.orl.2006.03.013</a>.</p>
</div>
<div id="ref-Pereira1991">
<p><span class="smallcaps">Pereira, M.V.F. and Pinto, L.M.V.G.</span> 1991. Multi-stage stochastic optimization applied to energy planning. <em>Mathematical Programming</em> <em>52</em>, 1-3, 359–375. DOI: <a href="https://doi.org/10.1007/bf01582895">10.1007/bf01582895</a>.</p>
</div>
<div id="ref-Puterman2014">
<p><span class="smallcaps">Puterman, M.L.</span> 2014. <em>Markov decision processes: Discrete stochastic dynamic programming</em>. John Wiley &amp; Sons. DOI: <a href="https://doi.org/10.1002/9780470316887">10.1002/9780470316887</a>.</p>
</div>
<div id="ref-Sennott:book">
<p><span class="smallcaps">Sennott, L.I.</span> 1999. <em>Stochastic dynamic programming and the control of queueing systems</em>. Wiley, New York, NY, USA.</p>
</div>
<div id="ref-Shapiro2011">
<p><span class="smallcaps">Shapiro, A.</span> 2011. Analysis of stochastic dual dynamic programming method. <em>European Journal of Operational Research</em> <em>209</em>, 1, 63–72. DOI: <a href="https://doi.org/10.1016/j.ejor.2010.08.007">10.1016/j.ejor.2010.08.007</a>.</p>
</div>
<div id="ref-Smith2002">
<p><span class="smallcaps">Smith, J.E. and McCardle, K.F.</span> 2002. Structural properties of stochastic dynamic programs. <em>Operations Research</em> <em>50</em>, 5, 796–809. DOI: <a href="https://doi.org/10.1287/opre.50.5.796.365">10.1287/opre.50.5.796.365</a>.</p>
</div>
<div id="ref-Topkis1998">
<p><span class="smallcaps">Topkis, D.M.</span> 1998. <em>Supermodularity and complementarity</em>. Princeton University Press.</p>
</div>
<div id="ref-Whittle1980">
<p><span class="smallcaps">Whittle, P.</span> 1980. Multi-armed bandits and the Gittins index. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em> <em>42</em>, 2, 143–149.</p>
</div>
<div id="ref-Whittle1988">
<p><span class="smallcaps">Whittle, P.</span> 1988. Restless bandits: Activity allocation in a changing world. <em>Journal of applied probability</em> <em>25</em>, A, 287–298.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>There are two classes of models that go under the heading of multi-armed bandits. One is the class of models that I have described here. The second are a different class of models which build on the work of <span class="citation" data-cites="LaiRobbins1985">Lai and Robbins (<a href="#ref-LaiRobbins1985" role="doc-biblioref">1985</a>)</span>, which derived regret bounds for such models. These days the latter class of models are more popular, primarily because they have been used extensively in algorithms which display online advertisements.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>


      <script type="text/javascript">
      
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-6887174-4']);
            _gaq.push(['_trackPageview']);
            (function() {
                var ga   = document.createElement('script');
                ga.type  = 'text/javascript';
                ga.async = true;
                ga.src   = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
              })();
      
      </script>
    </div>
  </body>
</html>


