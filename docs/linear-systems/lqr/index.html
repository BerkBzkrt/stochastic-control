<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="Linear systems,Riccati equation,LQR,Optimal tracking" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2020
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Linear Quadratic Regulation (LQR)</div>

<p><em>Note: To be consistent with the notation used in linear systems, we denote the state and action by lowercase <span class="math inline">\(x\)</span> and <span class="math inline">\(u\)</span>, even for stochastic systems (unlike the notation used for other models where we use uppercase <span class="math inline">\(X\)</span> and <span class="math inline">\(U\)</span> for state and actions to emphasize the fact they are random variables).</em></p>
<p>We start by considering a <em>determinisitc</em> linear system with state <span class="math inline">\(x_t \in \reals^n\)</span> and control actions <span class="math inline">\(u_t \in \reals^m\)</span> and dynamics <span class="math display">\[ x_{t+1} = A_t x_t + B_t u_t,\]</span> where <span class="math inline">\(A_t \in \reals^{n \times n}\)</span> and <span class="math inline">\(B_t \in \reals^{n \times m}\)</span> are known matrices. The objective is to choose the control actions to minimize the finite horizon cost given by <span class="math display">\[ \sum_{t=1}^{T-1} c_t(x_t, u_t) + c_T(x_T),\]</span> where <span class="math inline">\(c_t(x_t, u_t)\)</span> is the per-step cost and <span class="math inline">\(c_T(x_T)\)</span> is the terminal cost. Depending on the cost, such models can be classified as follows:</p>
<ul>
<li><p><strong>Regulation problem</strong> where the objective is to keep the state of the system close to origin. These are modeled by considering that there are sequence of <a href="../../appendix/positive-definite-matrix">positive semi-definite matrices</a> <span class="math inline">\(\{Q_t\}_{t=1}^{T}\)</span> and <a href="../../appendix/positive-definite-matrix">positive definite matrices</a> <span class="math inline">\(\{R_t\}_{t=1}^{T-1}\)</span> where <span class="math display">\[
  c_t(x_t, u_t) = x_t^\TRANS Q_t x_t + u_t^\TRANS R_t u_t
  \quad\text{and}\quad
  c_T(x_T) = x_T^\TRANS Q_T x_T.
\]</span> This is often referred to as the <em>Linear Quadratic Regulartor (LQR)</em>.</p></li>
<li><p><strong>Tracking problem</strong> where it is assumed that the system has an output <span class="math inline">\(y_t = C_t x_t\)</span> and the objective is to keep the output of the system close to a pre-specified trajectory <span class="math inline">\(\{y^\circ_t\}_{t=1}^T\)</span>. These are modeled by considering the per-step cost as <span class="math display">\[ \begin{align*}
  c_t(x_t, u_t) &amp;= (C_tx_t - y^\circ_t)^\TRANS Q_t (C_tx_t - y^\circ_t) + u_t^\TRANS R_t u_t
  \\
  \text{and}\quad
  c_T(x_T) &amp;= (C_Tx_T - y^\circ_T)^\TRANS Q_T (C_Tx_T - y^\circ_T).
\end{align*} \]</span></p></li>
</ul>
<h1 data-number="1" id="optimal-solution-of-the-regulation-problem"><span class="header-section-number">1</span> Optimal solution of the regulation problem</h1>
<p>The LQR problem is one of the few instances where the solution of the Markov decision process can be obtained in closed form.</p>
<div class="highlight">
<dl>
<dt><span id="thm:lqr" class="pandoc-numbering-text thm"><strong>Theorem 1</strong></span></dt>
<dd><p>The value function at time <span class="math inline">\(t\)</span> is <span class="math display">\[\begin{equation}\label{eq:Vt}
  V_t(x_t) = x_t^\TRANS S_t x_t
\end{equation}\]</span> and the optimal control action is <span class="math display">\[\begin{equation}\label{eq:gt}
  u_t = - L_t x_t
\end{equation}\]</span> where the <em>gain matrices</em> <span class="math inline">\(\{L_t\}_{t\ge 1}\)</span> are given by: <span class="math display">\[ L_t = [R_t + B_t^\TRANS S_{t+1} B_t]^{-1} \Lambda_t \]</span> where <span class="math display">\[ \Lambda_t = B_t^\TRANS S_{t+1} A_t \]</span> and <span class="math inline">\(\{S_t\}_{t \ge 1}\)</span> are determined by the <em>backward Riccati difference equations</em>: <span class="math inline">\(S_T = Q_T\)</span> and for <span class="math inline">\(t \in \{T-1, \dots, 1\}\)</span>: <span class="math display">\[\begin{equation}\label{eq:riccati}
  S_t = A_t^\TRANS S_{t+1} A_t + Q_t - 
  \Lambda_t^\TRANS [ R_t + B_t^\TRANS S_{t+1} B_t ] ^{-1} \Lambda_t. 
\end{equation}\]</span></p>
</dd>
</dl>
</div>
<dl>
<dt>Side remark</dt>
<dd><p><strong>Riccati equations</strong> are named after <em>Jacopo Riccati</em> (1670–1754) who studied the differential equations of the form <span class="math display">\[\dot x = a x^2 + b t + c t^2\]</span> and its variations. In modern control, such equations arise in the calculus of variations and optimal filtering. The discrete-time version of these equations are also named after Riccati.</p>
</dd>
</dl>
<p>Since the LQR model is really simple, there are multiple ways in which the above result can be proved. We first present a dynamic programming based proof. The proof relies on the following completion of squares lemma.</p>
<div class="highlight">
<dl>
<dt>Lemma</dt>
<dd><p><strong>(Completion of squares)</strong></p>
<p>For any <span class="math inline">\(x \in \reals^n\)</span> and <span class="math inline">\(u \in \reals^m\)</span> and matrices <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(S\)</span>, and <span class="math inline">\(R\)</span> of appropriate dimensions, we have <span class="math display">\[\begin{align*}
  &amp;u^\TRANS R u + (Ax + Bu)^\TRANS S (Ax + Bu) 
  \\ &amp;\qquad = 
 (u + L x)^\TRANS [R + B^\TRANS S B] (u + L x) + x^\TRANS K x
 \end{align*}\]</span> where</p>
<ul>
<li><span class="math inline">\(K = A^\TRANS S A - \Lambda^\TRANS[R + B^\TRANS S B]^{-1} \Lambda_t\)</span></li>
<li><span class="math inline">\(L = [R + B^\TRANS S B]^{-1} \Lambda\)</span></li>
<li><span class="math inline">\(\Lambda = B^\TRANS S A\)</span></li>
</ul>
</dd>
</dl>
</div>
<h4 class="unnumbered" data-number="" id="proof">Proof</h4>
<p>The proof follows immediately by completing the square on the left hand side. In particular</p>
<p><span class="math display">\[ \begin{align*}
&amp; u^\TRANS R u +  (Ax+Bu)^\TRANS S (Ax + Bu) \\
&amp; \quad = u^\TRANS (R + B^\TRANS S B) u + 2 u^\TRANS B^\TRANS S A x 
  + x^\TRANS A^\TRANS S A x \\
&amp; \quad = u^\TRANS [R + B^\TRANS S B] u + 2 u^\TRANS B^\TRANS S A x 
  + x^\TRANS L^\TRANS [R + B^\TRANS S B] L x + x^\TRANS L x \\
&amp; \quad = 
  (u + L x)^\TRANS [R + B^\TRANS S B] (u + Kx) + x^\TRANS L x.
\end{align*} \]</span></p>
<h4 class="unnumbered" data-number="" id="proof-of-dynamic-programming-decomposition">Proof of dynamic programming decomposition</h4>
<p>The proof of the dynamic program now follows from backward induction.</p>
<p>For <span class="math inline">\(t=T\)</span>, we have <span class="math display">\[ V_T(x) = c_T(x) = x^\TRANS Q_T x \]</span> which forms the basis of induction. Assume that \eqref{eq:Vt} holds for <span class="math inline">\(t+1\)</span> and consider the value function at time <span class="math inline">\(t\)</span>. <span class="math display">\[\begin{align*}
  V_{t}(x) &amp;= \min_{u \in \reals^m} \Big\{
  x^\TRANS Q_t x + u^\TRANS R_t u + V_{t+1}(Ax + Bu) \Big\} \\
  &amp;\stackrel{(a)}= \min_{u \in \reals^m} \Big\{
  x^\TRANS Q_t x + u^\TRANS R_t u + (Ax + Bu)^\TRANS S_{t+1} (Ax + Bu) \Big\} \\
  &amp;\stackrel{(b)}= \min_{u \in \reals^m} \Big\{
  x^\TRANS Q_t x + (u + L_t x)^\TRANS [B_t^\TRANS S_{t+1} B_t + R_t] (u+ L_tx)
  + x^\TRANS L_t x \Big\}
  \\
  &amp;\stackrel{(c)}=x^\TRANS (Q_t + L_t) x
\end{align*}\]</span> where <span class="math inline">\((a)\)</span> follows from the induction hypothesis, <span class="math inline">\((b)\)</span> follows from completion of squares lemma with <span class="math inline">\(L_t\)</span> and <span class="math inline">\(L_t\)</span> defined similar to the lemma, and <span class="math inline">\((c)\)</span> follows from minimizing over <span class="math inline">\(u\)</span>, where the minima is achieved by choosing <span class="math inline">\(u = -L_t x\)</span>. This proves the induction step.</p>
<h1 data-number="2" id="LQT"><span class="header-section-number">2</span> Linear quadratic tracking</h1>
<p>Now consider the (simplified form of) LQR tracking problem where we want to ensure that the state signal <span class="math inline">\(\{x_t\}_{t \ge 1}\)</span> is close to a reference trajectory <span class="math inline">\(\{x^\circ_t\}_{t \ge 1}\)</span>. The per step cost function is <span class="math display">\[ c_t(x_t, u_t) = (x_t - x^\circ_t)^\TRANS Q_t (x_t - x^\circ_t) +
u_t^\TRANS R_t u_t\]</span> and the terminal cost is <span class="math display">\[ c_T(x_T) = (x_T - x^\circ_T)^\TRANS Q_T (x_T - x^\circ_T).\]</span></p>
<div class="highlight">
<dl>
<dt><span id="thm:lqt" class="pandoc-numbering-text thm"><strong>Theorem 2</strong></span></dt>
<dd><p>The value function at time <span class="math inline">\(t\)</span> is <span class="math display">\[ V_t(x) = x^\TRANS S_t x - 2 x^\TRANS r_t + ρ_t \]</span> and the optimal control action is <span class="math display">\[ u_t = - L_t x_t + L^\circ_t r_{t+1}\]</span> where <span class="math inline">\(\{S_t\}_{t=1}^T\)</span> and <span class="math inline">\(\{L_t\}_{t=1}^T\)</span> follow the same recursions as before. The gain matrices <span class="math inline">\(\{L^\circ_t\}\)</span> are given by <span class="math display">\[ L^\circ_t = [R_t + B_t^\TRANS S_{t+1} B_t]^{-1} B_t^\TRANS \]</span> and the correction terms <span class="math inline">\(\{r_t\}\)</span> are given by <span class="math display">\[ \begin{align*}
  r_T &amp;= Q_T x^\circ_T \\
  r_t &amp;= [A_t - B_t L_t]^\TRANS r_{t+1} + Q_t x^\circ_t
\end{align*} \]</span> and the tracking error <span class="math inline">\(\{ρ_t\}_{t=1}^T\)</span> is given by <span class="math display">\[ \begin{align*}
  ρ_T &amp;= (x^\circ_T)^\TRANS Q_T x^\circ_T, \\
  ρ_t &amp;= (x^\circ_t)^\TRANS Q_t x^\circ_t  
  - 2 r^\TRANS_{t+1} B_t [R_t + B_t^\TRANS S_{t+1} B_t]^{-1} B_t^\TRANS
    r_{t+1}
  + ρ_{t+1}.
\end{align*} \]</span></p>
</dd>
</dl>
</div>
<p>The proof follows from backward induction and basic algebra and is left as an exercise.</p>
<h1 data-number="3" id="stochastic-linear-quadratic-regulator"><span class="header-section-number">3</span> Stochastic linear quadratic regulator</h1>
<p>Now consider a system with stochastic dynamics <span class="math display">\[ x_{t+1} = A_t x_t + B_t u_t + w_t \]</span> where <span class="math inline">\(\{x_1,w_1,\dots,w_T\}\)</span> are independent random variables with zero mean and finite variance given by <span class="math inline">\(\EXP[w_t^\TRANS w_t] = \Sigma^w_t\)</span>.</p>
<div class="highlight">
<dl>
<dt><span id="thm:stochastic-lqr" class="pandoc-numbering-text thm"><strong>Theorem 3</strong></span></dt>
<dd><p>The value function at time <span class="math inline">\(t\)</span> is <span class="math display">\[\begin{equation}\label{eq:Vt-s}
  V_t(x_t) = x_t^\TRANS S_t x_t + α_t
\end{equation}\]</span> and the optimal control action is <span class="math display">\[\begin{equation}\label{eq:gt-s}
  u_t = - L_t x_t
\end{equation}\]</span> where the matrices <span class="math inline">\(\{L_t\}_{t\ge 1}\)</span> and <span class="math inline">\(\{S_t\}_{t \ge 1}\)</span> follow <em>the same recursion</em> as before and the sequence <span class="math inline">\(\{α_t\}_{t=1}^T\)</span> is computed in a backward manner as follows: <span class="math inline">\(α_T = 0\)</span> and <span class="math display">\[α_t = α_{t+1} + \TR(Σ^w_t S_{t+1}) = \sum_{τ=t+1}^{T-1} \TR(Σ^w_t S_{t+1}).\]</span></p>
</dd>
</dl>
</div>
<p>The proof is similar to the deterministic case and relies on the following observation.</p>
<dl>
<dt>Lemma</dt>
<dd><p>For any deterministic value of <span class="math inline">\(x \in \reals^n\)</span> and a random zero mean variable <span class="math inline">\(w \in \reals^n\)</span> with finite variance given by <span class="math inline">\(\EXP[w^\TRANS w] = Σ^w\)</span>, we have <span class="math display">\[ \EXP[ (x + w)^\TRANS Q (x+w) ] = x^\TRANS Q x + \TR(Q Σ^w). \]</span></p>
</dd>
</dl>
<h4 class="unnumbered" data-number="" id="proof-1">Proof</h4>
<p>Note that</p>
<p><span class="math display">\[\begin{align*}
  \EXP[ (x+w)^\TRANS Q (x+w) ] &amp;= 
  \EXP[ x^\TRANS Q x + 2 x^\TRANS Q w + w^\TRANS Q w ] \\
  &amp;= x^\TRANS Q x + \TR(Σ^w Q).
\end{align*}\]</span> where the second term is zero because <span class="math inline">\(w\)</span> is a zero mean random variable. For the third term, we are using <span class="math display">\[ \begin{align*}
  \EXP[ w^\TRANS Q w]  &amp;= 
  \EXP\bigg[ \sum_{i=1}^n \sum_{j=1}^n w_i Q_{ij} w_j \bigg]  \\
  &amp;= \sum_{i=1}^n \sum_{j=1}^n \EXP[w_i w_j] Q_{ij} \\
  &amp;= \sum_{i=1}^n \sum_{j=1}^n Σ^w_{ij} Q_{ij} \\
  &amp;= \TR(Σ^w Q)
\end{align*} \]</span></p>
<h4 class="unnumbered" data-number="" id="proof-of-stochastic-lqr">Proof of stochastic LQR</h4>
<p>Using the above lemma, we can prove the result for stochastic LQR using backward induction. The details are left as an exercise.</p>
<h2 data-number="3.1" id="a-remark-on-white-noise"><span class="header-section-number">3.1</span> A remark on “white” noise</h2>
<p>An implication of <a href="#thm:stochastic-lqr" title="Theorem 3"><span class="pandoc-numbering-link thm">Theorem 3</span></a> is that the presence of “white” stochastic disturbance in the system dynamics does not change the optimal control rule (in closed-loop form) and increases the cost only by a term independent of the state or the policy.</p>
<p>Suppose the noise was not white (but still independent of the initial state <span class="math inline">\(x_1\)</span>). Then, under assumptions on the linear/Gausian nature of the observations, the optimal control at time <span class="math inline">\(t\)</span> for the system will be the same as that for the deterministic system <span class="math display">\[ x_{τ + 1} = A x_τ + B u_τ + w_{τ|t}, \qquad τ \ge t, \]</span> where <span class="math inline">\(w_{τ|t}\)</span> is an appropriate estimate of <span class="math inline">\(w_τ\)</span> based on the information availalbe at time <span class="math inline">\(t\)</span>. That is, at time <span class="math inline">\(t\)</span> one replaces future stochastic noise <span class="math inline">\(w_τ\)</span> (<span class="math inline">\(τ \ge t\)</span>) by an ‘equivalent’ deterministic noise <span class="math inline">\(w_{τ|t}\)</span> and then applies the method of deterministic LQR to deduce the optimal feedback control in terms of the predicted noise. This is an instance of a general result known as the certainty equivalence principle, which also extends to the case when the state is not perfectly observed.</p>
<hr />
<h1 data-number="4" id="astrom"><span class="header-section-number">4</span> An alternative proof for stochastic LQR without using dynamic programming</h1>
<p>We now present a proof of the stochastic LQR that does not use dynamic programming. This is based on a simple idea of transforming the total cost using the solution of the Riccati equation.</p>
<div class="highlight">
<dl>
<dt><span id="prop:astrom" class="pandoc-numbering-text prop"><strong>Prop. 1</strong></span></dt>
<dd><p>For any control strategy, the total cost <span class="math display">\[ \EXP\bigg[\sum_{t=1}^{T-1} \big[ x_t^\TRANS Q_t x_t + u_t^\TRANS R_t
u_t \big] + x_T^\TRANS Q_T x_T \bigg]\]</span> may be written as <span class="math display">\[ \begin{align}
    &amp; \EXP\bigg[ \sum_{t=1}^{T-1} (u_t + L_t x_t)^\TRANS [R_t + B_t^\TRANS
    S_{t+1}B_t] (u_t + L_t x_t) \bigg] \nonumber \\
    &amp; \quad + 
    \EXP\bigg[ x_1^\TRANS S_1 x_t + \sum_{t=1}^{T-1} w_t S_{t+1} w_t \bigg], \label{eq:astrom}
\end{align}\]</span> where the matrices <span class="math inline">\(\{L_t\}_{t\ge 1}\)</span> and <span class="math inline">\(\{S_t\}_{t \ge 1}\)</span> follow are given as in <a href="#thm:lqr" title="Theorem 1"><span class="pandoc-numbering-link thm">Theorem 1</span></a>.</p>
</dd>
</dl>
</div>
<h4 class="unnumbered" data-number="" id="proof-2">Proof</h4>
<p>This result can be obtained by repeatedly applying the completion of squares lemma. In particular, note that <span class="math inline">\(S_T = Q_T\)</span> and using the telescopic sum, we can write <span class="math display">\[\begin{equation}\label{eq:astrom-1}
  x_T^\TRANS Q_T x_T = x_1^\TRANS S_1 x_1 + 
  \sum_{t=1}^{T-1} \big[ x_{t+1}^\TRANS S_{t+1} x_{t+1} - x_t^\TRANS S_t x_t \big].
\end{equation}\]</span></p>
<p>Furthermore, since <span class="math inline">\(w_t\)</span> is independet of <span class="math inline">\(x_t\)</span>, we have <span class="math display">\[ \begin{align}
  \EXP[ x_{t+1}^\TRANS S_{t+1} x_{t+1}] &amp;= 
   \EXP[ (A_t x_t + B_t u_t + w_t)^\TRANS S_{t+1} (A_t x_t + B_t u_t + w_t) 
   \nonumber \\
   &amp;= \EXP[ (A_t x_t + B_t u_t)^\TRANS S_{t+1} (A_t x_t + B_t u_t) ]
     + \EXP[ w_t^\TRANS S_{t+1} w_t ]. \label{eq:astrom-2}
   \end{align}\]</span></p>
<p>Using \eqref{eq:astrom-1} and \eqref{eq:astrom-2}, we get that the total cost can be written as <span class="math display">\[ \begin{align} 
      &amp; \EXP\bigg[ \sum_{t=1}^{T-1} \big[
        x_t^\TRANS Q_t x_t + u_t^\TRANS R_t u_t + x_{t+1}^\TRANS S_{t+1}
        x_{t+1} - x_t^\TRANS S_t x_t \big]
      \nonumber \\
      &amp; \quad + 
      \EXP\bigg[ x_1^\TRANS S_1 x_1 + \sum_{t=1}^{T-1} w_t S_{t+1} w_t \bigg].
      \label{eq:astrom-3}
\end{align} \]</span></p>
<p>Now, from the completion of squares lemma, we get that “Term 1” is equal to <span class="math display">\[ (u_t + L_t x_t)^\TRANS [R_t + B_t^\TRANS S_{t+1} B_t] (u_t + L_t x_t).\]</span> Substituting this back in \eqref{eq:astrom-3}, we get \eqref{eq:astrom}.  <span class="math inline">\(\Box\)</span></p>
<hr />
<p>Now we can prove <a href="#thm:stochastic-lqr" title="Theorem 3"><span class="pandoc-numbering-link thm">Theorem 3</span></a> using <a href="#prop:astrom" title="Prop. 1"><span class="pandoc-numbering-link prop">Prop. 1</span></a>. Note that the second term in \eqref{eq:astrom} does not depend on the choice of control actions <span class="math inline">\(u_t\)</span>. Thus, in order to minimimze the total expected cost, it suffices to minimize the first term of \eqref{eq:astrom}. Since <span class="math inline">\(R_t\)</span> is positive definite and <span class="math inline">\(S_{t+1}\)</span> is positive semi-definite, <span class="math inline">\(R_t + B_t^\TRANS S_{t+1} B_t\)</span> is positive definite. Thus, the first term of \eqref{eq:astrom} is sum of squares. Choosing $ u_t = -L_t x_t$ sets this term to its minimum value of <span class="math inline">\(0\)</span>. Hence, <span class="math inline">\(u_t = -L_t x_t\)</span> is the optimal control strategy.</p>
<h1 data-number="5" id="alternative-forms-of-the-riccati-equation"><span class="header-section-number">5</span> Alternative forms of the Riccati equation</h1>
<p>For a fixed <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(Q\)</span>, and <span class="math inline">\(R\)</span> matrices, define the operator <span class="math inline">\(\mathcal{R}\)</span> as <span class="math display">\[ 
\mathcal{R}S = A^\TRANS S A + Q - A^\TRANS S B[ R + B^\TRANS S B]^{-1}
B^\TRANS S A. 
\]</span></p>
<p>With this notation, we can define the recursive computation of <span class="math inline">\(\{S_t\}_{t=1}^T\)</span> as follows: <span class="math inline">\(S_T = Q_T\)</span> and <span class="math inline">\(S_t = \mathcal R_t S_{t+1}\)</span>. In this section, we derive alternative characterizations of the operator <span class="math inline">\(\mathcal{R}\)</span>.</p>
<div class="highlight">
<dl>
<dt><span id="lemma:ricatti" class="pandoc-numbering-text lemma"><strong>Lemma 1</strong></span></dt>
<dd><p>The following form are equivalent to the Riccati operator:</p>
<ol type="1">
<li><span class="math inline">\(A^\TRANS S(I + B R^{-1}B^\TRANS S)^{-1} A + Q\)</span>.</li>
<li><span class="math inline">\(A^\TRANS (S^{-1} + B R^{-1}B)^{-1} A + Q\)</span>.</li>
</ol>
</dd>
</dl>
</div>
<h4 class="unnumbered" data-number="" id="proof-3">Proof</h4>
<p>The proof of the first part relies on the simplified form of the <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Sherman-Morrison-Woodbudy</a> formula:</p>
<p><span class="math display">\[ (I + UV)^{-1} = I - U(I + VU)^{-1} V. \]</span></p>
<p>Taking <span class="math inline">\(U = BR^{-1}\)</span> and <span class="math inline">\(V = B^\TRANS S\)</span>, we get</p>
<p><span class="math display">\[ \begin{align} 
(I + BR^{-1}B^\TRANS S)^{-1} 
  &amp;= I - B R^{-1}(I + B^\TRANS S B R^{-1})^{-1} B^\TRANS S \notag \\
  &amp;= I - B(R + B^\TRANS S B)^{-1} B^\TRANS S. \label{eq:SMW}
\end{align} \]</span></p>
<p>Now multiplying both sides by <span class="math inline">\(A^\TRANS(\cdots)A\)</span> and adding <span class="math inline">\(Q\)</span>, we get the first formula. Multiplying <span class="math inline">\(S\)</span> inside the inverse gives us the second formula. <span class="math inline">\(\Box\)</span></p>
<h1 class="unnumbered" data-number="" id="references">References</h1>
<p>See <span class="citation" data-cites="Athans1971">Athans (<a href="#ref-Athans1971" role="doc-biblioref">1971</a>)</span> for a general discussion of philosophical approach of approximating general stochastic control problem as linear quadratic models.</p>
<p>The term <em>certainty equivalence</em> is due to <span class="citation" data-cites="Simon1956">Simon (<a href="#ref-Simon1956" role="doc-biblioref">1956</a>)</span>, who was looking at a static problem; a similar result had earlier been shown by <span class="citation" data-cites="Theil1954">Theil (<a href="#ref-Theil1954" role="doc-biblioref">1954</a>)</span>. A result which is essentially equivalent to the stochastic LQR problem is proved by <span class="citation" data-cites="Theil1957">Theil (<a href="#ref-Theil1957" role="doc-biblioref">1957</a>)</span>. The model for deterministic LQR is due to <span class="citation" data-cites="Kalman1960b">Kalman (<a href="#ref-Kalman1960b" role="doc-biblioref">1960</a>)</span>, who proved the result for continuous time systems.</p>
<p>The alternative proof that does not use dynamic programming is due to <span class="citation" data-cites="Astrom1970">Aström (<a href="#ref-Astrom1970" role="doc-biblioref">1970</a>)</span>.</p>
<!--
Lemma

:   Suppose $x$ and $u$ are vectors. Consider a quadratic form
    $$ Q(x,u) = \MATRIX{ x \\ u }^\TRANS
       \MATRIX{ Q_{xx} & Q_{xu} \\ Q_{ux} & Q_{uu} }
       \MATRIX{ x \\ u }.
       $$ {#eq:quadratic}

    Assume it is symmetric and $Q_{uu}$ is [positive definite]. Then the
    minimum with respect to $u$ is achieved at 
    $$\begin{equation}\label{eq:opt-schur}
      u = - Q_{uu}^{-1} Q_{ux} x,
    \end{equation}$$
    and is equal to
    $$\begin{equation}\label{eq:schur}
      x^\TRANS [ Q_{xx} - Q_{xu} Q_{uu}^{-1} Q_{ux} ] x. 
    \end{equation}$$

The term in the square brackets in \\eqref{eq:schur} is the [Schur complement] of 
$\MATRIX{ Q_{xx} & Q_{xu} \\ Q_{ux} & Q_{uu} }$

[positive definite]: ../../appendix/positive-definite-matrix
[Schur complement]: https://en.wikipedia.org/wiki/Schur_complement

#### Proof {-}

Observe that
$$ Q(x,u) = x^\TRANS Q_{xx} x + 2 x^\TRANS Q_{xu} u + u^\TRANS Q_{uu} u$$

Taking the derivative w.r.t. $u$, we get
$$ \frac{\partial Q(x,u)}{\partial u} = 2 x^\TRANS Q_{xu} + 2 u^\TRANS
Q_{uu}
\quad \text{and} \quad
\frac{\partial^2 Q(x,u)}{\partial u^2} = Q_{uu} > 0.
$$

Thus, the optimal $u$ is given by \\eqref{eq:opt-schur} and the minimum value is
\\eqref{eq:schur}. 

-->
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-Astrom1970">
<p><span class="smallcaps">Aström, K.J.</span> 1970. <em>Introduction to stochastic control theory</em>. Dover.</p>
</div>
<div id="ref-Athans1971">
<p><span class="smallcaps">Athans, M.</span> 1971. The role and use of the stochastic linear-quadratic-gaussian problem in control system design. <em>IEEE Transactions on Automatic Control</em> <em>16</em>, 6, 529–552. DOI: <a href="https://doi.org/10.1109/tac.1971.1099818">10.1109/tac.1971.1099818</a>.</p>
</div>
<div id="ref-Kalman1960b">
<p><span class="smallcaps">Kalman, R.E.</span> 1960. Contributions to the theory of optimal control. <em>Boletin de la Sociedad Matematica Mexicana</em> <em>5</em>, 102–119.</p>
</div>
<div id="ref-Simon1956">
<p><span class="smallcaps">Simon, H.A.</span> 1956. Dynamic programming under uncertainty with a quadratic criterion function. <em>Econometrica</em> <em>24</em>, 1, 74–81. DOI: <a href="https://doi.org/10.2307/1905261">10.2307/1905261</a>.</p>
</div>
<div id="ref-Theil1954">
<p><span class="smallcaps">Theil, H.</span> 1954. Econometric models and welfare maximization. <em>Wirtschaftliches Archiv</em> <em>72</em>, 60–83. DOI: <a href="https://doi.org/10.1007/978-94-011-2410-2_1">10.1007/978-94-011-2410-2_1</a>.</p>
</div>
<div id="ref-Theil1957">
<p><span class="smallcaps">Theil, H.</span> 1957. A note on certainty equivalence in dynamic planning. <em>Econometrica</em>, 346–349. DOI: <a href="https://doi.org/10.1007/978-94-011-2410-2_3">10.1007/978-94-011-2410-2_3</a>.</p>
</div>
</div>


<p class="categories">
This entry 

 was last updated on 16 Jun 2020
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/mdp">
  MDP</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/linear-systems">linear systems</a>,
<a href="https://adityam.github.io/stochastic-control/tags/riccati-equation">riccati equation</a>,
<a href="https://adityam.github.io/stochastic-control/tags/lqr">lqr</a>,
<a href="https://adityam.github.io/stochastic-control/tags/optimal-tracking">optimal tracking</a>.</p>



    </div>
  </body>
</html>


