<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2022
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Sub-Gaussian random variables</div>

<h1 data-number="1" id="prelim-concentration-inequality-of-sum-of-gaussian-random-variables"><span class="header-section-number">1</span> Prelim: Concentration inequality of sum of Gaussian random variables</h1>
<p>Let <span class="math inline">\(\phi(\cdot)\)</span> denote the density of <span class="math inline">\(\mathcal{N}(0,1)\)</span> Gaussian random variable: <span class="math display">\[ \phi(x) = \frac{1}{\sqrt{2π}} \exp\biggl( - \frac{x^2}{2} \biggr). \]</span></p>
<p>Note that if <span class="math inline">\(X \sim \mathcal{N}(μ,σ^2)\)</span>, then the density of <span class="math inline">\(X\)</span> is <span class="math display">\[
\frac{1}{σ}\phi\biggl( \frac{x-μ}{σ} \biggr) 
= \frac{1}{\sqrt{2π}\,σ} \exp\biggl( - \frac{(x-μ)^2}{2 σ^2} \biggr). \]</span></p>
<p>The tails of Gaussian random variables decay fast which can be quantified using the following inequality.</p>
<div class="highlight">
<p><strong>Mills inequality.</strong> If <span class="math inline">\(X \sim \mathcal{N}(0, 1)\)</span>, then for any <span class="math inline">\(t &gt; 0\)</span>, <span class="math display">\[ \PR( |X| &gt; t ) \le \frac{2\phi(t)}{t}  \]</span></p>
<p>More generally, if <span class="math inline">\(X \sim \mathcal{N}(0, σ^2)\)</span>, then for any <span class="math inline">\(t &gt; 0\)</span>, <span class="math display">\[ \PR( |X| &gt; t ) \le 2\frac{σ}{t} \phi\biggl(\frac{t}{σ}\biggr) =
 \sqrt{\frac{2}{π} } \frac{σ}{t} 
  \exp\biggl( - \frac{t^2}{2σ^2} \biggr). \]</span></p>
</div>
<dl>
<dt>Remark</dt>
<dd>
<p>In the communication theory literature, this bound is sometimes known as the bound on the <a href="https://en.wikipedia.org/wiki/Error_function">erfc</a> or <a href="https://en.wikipedia.org/wiki/Q-function"><span class="math inline">\(Q\)</span> function</a>.</p>
</dd>
</dl>
<h4 class="unnumbered" id="proof">Proof</h4>
<p>We’ll first prove the result for unit variance random variable. Note that <span class="math inline">\(X\)</span> is symmetric around origin. Therefore, <span class="math display">\[ \PR(|X| &gt; t) = 2\PR(X &gt; t). \]</span></p>
<p>Now, by using an idea similar to the proof of Markov’s inequality, we have <span class="math display">\[\begin{align*}
t \cdot \PR( |X| &gt; t) &amp;= t \int_{t}^∞ \phi(x) dx  \\
&amp; \le \int_{t}^∞ x \phi(x) dx \\
&amp; = \int_{t}^∞ \frac{1}{\sqrt{2π}} x \exp\biggl( - \frac{x^2}{2} \biggr) dx \\
&amp;= \frac{1}{\sqrt{2π}} \int_{t}^∞ - \frac{∂}{∂x} \exp\biggl( -\frac{x^2}{2}
\biggr) dx \\
&amp; = \frac{1}{\sqrt{2π}} \exp\biggl( - \frac{t^2}{2} \biggr)
\end{align*}\]</span></p>
<p>The proof for the general case follows by observing that <span class="math display">\[
\PR(|X| &gt; t) = \PR\biggl( \biggl| \frac{X}{σ} \biggr| &gt; \frac{t}{σ} \biggr)
\]</span> where <span class="math inline">\(X/σ \sim \mathcal{N}(0,1)\)</span>. <span class="math inline">\(\Box\)</span></p>
<p>The fact that a Gaussian random variable has tails that decay to zero exponentially fast can be be seen in the moment generating function: <span class="math display">\[
  M(s) = \EXP[ \exp(sX) ] = \exp\bigl( sμ + \tfrac12 s^2 σ^2\bigr).
\]</span></p>
<p>A useful application of Mills inequality is the following concentration inequality.</p>
<div class="highlight">
<p><strong>Concentration inequality.</strong> Let <span class="math inline">\(X_i \sim \mathcal{N}(0, σ^2)\)</span> (not necessarily independent). Then, for any <span class="math inline">\(t &gt; 0\)</span>, <span class="math display">\[
  \PR\Bigl( \max_{1 \le i \le n} |X_i| &gt; t\Bigr) \le 
  2n \frac{σ}{t} \phi\biggl( \frac{t}{σ} \biggr). 
\]</span></p>
</div>
<h4 class="unnumbered" id="proof-1">Proof</h4>
<p>This follows immediately from Mills inequality and the union bound. <span class="math inline">\(\Box\)</span></p>
<p>Another useful result is the following:</p>
<div class="highlight">
<p><strong>Max of Gaussian random variables.</strong></p>
<p>Let <span class="math inline">\(X_i \sim \mathcal{N}(0,σ^2)\)</span> (not necessarily independent). Then, <span class="math display">\[
  \EXP\Bigl[ \max_{1 \le i \le n} X_i \Bigr] \le σ \sqrt{2 \log n}
\]</span> and <span class="math display">\[
  \EXP\Bigl[ \max_{1 \le i \le n} |X_i| \Bigr] \le σ \sqrt{2 \log 2n}.
\]</span></p>
</div>
<p>See <a href="http://www.gautamkamath.com/writings/gaussian_max.pdf">these notes</a> for a lower bound with the same rate!</p>
<h4 class="unnumbered" id="proof-2">Proof</h4>
<p>We prove the first inequality. The second follows by considering <span class="math inline">\(2n\)</span> random variables <span class="math inline">\(X_1, \dots, X_n\)</span>, <span class="math inline">\(-X_1, \dots, -X_n\)</span>.</p>
<p>For any <span class="math inline">\(s &gt; 0\)</span>, <span class="math display">\[\begin{align*}
\EXP\Bigl[ \max_{1 \le i \le n} X_i \Bigr] &amp;=
\frac{1}{s} 
\EXP\Bigl[ \log \Bigl( \exp\Bigl( s \max_{1 \le i \le n} X_i \Bigr) \Bigr) \Bigr]
\\
&amp;\stackrel{(a)}\le
\frac{1}{s} 
\log \Bigl( \EXP\Bigl[ \exp\Bigl( s \max_{1 \le i \le n} X_i \Bigr) \Bigr] \Bigr)
\\
&amp;\stackrel{(b)}=
\frac{1}{s} 
\log \Bigl( \EXP\Bigl[ \max_{1 \le i \le n} \exp( s X_i ) \Bigr] \Bigr)
\\
&amp;\stackrel{(c)}\le
\frac{1}{s} 
\log \Bigl(\sum_{i=1}^n \EXP\bigl[ \exp( s X_i ) \bigr] \Bigr)
\\
&amp;\stackrel{(d)}=
\log \Bigl( \sum_{i=1}^n\exp\Bigl( \frac{s^2 σ^2}{2} \Bigr) \Bigr) 
\\
&amp;= \frac{\log n}{s} + \frac{s^2 σ^2}{2}
\end{align*}\]</span> where <span class="math inline">\((a)\)</span> follows from Jensen’s inequality, <span class="math inline">\((b)\)</span> follows from monotonicity of <span class="math inline">\(\exp(\cdot)\)</span>, <span class="math inline">\((c)\)</span> follows from definition of max, <span class="math inline">\((d)\)</span> follows from the definition of moment generating function of Gaussian random variables. We get the result by setting <span class="math inline">\(s = \sqrt{2 \log n}/σ\)</span> (which minimizes the upper bound). <span class="math inline">\(\Box\)</span></p>
<dl>
<dt>Remark</dt>
<dd>
<p>We have stated and proved these inequalities for real-valued random variables. But a version of them continue to hold for vector valued Gaussian variables as well. For a complete treatment, see <span class="citation" data-cites="Picard2007">Picard (<a href="#ref-Picard2007" role="doc-biblioref">2007</a>)</span>.</p>
</dd>
</dl>
<h1 data-number="2" id="sub-gaussian-random-variables"><span class="header-section-number">2</span> Sub-Gaussian random variables</h1>
<p>It turns out that the concentration inequalities of the form above continue to hold for more general distributions than the Gaussian. In particular, consider the bound on the max of Gaussian random variables that we established above. The only step which depends on the assumption that the random variables <span class="math inline">\(X_i\)</span> were Gaussian in step <span class="math inline">\((d)\)</span>. Thus, as long as <span class="math inline">\(\EXP[ \exp(s X_i) ] \le \exp(\tfrac12 s^2 σ^2)\)</span>, the result will continue to hold! This motivates the definition of sub-Gaussian random variables.</p>
<div class="highlight">
<p><strong>Sub-Gaussian random variable.</strong> A random variable <span class="math inline">\(X \in \reals\)</span> is said to be <em>sub-Gaussian</em> with variance proxy <span class="math inline">\(σ^2\)</span> if <span class="math inline">\(\EXP[X] = 0\)</span> and its moment generating function satisfies <span class="math display">\[ \EXP[ \exp(sX) ] \le \exp( \tfrac12 s^2 σ^2),
\quad \forall s \in \reals. \]</span></p>
</div>
<p>The reason the parameter <span class="math inline">\(σ^2\)</span> is called a variance proxy is because by a straight forward application of Taylor series expansion and comparing coefficients, it can be shown that <span class="math inline">\(\text{var}(X) \le σ^2\)</span>. See <span class="citation" data-cites="Rivasplata2012">Rivasplata (<a href="#ref-Rivasplata2012" role="doc-biblioref">2012</a>)</span> for a proof.</p>
<p>This definition can be generalized to random vectors and matrices. A random vector <span class="math inline">\(X \in \reals^d\)</span> is said the be <em>sub-Gaussian</em> with variance proxy <span class="math inline">\(σ^2\)</span> if <span class="math inline">\(\EXP[X] = 0\)</span> and for any unit vector <span class="math inline">\(u \in \reals^d\)</span>, <span class="math inline">\(u^\TRANS X\)</span> is sub-Gaussian with variance proxy <span class="math inline">\(σ^2\)</span>.</p>
<p>Similarly, a random matrix <span class="math inline">\(X \in \reals^{d_1 × d_2}\)</span> is said to be <em>sub-Gaussian</em> with variance proxy <span class="math inline">\(σ^2\)</span> if <span class="math inline">\(\EXP[X] = 0\)</span> and for any unit vectors <span class="math inline">\(u \in \reals^{d_1}\)</span> and <span class="math inline">\(v \in \reals^{d_2}\)</span>, <span class="math inline">\(u^\TRANS X v\)</span> is sub-Gaussian with variance proxy <span class="math inline">\(σ^2\)</span>.</p>
<p>We will use the phrase “<span class="math inline">\(σ\)</span>-sub-Gaussian” as a short form of “sub-Gaussian with variance proxy <span class="math inline">\(σ^2\)</span>”. One typically writes <span class="math inline">\(X \sim \text{subG}(σ^2)\)</span> to denote a random variable with sub-Gaussian distribution with variance proxy <span class="math inline">\(σ^2\)</span>. (Strictly speaking, this notation is a bit ambiguous since <span class="math inline">\(\text{subG}(σ^2)\)</span> is a class of distributions rather than a single distribution.)</p>
<h2 data-number="2.1" id="examples-of-sub-gaussian-distributions"><span class="header-section-number">2.1</span> Examples of sub-Gaussian distributions</h2>
<ol type="1">
<li><p>If <span class="math inline">\(X\)</span> be a Rademacher random variable, i.e., <span class="math inline">\(X\)</span> takes the values <span class="math inline">\(\pm 1\)</span> with probability <span class="math inline">\(1/2\)</span>. Then, <span class="math display">\[ \EXP[ \exp(sX) ] = \frac12 e^{-s} + \frac12 e^s = \cosh s \le 
\exp(\tfrac12 s^2), \]</span> so <span class="math inline">\(X\)</span> is</p></li>
<li><p>If <span class="math inline">\(X\)</span> is uniformly distributed over <span class="math inline">\([-a, a]\)</span>. Then, for any <span class="math inline">\(s \neq 0\)</span>, <span class="math display">\[ \EXP[ \exp(s X) ] = \frac{1}{2as}[ e^{as} - e^{-as} ]
   = \sum_{n=0}^∞ \frac{(as)^{2n}}{(2n+1)!}. \]</span> Using the inequality <span class="math inline">\((2n+1)! \ge n!2^n\)</span>, we get that <span class="math inline">\(X\)</span> is <span class="math inline">\(a\)</span>-sub-Gaussian.</p></li>
<li><p>It can be shown that (see <span class="citation" data-cites="Rivasplata2012">Rivasplata (<a href="#ref-Rivasplata2012" role="doc-biblioref">2012</a>)</span> ) if <span class="math inline">\(X\)</span> is a random variable with <span class="math inline">\(\EXP[X] = 0\)</span> and <span class="math inline">\(|X| &lt; 1\)</span> a.s., then <span class="math display">\[ \EXP[ \exp(sX) ] \le \cosh s, \quad \forall s \in \reals. \]</span> Therefore, <span class="math inline">\(X\)</span> is 1-sub-Gaussian.</p></li>
<li><p>An immediate corollary of the previous example is that if <span class="math inline">\(X\)</span> is a random variable with <span class="math inline">\(\EXP[X] = 0\)</span> and <span class="math inline">\(|X| \le b\)</span> a.s., then <span class="math inline">\(X\)</span> is <span class="math inline">\(b\)</span>-sub-Gaussian.</p></li>
<li><p>By a similar arguement, we can show that if <span class="math inline">\(X\)</span> is a zero mean random variable supported on some interval <span class="math inline">\([a,b]\)</span>, then <span class="math inline">\(X\)</span> is <span class="math inline">\((b-a)/2\)</span> sub-Gaussian.</p></li>
<li><p>If <span class="math inline">\(X\)</span> is <span class="math inline">\(σ^2\)</span> sub-Gaussian, then for any <span class="math inline">\(α \in \reals\)</span>, <span class="math inline">\(α X\)</span> is <span class="math inline">\(|α|σ\)</span>-sub-Gaussian.</p></li>
<li><p>If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are <span class="math inline">\(σ_1\)</span> and <span class="math inline">\(σ_2\)</span>-sub-Gaussian, then <span class="math inline">\(X_1 + X_2\)</span> is <span class="math inline">\(\sqrt{σ_1^2 + σ_2^2}\)</span>-sub-Gaussian.</p></li>
</ol>
<h2 data-number="2.2" id="characterization-of-sub-gaussian-random-variables"><span class="header-section-number">2.2</span> Characterization of sub-Gaussian random variables</h2>
<p>Sub-Gaussian random variables satisfy a concentration result similar to Mills inequality.</p>
<div class="highlight">
<dl>
<dt><span id="lemma:1"></span><span id="lemma:tail" class="pandoc-numbering-text lemma"><strong>Lemma 1</strong></span></dt>
<dd>
<p>Let <span class="math inline">\(X \in \reals\)</span> be <span class="math inline">\(σ\)</span>-sub-Gaussian. Then, for any <span class="math inline">\(t &gt; 0\)</span>, <span class="math display">\[\begin{equation}\label{eq:sG-tail-bounds}
  \PR(X &gt; t) \le \exp\biggl( - \frac{t^2}{2σ^2} \biggr)
  \quad\text{and}\quad
  \PR(X &lt; t) \le \exp\biggl( - \frac{t^2}{2σ^2} \biggr)
  \end{equation}\]</span></p>
</dd>
</dl>
</div>
<h4 class="unnumbered" id="proof-3">Proof</h4>
<p>This follows from Chernoff’s bound and the definition of sub-Gaussianity. In particular, for any <span class="math inline">\(s &gt; 0\)</span> <span class="math display">\[
\PR(X &gt; t) = \PR(\exp(sX) &gt; \exp(st)) \le \frac{ \EXP[\exp(sX) ]} { \exp(st) }
\le \exp\biggl( \frac{s^2 σ^2}{2} - st \biggr).
\]</span> Now, to find the tightest possible bound, we minimize the above bound with respect to <span class="math inline">\(s\)</span>, which is attained at <span class="math inline">\(s = t/σ^2\)</span>. Substituting this in the above bound, we get the first inequality. The second inequality follows from a similar argument. <span class="math inline">\(\Box\)</span></p>
<p>Recall that the moments of <span class="math inline">\(Z \sim \mathcal{N}(0,σ^2)\)</span> are given by <span class="math display">\[
  \EXP[ |Z|^k ] = \frac{1}{\sqrt{π}} (2σ^2)^{k/2} Γ\biggl(\frac{k+1}{2}\biggr),
\]</span> where <span class="math inline">\(Γ(\cdot)\)</span> denotes the <a href="https://en.wikipedia.org/wiki/Gamma_function">Gamma function</a>. The next result shows that the tail bounds \eqref{eq:sG-tail-bounds} are sufficient to show that the absolute moments of <span class="math inline">\(X \sim \text{subG}(σ^2)\)</span> can be bounded by those of <span class="math inline">\(Z \sim \mathcal{N}(0,σ^2)\)</span> up to multiplicative constants.</p>
<div class="highlight">
<dl>
<dt><span id="lemma:2"></span><span id="lemma:moment" class="pandoc-numbering-text lemma"><strong>Lemma 2</strong></span></dt>
<dd>
<p>Let <span class="math inline">\(X\)</span> be a random variable such that <span class="math display">\[ \PR( |X| &gt; t) \le 2 \exp\biggl(- \frac{t^2}{2σ^2} \biggr),\]</span> then for any positive integer <span class="math inline">\(k \ge 1\)</span>, <span class="math display">\[ \EXP[ |X|^k ] \le (2σ^2)^{k/2} k Γ(k/2). \]</span></p>
</dd>
</dl>
</div>
<p>Note that for the special case of <span class="math inline">\(k=1\)</span>, the above bound implies <span class="math inline">\(\EXP[ |X| ] \le σ \sqrt{2π}\)</span> and for <span class="math inline">\(k=2\)</span>, <span class="math inline">\(\EXP[|X|^2] \le 4σ^2\)</span>.</p>
<h4 class="unnumbered" id="proof-4">Proof</h4>
<p>This is a simple application of the tail bound. <span class="math display">\[\begin{align*}
\EXP[ |X|^k ] &amp;= \int_{0}^∞ \PR( |X|^k &gt; t ) dt \\
&amp;= \int_{0}^∞ \PR( |X| &gt; t^{1/k}) dt \\
&amp;\le 2 \int_{0}^∞ \exp\biggl( - \frac{t^{2/k}}{2σ^2} \biggr) dt \\
&amp;= (2σ^2)^{k/2} k \int_{0}^∞ e^{-u} u^{k/2 - 1} du, 
\qquad u = \frac{t^{2/k}}{2σ^2} \\
&amp;= (2σ^2)^{k/2}k Γ(k/2).
\end{align*}\]</span></p>
<p>The result for <span class="math inline">\(k=1\)</span> follows from <span class="math inline">\(Γ(1/2) = \sqrt{π/2}\)</span>. <span class="math inline">\(\Box\)</span></p>
<p>Using moments, we can bound the moment generating function in terms of the tail bounds.</p>
<div class="highlight">
<dl>
<dt><span id="lemma:3"></span><span id="lemma:mgf" class="pandoc-numbering-text lemma"><strong>Lemma 3</strong></span></dt>
<dd>
<p>Let <span class="math inline">\(X\)</span> be a random variable such that <span class="math display">\[ \PR( |X| &gt; t) \le 2 \exp\biggl(- \frac{t^2}{2σ^2} \biggr)\]</span> then, <span class="math display">\[\EXP[ \exp(sX) ] \le \exp(4 s^2 σ^2). \]</span></p>
</dd>
</dl>
</div>
<p>For this reason, sometimes it is stated that <span class="math inline">\(X \sim \text{subG}(s^2)\)</span> when it satisfies the tail bound \eqref{eq:sG-tail-bounds}.</p>
<p>The proof follows from the following Taylor series bound on the exponential function. <span class="math display">\[ 
\exp(sX) \le 1 + \sum_{k=2}^∞ \frac{s |X|^k}{k!}
\]</span> and apply the result of <a href="#lemma:moment" title="Lemma 2"><span class="pandoc-numbering-link lemma">Lemma 2</span></a>. See <span class="citation" data-cites="MIT18.S997">Rigollet (<a href="#ref-MIT18.S997" role="doc-biblioref">2015</a>)</span> for details.</p>
<h2 data-number="2.3" id="properties-of-sub-gaussian-random-vectors"><span class="header-section-number">2.3</span> Properties of sub-Gaussian random vectors</h2>
<div class="highlight">
<dl>
<dt><span id="theorem:1"></span><span id="theorem:subG-vector" class="pandoc-numbering-text theorem"><strong>Theorem 1</strong></span></dt>
<dd>
<p>Let <span class="math inline">\(X = (X_1, \dots, X_n)\)</span> be a vector of independent <span class="math inline">\(σ\)</span>-sub-Gaussian random variables. Then, the random vector <span class="math inline">\(X\)</span> is <span class="math inline">\(σ\)</span>-sub-Gaussian.</p>
</dd>
</dl>
</div>
<h4 class="unnumbered" id="proof-5">Proof</h4>
<p>For any unit vector <span class="math inline">\(u \in \reals^n\)</span>, and any <span class="math inline">\(s \in \reals\)</span> <span class="math display">\[\begin{align*}
\EXP[ \exp( s u^\TRANS X) ] &amp;= \prod_{i=1}^n \EXP[ \exp(s u_i X_i) ] \\
&amp;\le \prod_{i=1}^n \exp\bigl( \tfrac{1}{2} s^2 u_i^2 σ^2 \bigr) \\
&amp;= \exp\bigl( \tfrac{1}{2} s^2 \| u \|^2 σ^2 \bigr) \\
&amp;= \exp\bigl( \tfrac{1}{2} s^2 σ^2 \bigr). \tag*{$\Box$}
\end{align*}\]</span></p>
<h2 data-number="2.4" id="concentration-inequalities"><span class="header-section-number">2.4</span> Concentration inequalities</h2>
<p>Recall that if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> and <span class="math inline">\(σ_1\)</span> and <span class="math inline">\(σ_2\)</span>-sub-Gaussian, then <span class="math inline">\(X_1 + X_2\)</span> is sub-Gaussian with variance proxy <span class="math inline">\(σ_1^2 + σ_2^2\)</span>. An immediate implication of this property is the following:</p>
<div class="highlight">
<p><strong>Hoeffding inequality.</strong> Suppose that variables <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i \in \{1,\dots,n\}\)</span>, are independent and <span class="math inline">\(X_i\)</span> has mean <span class="math inline">\(μ_i\)</span> and <span class="math inline">\(σ_i\)</span>-sub-Gaussian. Then, for all <span class="math inline">\(t &gt; 0\)</span>, we have</p>
<p><span class="math display">\[ \PR\biggl( \sum_{i=1}^n( X_i - μ_i) \ge t \biggr)
   \le \exp\biggl( - \frac{t^2}{2 \sum_{i=1}^n σ_i^2 } \biggr).
\]</span></p>
</div>
<p>The Hoeffding inequality is often stated for the special case of bounded random variables. In particular, if <span class="math inline">\(X_i \in [a,b]\)</span>, then we know that <span class="math inline">\(X_i\)</span> is sub-Gaussian with parameter <span class="math inline">\(σ = (b-a)/2\)</span>, so we obtain the bound <span class="math display">\[ \PR\biggl( \sum_{i=1}^n( X_i - μ_i) \ge t \biggr)
   \le \exp\biggl( - \frac{2t^2}{\sum_{i=1}^n n(b-a)^2 } \biggr).
\]</span></p>
<p>The Hoeffding inequality can be generalized to Martingales. Recall that a sequence <span class="math inline">\(\{ (D_i, \mathcal F_i)\}_{i \ge 1}\)</span> is called a <em>martingale difference sequence</em> is for all <span class="math inline">\(i \ge 1\)</span>, <span class="math inline">\(D_i\)</span> is <span class="math inline">\(\mathcal{F}_i\)</span> measurable, <span class="math display">\[ \EXP[ |D_i| ] &lt; ∞ \quad\text{and}\quad
   \EXP[ D_{i+1} \mid \mathcal{F}_i ] = 0. \]</span></p>
<div class="highlight">
<p><strong>Asuma-Hoeffding Inequality.</strong> Let <span class="math inline">\(\{ (D_i, \mathcal{F}_i)\}_{i \ge 1}\)</span> be a martingale difference sequence and suppose that <span class="math inline">\(|D_i| \le b_i\)</span> almost surely for all <span class="math inline">\(i \ge 1\)</span>. Then for all <span class="math inline">\(t \ge 0\)</span> <span class="math display">\[ \PR\biggl( \biggl| \sum_{i=1}^n D_i \biggr| \ge t \biggr)
     \le 2 \exp\biggl( - \frac{t^2}{2 \sum_{i=1}^n b_i^2 } \biggr). 
  \]</span></p>
</div>
<h4 class="unnumbered" id="proof-6">Proof</h4>
<p>Since <span class="math inline">\(|D_i| \le b_i\)</span>, <span class="math inline">\(D_i\)</span> is <span class="math inline">\(b_i\)</span>-subGaussian. Using the smoothing property of conditional expectation, we have <span class="math display">\[\begin{align}
\EXP\biggl[ \exp\biggl( s \biggl( \sum_{i=1}^n D_i \biggr) \biggr) \biggl]
&amp;= 
\EXP\biggl[ \exp\biggl( s \biggl( \sum_{i=1}^{n-1} D_i \biggr) \biggr) \biggl]
\,
\EXP\bigl[ \exp\bigl( s D_n \bigr) \bigm| \mathcal{F}_{n-1} \bigl]
\notag \\
&amp;\le 
\EXP\biggl[ \exp\biggl( s \biggl( \sum_{i=1}^{n-1} D_i \biggr) \biggr) \biggl]
\, \exp\bigl( \tfrac12 s^2 b_n^2 \bigr),
\end{align}\]</span> where the inequality followed from <span class="math inline">\(D_n\)</span> being <span class="math inline">\(b_n\)</span>-subGaussian. Iterating backwards this way, we get <span class="math display">\[ \PR\biggl(  \sum_{i=1}^n D_i  \ge t \biggr)
   \le  \exp\biggl( - \frac{t^2}{2 \sum_{i=1}^n b_i^2 } \biggr). 
\]</span> By a symmetric argument, we can show that <span class="math display">\[ \PR\biggl(  \sum_{i=1}^n D_i  \le -t \biggr)
   \le  \exp\biggl( - \frac{t^2}{2 \sum_{i=1}^n b_i^2 } \biggr). 
\]</span> Conbining these two, we get the stated result. <span class="math inline">\(\Box\)</span></p>
<p>Note that we can easily generalize the above inequality to the case when <span class="math inline">\(D_k \in [a_i, b_i]\)</span> because in that case <span class="math inline">\(D_k\)</span> will be <span class="math inline">\((b_i - a_i)/2\)</span> sub-Gaussian.</p>
<h2 data-number="2.5" id="maximal-inequalities"><span class="header-section-number">2.5</span> Maximal inequalities</h2>
<p>As we explained in the motivation for the definition of sub-Gaussian random variables, the definition implies that sub-Gaussian random variables will satisfy the concentration and maximal inequalities for Gaussian random variables. In particular, we have the following general result.</p>
<div class="highlight">
<dl>
<dt><span id="theorem:2"></span><span id="theorem:maximal" class="pandoc-numbering-text theorem"><strong>Theorem 2</strong></span></dt>
<dd>
<p>Let <span class="math inline">\(X_i \in \reals\)</span> be <span class="math inline">\(σ\)</span>-sub-Gaussian random variables (not necessarily independent). Then, <span class="math display">\[
\EXP\Bigl[ \max_{1 \le i \le n} X_i \Bigr] \le σ \sqrt{2 \log n}
  \quad\text{and}\quad
\EXP\Bigl[ \max_{1 \le i \le n} |X_i| \Bigr] \le σ \sqrt{2 \log 2n}.
  \]</span> Moreover, for any <span class="math inline">\(t &gt; 0\)</span>, <span class="math display">\[
\PR\Bigl( \max_{1 \le i \le n} X_i &gt; t\Bigr) \le 
n \exp\biggl( -\frac{t^2}{2σ^2} \biggr)
  \quad\text{and}\quad
\PR\Bigl( \max_{1 \le i \le n} |X_i| &gt; t\Bigr) \le 
2n \exp\biggl( -\frac{t^2}{2σ^2} \biggr).
  \]</span></p>
</dd>
</dl>
</div>
<p>The proof is exactly the same as the Gaussian case!</p>
<p>Now we state two generalizations without proof. See <span class="citation" data-cites="MIT18.S997">Rigollet (<a href="#ref-MIT18.S997" role="doc-biblioref">2015</a>)</span> for proof.</p>
<h3 class="unnumbered" id="maximum-over-a-convex-polytope">Maximum over a convex polytope</h3>
<div class="highlight">
<dl>
<dt><span id="theorem:3"></span><span id="theorem:polytope" class="pandoc-numbering-text theorem"><strong>Theorem 3</strong></span></dt>
<dd>
<p>Let <span class="math inline">\(\mathsf{P}\)</span> be a polytope with <span class="math inline">\(n\)</span> vertices <span class="math inline">\(v^{(1)}, \dots, v^{(n)} \in  \reals^d\)</span> and let <span class="math inline">\(X \in \reals^d\)</span> be a random variable such that <span class="math inline">\([  v^{(i)} ]^\TRANS X\)</span>, <span class="math inline">\(i \in \{1, \dots, n\}\)</span> are <span class="math inline">\(σ\)</span>-sub-Gaussian random variables. Then, <span class="math display">\[
\EXP\Bigl[ \max_{θ \in \mathsf{P}} θ^\TRANS X \Bigr] \le σ \sqrt{2 \log n}
  \quad\text{and}\quad
\EXP\Bigl[ \max_{θ \in \mathsf{P}} | θ^\TRANS X | \Bigr] \le σ \sqrt{2 \log 2n}.
  \]</span> Moreover, for any <span class="math inline">\(t &gt; 0\)</span>, <span class="math display">\[
\PR\Bigl(  \max_{θ \in \mathsf{P}} θ^\TRANS X &gt; t\Bigr) \le 
n \exp\biggl( -\frac{t^2}{2σ^2} \biggr)
  \quad\text{and}\quad
\PR\Bigl(  \max_{θ \in \mathsf{P}} |θ^\TRANS X| &gt; t\Bigr) \le 
2n \exp\biggl( -\frac{t^2}{2σ^2} \biggr).
  \]</span></p>
</dd>
</dl>
</div>
<h3 class="unnumbered" id="maximum-over-the-ell_2-ball">Maximum over the <span class="math inline">\(\ell_2\)</span> ball</h3>
<div class="highlight">
<dl>
<dt><span id="theorem:4"></span><span id="theorem:ball" class="pandoc-numbering-text theorem"><strong>Theorem 4</strong></span></dt>
<dd>
<p>Let <span class="math inline">\(X \in \reals^d\)</span> be a <span class="math inline">\(σ\)</span>-sub-Gaussian random variable. Then, <span class="math display">\[ \EXP[ \max_{ \| θ \| \le 1 } θ^\TRANS X ] = 
 \EXP[ \max_{ \| θ \| \le 1 } | θ^\TRANS X | ] \le 4σ \sqrt{d}.
  \]</span> Moreover, for any <span class="math inline">\(t &gt; 0\)</span> <span class="math display">\[ \PR( \max_{ \| θ \| \le 1 } θ^\TRANS X &gt; t) = 
 \PR( \max_{ \| θ \| \le 1 } | θ^\TRANS X | &gt; t ) \le 
6^d \exp\biggl(- \frac{t^2}{8σ^2} \biggr).
  \]</span></p>
</dd>
</dl>
</div>
<dl>
<dt>Remark</dt>
<dd>
<p>For any <span class="math inline">\(δ &gt; 0\)</span>, take <span class="math inline">\(t = σ\sqrt{8d \log 6} + 2σ\sqrt{2 \log(1/δ)}\)</span>, we obtain that with probability less than <span class="math inline">\(1-δ\)</span>, it holds that <span class="math display">\[
\max_{\|θ\| \le 1} θ^\TRANS X 
=
\max_{\|θ\| \le 1} | θ^\TRANS X |
\le 4σ\sqrt{d} + 2σ \sqrt{2\log(1/δ)}.
   \]</span></p>
</dd>
</dl>
<h2 data-number="2.6" id="lipschitz-functions-of-gaussian-variables."><span class="header-section-number">2.6</span> Lipschitz functions of Gaussian variables.</h2>
<p>Recall that a function <span class="math inline">\(f \colon \reals^d \to \reals\)</span> is <span class="math inline">\(L\)</span>-Lipschitz with respect to the Eucledian norm if <span class="math display">\[
  | f(x) - f(y) | \le L \| x - y \|_2,
  \quad \forall x, y \in \reals^d.
\]</span></p>
<p>The following results shows that any Lipschitz function of a Gaussian random variable is <span class="math inline">\(L\)</span>-sub-Gaussian.</p>
<div class="highlight">
<dl>
<dt><span id="theorem:5"></span><span id="theorem:lipschitz" class="pandoc-numbering-text theorem"><strong>Theorem 5</strong></span></dt>
<dd>
<p>Let <span class="math inline">\(X = (X_1, \dots, X_n)\)</span> be a vector of i.i.d. standard Gaussian random variables and let <span class="math inline">\(f \colon \reals^n \to \reals\)</span> be <span class="math inline">\(L\)</span>-Lipschitz with respect to the Euclidean norm. Then, the variable <span class="math inline">\(f(X) - \EXP[ f(X) ]\)</span> is <span class="math inline">\(L\)</span>-sub-Gaussian and therefore <span class="math display">\[
\PR\bigl[ \bigl| f(X) - \EXP[f(X)] \bigr| \ge t \bigr] 
\le 2 \exp\biggl(- \frac{t^2}{2L^2} \biggr).
  \]</span></p>
</dd>
</dl>
</div>
<p>This result is remarkable because it guarantees that any <span class="math inline">\(L\)</span>-Lipschitz function of a standard Gaussian random vector, irrespective of the dimension, exhibits concetration like a scalar Gaussian variable with variance <span class="math inline">\(L^2\)</span>.</p>
<p>For a proof, see Chapter 2 of <span class="citation" data-cites="Wainwright2019">Wainwright (<a href="#ref-Wainwright2019" role="doc-biblioref">2019</a>)</span>.</p>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Picard2007" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Picard, J.</span> 2007. <em>Concentration inequalities and model selection</em>. Springer Berlin Heidelberg. DOI: <a href="https://doi.org/10.1007/978-3-540-48503-2">10.1007/978-3-540-48503-2</a>.
</div>
<div id="ref-MIT18.S997" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Rigollet, P.</span> 2015. High-dimensional statistics. Available at: <a href="https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/">https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/</a>.
</div>
<div id="ref-Rivasplata2012" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Rivasplata, O.</span> 2012. Subgaussian random variables: An expository note. Available at: <a href="http://stat.cmu.edu/~arinaldo/36788/subgaussians.pdf">http://stat.cmu.edu/~arinaldo/36788/subgaussians.pdf</a>.
</div>
<div id="ref-Wainwright2019" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Wainwright, M.J.</span> 2019. <em>High-dimensional statistics</em>. Cambridge University Press. DOI: <a href="https://doi.org/10.1017/9781108627771">10.1017/9781108627771</a>.
</div>
</div>


<p class="categories">
This entry 

 was last updated on 24 Aug 2020</p>



    </div>
  </body>
</html>


