<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Appendix on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/appendix/</link>
    <description>Recent content in Appendix on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://adityam.github.io/stochastic-control/appendix/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reproducing Kernel Hilbert Space</title>
      <link>https://adityam.github.io/stochastic-control/appendix/rkhs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/rkhs/</guid>
      <description>1 Linear Operators  Definition 1  (Linear operator) Let \(\mathcal F\) and \(\mathcal G\) be normed vector spaces over \(\reals\). A function \(A \colon \mathcal F \to \mathcal G\) is called a linear operator if it satisfies the following properties:
 Honogeneity: For any \(α \in \reals\) and \(f \in \mathcal F\), \(A(αf) = α (Af)\). Additivity: For any \(f,g \in \mathcal F\), \(A(f + g) = Af + Ag\).</description>
    </item>
    
    <item>
      <title>Vectorization</title>
      <link>https://adityam.github.io/stochastic-control/appendix/vectorization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/vectorization/</guid>
      <description>Vectorization is a linear transformation that converts a matrix to a column vector. For example, \[\VEC\left(\MATRIX{a &amp;amp; b \\ c &amp;amp; d }\right) = \MATRIX{a \\ c \\ b \\ d}.\]
Vectorization is often used to express matrix multiplication as a linear transformation on matrices. In particular, we have the following three properties:
\(\VEC(ABC) = (C^\TRANS \otimes A) \VEC(B).\) \(\VEC(ABC) = (I \otimes AB)\VEC(C).\) \(\VEC(ABC) = (C^\TRANS B^\TRANS \otimes I)\VEC(A).\)  Another useful formulation is the following</description>
    </item>
    
    <item>
      <title>Positive definite matrices</title>
      <link>https://adityam.github.io/stochastic-control/appendix/positive-definite-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/positive-definite-matrix/</guid>
      <description>[WARNING] Citeproc: citation Abbasi-Yadkori2011 not found 1 Definite and basic properties  Definition  A \(n \times n\) symmetric matrix \(M\) is called
 positive definite (written as \(M &amp;gt; 0\)) if for all \(x \in \reals^n\), \(x \neq 0\), we have \[x^\TRANS M x &amp;gt; 0.\]
 positive semi definite (written as \(M \ge 0\)) if for all \(x \in \reals^n\), \(x \neq 0\), we have \[x^\TRANS M x \ge 0.</description>
    </item>
    
    <item>
      <title>Singular value decomposition</title>
      <link>https://adityam.github.io/stochastic-control/appendix/singular-value-decomposition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/singular-value-decomposition/</guid>
      <description>Recall that if \(A\) is a symmetric \(n × n\) matrix, then \(A\) has real eigenvalues \(λ_1, \dots, λ_n\) (possibly repeated) and \(\reals^n\) has an orthonormal basis \(v_1, \dots, v_n\), where each \(v_i\) is an eigenvector of \(A\) with eigenvalue \(λ_i\). Then, \[ A = V D V^{-1} \] where \(V\) is the matrix whose columns are \(v_1, \dots, v_n\) and \(D\) is a diagonal matrix whose diagonals are \(λ_1, \dots, λ_n\).</description>
    </item>
    
    <item>
      <title>Sub-Gaussian random variables</title>
      <link>https://adityam.github.io/stochastic-control/appendix/sub-gaussian/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/sub-gaussian/</guid>
      <description>1 Prelim: Concentration inequality of sum of Gaussian random variables Let \(\phi(\cdot)\) denote the density of \(\mathcal{N}(0,1)\) Gaussian random variable: \[ \phi(x) = \frac{1}{\sqrt{2π}} \exp\biggl( - \frac{x^2}{2} \biggr). \]
Note that if \(X \sim \mathcal{N}(μ,σ^2)\), then the density of \(X\) is \[ \frac{1}{σ}\phi\biggl( \frac{x-μ}{σ} \biggr) = \frac{1}{\sqrt{2π}\,σ} \exp\biggl( - \frac{(x-μ)^2}{2 σ^2} \biggr). \]
The tails of Gaussian random variables decay fast which can be quantified using the following inequality.
Mills inequality.</description>
    </item>
    
  </channel>
</rss>
