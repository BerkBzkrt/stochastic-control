<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="POMDP,belief state,hypothesis testing" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://cdn.plot.ly/plotly-basic-latest.min.js">
  </script><script type="text/javascript"
    src="https://www.geogebra.org/apps/deployggb.js">
  </script><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2022
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Example: Sequential hypothesis testing</div>

<p>Consider a decision maker (DM) that makes a series of i.i.d. observations which may be distributed according to PDF <span class="math inline">\(f_0\)</span> or <span class="math inline">\(f_1\)</span>. Let <span class="math inline">\(Y_t\)</span> denote the observaion at time <span class="math inline">\(t\)</span>. The DM wants to differentiate between two hypothesis: <span class="math display">\[\begin{gather*}
  h_0 : Y_t \sim f_0 \\
  h_1 : Y_t \sim f_1
\end{gather*}\]</span> Typically, we think of <span class="math inline">\(h_0\)</span> as the normal situation (or the null hypothesis) and <span class="math inline">\(h_1\)</span> as an anomaly. For example, the hypothesis may be <span class="math display">\[ h_0: Y_t \sim {\cal N}(0, σ^2) \quad h_1: Y_t \sim {\cal N}(μ, σ^2) \]</span> or <span class="math display">\[ h_0: Y_t \sim \text{Ber}(p) \quad h_1: Y_t \sim \text{Ber}(q). \]</span></p>
<p>Let the random variable <span class="math inline">\(H\)</span> denote the value of the hypothesis. The a priori probability <span class="math inline">\(\PR(H = h_0) = p\)</span>.</p>
<p>The system continues for a finite time <span class="math inline">\(T\)</span>. At each <span class="math inline">\(t &lt; T\)</span>, the DM has three options:</p>
<ul>
<li>stop and declare <span class="math inline">\(h_0\)</span></li>
<li>stop and declare <span class="math inline">\(h_1\)</span></li>
<li>continue and take another measurement</li>
</ul>
<p>At the terminal time step <span class="math inline">\(T\)</span>, the continuation option is not available. Each measurement has a cost <span class="math inline">\(c\)</span>. When the DM takes a stopping action <span class="math inline">\(ν\)</span>, it incurs a stopping cost <span class="math inline">\(\ell(ν, H)\)</span>.</p>
<p>We typically assume <span class="math inline">\(\ell(h_0, h_0) = \ell(h_1, h_1) = 0\)</span>. The term <span class="math inline">\(\ell(h_1, h_0)\)</span> indicates that the DM declares an anomaly when everything is okay. This is called <em>false alarm penalty</em>. The term <span class="math inline">\(\ell(h_0, h_1)\)</span> indicates that DM declares everything is okay when there is an anomaly. This is called the <em>missed detection penalty</em>.</p>
<p>Let <span class="math inline">\(τ\)</span> denote the time when the DM stops. Then the total cost of running the system is <span class="math inline">\(cτ + \ell(ν, H)\)</span>. The objective is to find the optimal stopping strategy that minimize the expected total cost.</p>
<h1 data-number="1" id="dynamic-programming-decomposition"><span class="header-section-number">1</span> Dynamic programming decomposition</h1>
<p>We use the belief-state as an information state to obtain a dynamic programming decomposition. Recall that the beief state is two-dimensional pdf where <span class="math display">\[ b_t(h) = \PR(H = h | Y_{1:t}), \quad h \in \{h_0, h_1\}. \]</span></p>
<dl>
<dt>Remarks</dt>
<dd><ul>
<li><p>We are only conditioning on <span class="math inline">\(Y_{1:t}\)</span> and not adding <span class="math inline">\(A_{1:t-1}\)</span> in the conditioning. This is because we are taking the standard approach used in optimal stopping problems where we are only defining the state for case when the stopping decision hasn’t been taken so far and all previous actions are continue. Taking a continue action does not effect the observations. For this reason, we do not condition on <span class="math inline">\(A_{1:t-1}\)</span>.</p></li>
<li><p>In class, I had exploited the fact that <span class="math inline">\(b_t = [p_t, 1 - p_t]^T\)</span> and written a simplified DP in terms of <span class="math inline">\(p_t\)</span>. In these notes, I don’t make this simplification so that we can see how these results will extend to the case of non-binary hypothesis.</p></li>
</ul>
</dd>
</dl>
<p>The dynamic program for the above model is then given by <span class="math display">\[
  V_T(b_T) = \min\{ \EXP[ \ell(h_0, H) | B_T = b_T], 
                    \EXP[ \ell(h_1, H) | B_T = b_T] \}
\]</span> and for <span class="math inline">\(t \in \{T-1, \dots, 1\}\)</span>, <span class="math display">\[ V_t(b_t) = \min \{ \EXP[ \ell(h_0, H) | B_t = b_t], 
                      \EXP[ \ell(h_1, H) | B_t = b_t],
                     c + \EXP[V_{t+1}(ψ(b_t, Y_{t+1})) | B_t = b_t] \},
\]</span> where <span class="math inline">\(ψ(b, y)\)</span> denotes the standard non-linear filtering update (there is no dependence on <span class="math inline">\(a\)</span> here because there are no state dynamics in this model).</p>
<p>We introduce some notation to simplify the discussion. Define</p>
<ul>
<li><span class="math inline">\(L_i(b) = \EXP[ \ell(h_i, H) | B = b] = \sum_{h \in \{h_0, h_1\}} \ell(h_i, h) b(h)\)</span>.</li>
<li><span class="math inline">\(W_t(b_t) = c + \EXP[V_{t+1}(ψ(b_t, Y_{t+1})) | B_t = b_t]\)</span>.</li>
</ul>
<p>Then, the above DP can be written as <span class="math display">\[
  V_T(b_T) = \min\{ L_0(b_T), L_1(b_T) \}
\]</span> and for <span class="math inline">\(t \in \{T-1, \dots, 1\}\)</span>, <span class="math display">\[ V_t(b_t) = \min \{ L_0(b_t), L_1(b_t), W_t(b_t) \}. \]</span></p>
<h1 data-number="2" id="structure-of-the-optimal-policy"><span class="header-section-number">2</span> Structure of the optimal policy</h1>
<p>We start by establishing simple properties of the different functions defined above.</p>
<div class="highlight">
<dl>
<dt><span id="lemma:1"></span><span id="lemma:concavity" class="pandoc-numbering-text lemma"><strong>Lemma 1</strong></span></dt>
<dd><p>The above functions statisfy the following properties:</p>
<ul>
<li><span class="math inline">\(L_i(b)\)</span> is linear in <span class="math inline">\(b\)</span>.</li>
<li><span class="math inline">\(V_t(b)\)</span> and <span class="math inline">\(W_t(b)\)</span> is concave in <span class="math inline">\(b\)</span>.</li>
<li><span class="math inline">\(V_t(b)\)</span> and <span class="math inline">\(W_t(b)\)</span> are increasing in <span class="math inline">\(t\)</span>.</li>
</ul>
</dd>
</dl>
</div>
<figure>
<div id="applet_container" style="width:800px;height:800px;display:block">

</div>
<figcaption>
An illustration of the minimum of two straight lines and a concave function. Move the points around to see how the shape of the function changes.
</figcaption>
</figure>
<script type="text/javascript">
      var params1 = {
        filename: "/stochastic-control/geogebra/pomdp/hypothesis-testing.ggb",
        enableShiftDragZoom: false,
        width: 800,
        height: 700,
      }

      var params2 = {
        filename: "/stochastic-control/geogebra/pomdp/hypothesis-testing-2.ggb",
        enableShiftDragZoom: false,
        width: 800,
        height: 700,
      }

      var applet1 = new GGBApplet(params1, true);
      var applet2 = new GGBApplet(params2, true);

      window.onload = function() {
          applet1.inject('applet_container');
          applet2.inject('applet_container2');
      }
  </script>
<h4 class="unnumbered" id="proof">Proof</h4>
<p>The linearity of <span class="math inline">\(L_i(b)\)</span> follows from definition. From the discussion on <a href="../pomdp/#theorem:belief-PWLC">POMDPs</a>, we know that <span class="math inline">\(V_{t+1}(b)\)</span> is concave in <span class="math inline">\(b\)</span> and so is <span class="math inline">\(\EXP[V_{t+1}(ψ(b, Y_{t+1})) | B_t = b]\)</span>. Therefore <span class="math inline">\(W_t(b)\)</span> is concave in <span class="math inline">\(b\)</span>.</p>
<p>Finally, by construction, we have that <span class="math inline">\(V_{T-1}(b) \le V_T(b)\)</span>. The monotonicity in time then follows from Q2 of <a href="../../assignments/02">Assignment 2</a>. Sincen <span class="math inline">\(V_t\)</span> is monotone in time, it implies that <span class="math inline">\(W_t\)</span> is also monotone. <span class="math inline">\(\Box\)</span></p>
<p>Now define stopping sets <span class="math inline">\(D_t(h) = \{ b \in Δ^2 : π_t(b) = h \}\)</span> for <span class="math inline">\(h \in \{h_0, h_1\}\)</span>. The key result is the following.</p>
<div class="highlight">
<dl>
<dt><span id="theorem:1"></span><span id="theorem:convex" class="pandoc-numbering-text theorem"><strong>Theorem 1</strong></span></dt>
<dd><p>For all <span class="math inline">\(t\)</span> and <span class="math inline">\(h \in \{h_0, h_1\}\)</span>, the set <span class="math inline">\(D_t(h)\)</span> is convex. Moreover, <span class="math inline">\(D_t(h_i) \subseteq D_{t+1}(h_i)\)</span>.</p>
</dd>
</dl>
</div>
<figure>
<div id="applet_container2" style="width:800px;height:800px;display:block">

</div>
<figcaption>
An illustration of the stopping sets. Move the points around to see how the shape of the stopping set changes.
</figcaption>
</figure>
<h4 class="unnumbered" id="proof-1">Proof</h4>
<p>Note that we can write <span class="math inline">\(D_t(h) = A_t(h) \cap B_t(h)\)</span>, where <span class="math display">\[ A_t(h_i) = \{ b \in Δ^2 : L_i(b)  \le L_j(b) \}
   \quad\text{and}\quad
   B_t(h_i) = \{ b \in Δ^2 : L_i(b) \le W_t(b)  \}. \]</span></p>
<p><span class="math inline">\(A_t(h_i)\)</span> is a the set of <span class="math inline">\(b\)</span> where one linear function of <span class="math inline">\(b\)</span> is less than or equal to another linear function of <span class="math inline">\(b\)</span>. Therefore, <span class="math inline">\(A_t(h_i)\)</span> is a convex set.</p>
<p>Similarly, <span class="math inline">\(B_t(h_i)\)</span> is the set of <span class="math inline">\(b\)</span> where a linear function of <span class="math inline">\(b\)</span> is less than or equal to a concave function of <span class="math inline">\(b\)</span>. Therefore <span class="math inline">\(B_t(h_i)\)</span> is also a convex set.</p>
<p><span class="math inline">\(D_t(h_i)\)</span> is the intersection of two convex sets, and hence convex.</p>
<p>The monotonicty of <span class="math inline">\(D_t(h_i)\)</span> in time follows from the monotonicity of <span class="math inline">\(W_t\)</span> in time. <span class="math inline">\(\Box\)</span></p>
<div class="highlight">
<dl>
<dt><span id="theorem:2"></span><span id="theorem:corner" class="pandoc-numbering-text theorem"><strong>Theorem 2</strong></span></dt>
<dd><p>Suppose the stopping cost satisfy the following: <span class="math display">\[\begin{equation} \label{eq:cost-ass}
\ell(h_0, h_0) \le c \le \ell(h_0, h_1)
  \quad\text{and}\quad
  \ell(h_1, h_1) \le c \le \ell(h_1, h_0). 
\end{equation}\]</span> Then, <span class="math inline">\(e_i \in D_t(h_i)\)</span>, where <span class="math inline">\(e_i\)</span> denotes the standard unit vector (i.e., <span class="math inline">\(e_0 = [1, 0]^T\)</span> and <span class="math inline">\(e_1 = [0, 1]^T\)</span>).</p>
</dd>
</dl>
</div>
<dl>
<dt>Remark</dt>
<dd><p>The assumption on observation cost states that: (i) the cost of observation is greater than the cost incurred when the DM chooses the right hypothesis, and (ii) the cost of observation is less than the cost incurred when the DM chooses the wrong hypothesis. Both these assumptions are fairly natural.</p>
</dd>
</dl>
<h4 class="unnumbered" id="proof-2">Proof</h4>
<p>Note that <span class="math inline">\(L_i(e_0) = \ell(h_i, h_0)\)</span> and <span class="math inline">\(L_i(e_1) = \ell(h_1, h_1)\)</span>. Moreover, by construction, <span class="math inline">\(W_t(b) \ge c\)</span>. Thus, under the above assumption on the cost, <span class="math display">\[ L_0(e_0) = \ell(h_0, h_0) \le c \le W_t(e_0) \]</span> and <span class="math display">\[ L_0(e_0) = \ell(h_0, h_0) \le \ell(h_1, h_0) = L_1(e_0). \]</span> Thus, <span class="math inline">\(e_0 \in D_t(h_0)\)</span>.</p>
<p>By a symmetric argument, we can show that <span class="math inline">\(e_1 \in D_t(h_1)\)</span>. <span class="math inline">\(\Box\)</span></p>
<p><a href="#theorem:convex" title="Theorem 1"><span class="pandoc-numbering-link theorem">Theorem 1</span></a> and <a href="#theorem:corner" title="Theorem 2"><span class="pandoc-numbering-link theorem">Theorem 2</span></a> imply that the optimal stopping regions are convex and include the “corner points” of the simplex. Note that although we formulated the problem for binary hypothesis, all the steps of the proof hold in general as well.</p>
<figure>
<img src="general-stopping.png" 
       title="Stopping regions"
       alt="Stopping regions"
       width="300" />
<figcaption>
Stopping regions for multiple hypothesis
</figcaption>
</figure>
<p>For binary hypothesis, we can present a more concerete characterizatin of the optimal policy. Note that the two-dimensional simplex is equivalent to the interval <span class="math inline">\([0,1]\)</span>. In particular, any <span class="math inline">\(b = Δ^2\)</span> is equal to <span class="math inline">\([p, 1-p]\)</span>, where <span class="math inline">\(p \in [0,1]\)</span>. Now define:</p>
<ul>
<li><span class="math inline">\(\displaystyle b_t = \min\left\{ p \in [0, 1] :  π_t\left(\begin{bmatrix} p \\ 1-p \end{bmatrix}\right) = h_0 \right\}.\)</span></li>
<li><span class="math inline">\(\displaystyle a_t = \max\left\{ p \in [0, 1] :  π_t\left(\begin{bmatrix} p \\ 1-p \end{bmatrix}\right) = h_1 \right\}.\)</span></li>
</ul>
<p>Then, by definition, the optimal policy has the following threshold property:</p>
<div class="highlight">
<dl>
<dt><span id="proposition:1"></span><span id="proposition:threshold" class="pandoc-numbering-text proposition"><strong>Proposition 1</strong></span></dt>
<dd><p>Let <span class="math inline">\(\bar π_t(p) = π_t([p, 1-p]^T)\)</span>. Then, under \eqref{eq:cost-ass}, <span class="math display">\[ \bar π_t(p) = \begin{cases}
   h_1, &amp; \text{if } p \le a_t \\
   \mathsf{C}, &amp; \text{if } a_t &lt; p &lt; b_t \\
   h_0, &amp; \text{if } b_t \le p.
  \end{cases} \]</span></p>
<p>Furthermore, the decision thresholds are monotone in time. In particular, for all <span class="math inline">\(t\)</span>, <span class="math display">\[ a_t \le a_{t+1} \le b_{t+1} \le b_t. \]</span></p>
</dd>
</dl>
</div>
<p>The above property is simplies stated slighted in terms of the likelihood ratio. In particular, define <span class="math inline">\(λ_t = b_t(0)/b_t(1) = p_t/(1 - p_t)\)</span>. Then, we have the following:</p>
<div class="highlight">
<dl>
<dt><span id="proposition:2"></span><span id="proposition:likelihood" class="pandoc-numbering-text proposition"><strong>Proposition 2</strong></span></dt>
<dd><p>Let <span class="math inline">\(\hat π_t(λ) = π_t([λ/(1+λ), 1/(1+λ)]^T)\)</span>. Then, under \eqref{eq:cost-ass}, <span class="math display">\[ \hat π_t(λ) = \begin{cases}
   h_1, &amp; \text{if } λ \le a_t/(1 - a_t) \\
   \mathsf{C}, &amp; \text{if } a_t/(1 - a_t) &lt; λ &lt; b_t/(1 - b_t)_t \\
   h_0, &amp; \text{if } b_t/(1 - b_t)_t \le λ.
  \end{cases} \]</span></p>
</dd>
</dl>
</div>
<h4 class="unnumbered" id="proof-3">Proof</h4>
<p>For any <span class="math inline">\(a, b \in [0, 1]\)</span>, <span class="math display">\[ a \le b \iff \frac{a}{1-a} \le \frac{b}{1-b}. \qquad \Box \]</span></p>
<p>The result of <a href="#proposition:likelihood" title="Proposition 2"><span class="pandoc-numbering-link proposition">Proposition 2</span></a> is called the <em>sequential</em> likelihood ratio test (SLRT) or <em>sequential</em> probability ratio test (SPRT) to contrast it with the standard <a href="https://en.wikipedia.org/wiki/Likelihood-ratio_test">likelihood ratio test</a> in hypotehsis testing.</p>
<h1 data-number="3" id="infinite-horizon-setup"><span class="header-section-number">3</span> Infinite horizon setup</h1>
<p>Assume that <span class="math inline">\(T = ∞\)</span> so that the continuation alternative is always available. Then, we have the following.</p>
<div class="highlight">
<dl>
<dt><span id="theorem:3"></span><span id="theorem:inf" class="pandoc-numbering-text theorem"><strong>Theorem 3</strong></span></dt>
<dd><p>Under \eqref{eq:cost-ass}, an optimal decision rule always exists, is time-homogeneous, and is given by the solution of the following DP: <span class="math display">\[ V(b) = \min\{ L_0(b) , L_1(b) , W(b) \} \]</span> where <span class="math display">\[ W(b) = c + \int_{y} [ pf_0(y) + (1-p)f_1(y)] V(ψ(b,y)) dy. \]</span></p>
<p>Therefore, the optimal thresholds <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are time-homogeneous.</p>
</dd>
</dl>
</div>
<h4 class="unnumbered" id="proof-4">Proof</h4>
<p>The result follows from standard results on non-negative dynamic programming. We did not cover non-negative DP. Essentially it determines conditions under which undiscounted infinite horizon problems have a solution when the per-step cost is non-negative.</p>
<h2 data-number="3.1" id="upper-bound-on-the-expected-number-of-measurements"><span class="header-section-number">3.1</span> Upper bound on the expected number of measurements</h2>
<p>For simplicity, we assume that <span class="math inline">\(\ell(h_0, h_0) = \ell(h_1, h_1) = 0\)</span>. For the infinite horizon model, we can get upper bound on the expected number of measurements that an optimal policy will take. Let <span class="math inline">\(τ\)</span> denote the number of measurements taken under policy <span class="math inline">\(π\)</span> and <span class="math inline">\(A_τ\)</span> denote the terminal action after stopping. Then, the performance of policy <span class="math inline">\(π\)</span> is given by <span class="math display">\[
  J(π) = \EXP[ c τ + \ell(H, A_\tau) \mid \Pi = b ].
\]</span> Note that <span class="math inline">\(\ell(H, A_\tau) \ge 0\)</span>. Therefore, the performance of the optimal policy is lower bounded by <span class="math display">\[
  J^* \ge c\, \EXP^{π^*}[  τ \mid \Pi = b] .
\]</span> Now, consider a policy <span class="math inline">\(\tilde π\)</span> which does not consider continuation action and takes the best stopping decision. The performance of <span class="math inline">\(\tilde π\)</span> is given by <span class="math display">\[ J(\tilde π) = \min \{ \ell(h_1, h_0) b_1, \ell(h_0, h_1) b_0 \}. \]</span> Since <span class="math inline">\(J(\tilde π) \ge J^*\)</span>, we get <span class="math display">\[
  \EXP^{π^*}[ τ  \mid \Pi = b ] \le \frac 1c 
  \min \{ \ell(h_1, h_0) b_1, \ell(h_0, h_1) b_0 \}. \]</span></p>
<h1 class="unnumbered" id="exercises">Exercises</h1>
<ol type="1">
<li><p>Consider the following modification of the sequential hypothesis testing. As in the model discussed above, there are two hypothesis <span class="math inline">\(h_0\)</span> and <span class="math inline">\(h_1\)</span>. The a priori probability that the hypothesis is <span class="math inline">\(h_0\)</span> is <span class="math inline">\(p\)</span>.</p>
<p>In contrast to the model discussed above, there are <span class="math inline">\(N\)</span> sensors. If the underlying hypothesis is <span class="math inline">\(h_i\)</span> and sensor <span class="math inline">\(m\)</span> is used at time <span class="math inline">\(t\)</span>, then the observation <span class="math inline">\(Y_t\)</span> is distrubted according to pdf (or pmf) <span class="math inline">\(f^m_i(y)\)</span>. The cost of using sensor <span class="math inline">\(m\)</span> is <span class="math inline">\(c_m\)</span>.</p>
<p>Whenever the decision maker takes a measurement, he picks a sensor <span class="math inline">\(m\)</span> uniformly at random from <span class="math inline">\(\{1, \dots, N\}\)</span> and observes <span class="math inline">\(Y_t\)</span> according to the distribution <span class="math inline">\(f^m_i(\cdot)\)</span> and incurs a cost <span class="math inline">\(c_m\)</span>.</p>
<p>The system continues for a finite time <span class="math inline">\(T\)</span>. At each time <span class="math inline">\(t &lt; T\)</span>, the decision maker has three options: stop and declare <span class="math inline">\(h_0\)</span>, stop and declare <span class="math inline">\(h_1\)</span>, or continue to take another measurement. At time <span class="math inline">\(T\)</span>, the continue alternative is unavailable.</p>
<ol type="a">
<li><p>Formulate the above problem as a POMDP. Identify an information state and write the dynamic programming decomposition for the problem.</p></li>
<li><p>Show that the optimal control law has a threshold property, similar to the threshold propertly for the model described above.</p></li>
</ol></li>
<li><p>In this exercise, we will derive an approximate method to compute the performance of a given threshold based policy for infinite horizon sequential hypothesis testing problem. Let <span class="math display">\[ θ_i(π,p) = \EXP^{π}[ τ | H = h_i] \]</span> denote the expected number of samples when using stopping rule <span class="math inline">\(π\)</span> assuming that the true hypothesis is <span class="math inline">\(h_i\)</span>. Note that for any belief state based stopping rule, <span class="math inline">\(θ_i\)</span> depends on the initial belief <span class="math inline">\([p, 1-p]\)</span>. Furthermore, let <span class="math display">\[ ξ_i(h_k ;π, p) = \PR^π(A_τ = h_k | H = h_i) \]</span> denote the probability that the stopping action is <span class="math inline">\(h_k\)</span> when using stopping rule <span class="math inline">\(π\)</span> assuming that the true hypothesis is <span class="math inline">\(h_i\)</span>.</p>
<ol type="a">
<li><p>Argue that the performance of any policy <span class="math inline">\(π\)</span> can be written as <span class="math display">\[\begin{align*}
V_π(p) &amp;= c [ p θ_0(π, p) + (1-p) θ_1(π,p) ] \\
&amp; \quad + p \sum_{a \in \{h_0, h_1\}} \ell(a, h_0) ξ_0(a; π, p) \\
&amp; \quad + (1-p) \sum_{a \in \{h_0, h_1\}} \ell(a, h_1) ξ_1(a; π, p).
\end{align*}\]</span> Thus, approximately computing <span class="math inline">\(θ_i\)</span> and <span class="math inline">\(ξ_i\)</span> gives an approximate value of <span class="math inline">\(V_π(p)\)</span>.</p></li>
<li><p>Now assume that the policy <span class="math inline">\(π\)</span> is of a threshold form with thresholds <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. To avoid trivial cases, we assume that <span class="math inline">\(p \in (a,b)\)</span>. The key idea to compute <span class="math inline">\(θ_i\)</span> and <span class="math inline">\(ξ_i\)</span> is that the evolution of <span class="math inline">\(p_t = \PR(H = h_t | Y_{1:t})\)</span> is a Markov chain which starts at a state <span class="math inline">\(p \in (a,b)\)</span> and stops the first time <span class="math inline">\(p_t\)</span> goes below <span class="math inline">\(a\)</span> or above <span class="math inline">\(b\)</span>.</p>
<figure>
<p><img src="discretization.png" 
       title="Discretization of the state space"
       alt="Discretization of the state space"
       width="300" /></p>
<figcaption>
<p>Discretization of the state space</p>
</figcaption>
</figure>
<p>Suppose we discretize the state space space <span class="math inline">\([0, 1]\)</span> into <span class="math inline">\(n+1\)</span> grid points <span class="math inline">\(\ALPHABET D_n = \{0, \frac1n, \dots, 1\}\)</span>. Assume that <span class="math inline">\(p\)</span>, <span class="math inline">\(a\)</span>, and <span class="math inline">\(b\)</span> lie on this discrete grid. Discreteize <span class="math inline">\(p_t\)</span> to the closest grid point and let <span class="math inline">\(P_i\)</span> denote the transition matrix of the discretized <span class="math inline">\(p_t\)</span> when the true hypothesis is <span class="math inline">\(h_i\)</span>. Partition the <span class="math inline">\(P_i\)</span> as <span class="math display">\[ \left[\begin{array}{c|c|c}
    A_i &amp; B_i &amp; C_i \\
    \hline
    D_i &amp; E_i &amp; F_i \\
    \hline
    G_i &amp; H_i &amp; J_i 
   \end{array}\right] \]</span> where the lines correspond to the index for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The transition matrix of the absorbing Markov chain is given by <span class="math display">\[ \hat P_i = \left[\begin{array}{c|c|c}
    I &amp; 0 &amp; I \\
    \hline
    D_i &amp; E_i &amp; F_i \\
    \hline
    I &amp; 0 &amp; I 
   \end{array}\right] \]</span> Now suppose <span class="math inline">\(j\)</span> is the index of <span class="math inline">\(p\)</span> in <span class="math inline">\(\ALPHABET D_n\)</span>. Using properties of absorbing Markov chains, show that</p>
<ul>
<li><span class="math inline">\(ξ_i(h_0; \langle a, b \rangle, p) \approx  [ (I - E_i)^{-1} F_i \mathbf{1} ]_j\)</span></li>
<li><span class="math inline">\(ξ_i(h_1; \langle a, b \rangle, p) \approx  [ (I - E_i)^{-1} D_i \mathbf{1} ]_j\)</span></li>
<li><span class="math inline">\(θ_i(\langle a, b \rangle, p) \approx  [ (I - E_i)^{-1} \mathbf{1} ]_j\)</span></li>
</ul></li>
</ol></li>
</ol>
<h1 class="unnumbered" id="references">References</h1>
<p>For more details on sequential hypothesis testing, incuding an approximate method to determine the thresholds, see <span class="citation" data-cites="Wald1945"><a href="#ref-Wald1945" role="doc-biblioref">Wald</a> (<a href="#ref-Wald1945" role="doc-biblioref">1945</a>)</span>. The optimal of sequential likelihood ratio test was proved in <span class="citation" data-cites="Wald1948"><a href="#ref-Wald1948" role="doc-biblioref">Wald and Wolfowitz</a> (<a href="#ref-Wald1948" role="doc-biblioref">1948</a>)</span>. The model described above was first considered by <span class="citation" data-cites="Arrow1949"><a href="#ref-Arrow1949" role="doc-biblioref">Arrow et al.</a> (<a href="#ref-Arrow1949" role="doc-biblioref">1949</a>)</span>. See <span class="citation" data-cites="DeGroot1970"><a href="#ref-DeGroot1970" role="doc-biblioref">DeGroot</a> (<a href="#ref-DeGroot1970" role="doc-biblioref">1970</a>)</span>.</p>
<p>The upper bound on expected number of measurements is adapted from an argument presented in <span class="citation" data-cites="Hay2012"><a href="#ref-Hay2012" role="doc-biblioref">Hay et al.</a> (<a href="#ref-Hay2012" role="doc-biblioref">2012</a>)</span>.</p>
<p>Execrise 1 is from <span class="citation" data-cites="Bai2015"><a href="#ref-Bai2015" role="doc-biblioref">Bai et al.</a> (<a href="#ref-Bai2015" role="doc-biblioref">2015</a>)</span>. Exercise 2 is from <span class="citation" data-cites="Woodall1983"><a href="#ref-Woodall1983" role="doc-biblioref">Woodall and Reynolds</a> (<a href="#ref-Woodall1983" role="doc-biblioref">1983</a>)</span>.</p>
<hr />
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Arrow1949" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Arrow, K.J., Blackwell, D., and Girshick, M.A.</span> 1949. Bayes and minimax solutions of sequential decision problems. <em>Econometrica</em> <em>17</em>, 3/4, 213. DOI: <a href="https://doi.org/10.2307/1905525">10.2307/1905525</a>.</p>
</div>
<div id="ref-Bai2015" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Bai, C.-Z., Katewa, V., Gupta, V., and Huang, Y.-F.</span> 2015. A stochastic sensor selection scheme for sequential hypothesis testing with multiple sensors. <em>IEEE transactions on signal processing</em> <em>63</em>, 14, 3687–3699.</p>
</div>
<div id="ref-DeGroot1970" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">DeGroot, M.</span> 1970. <em>Optimal statistical decisions</em>. Wiley-Interscience, Hoboken, N.J.</p>
</div>
<div id="ref-Hay2012" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Hay, N., Russell, S., Tolpin, D., and Shimony, S.E.</span> 2012. Selecting computations: Theory and applications. <em>UAI</em>. Available at: <a href="http://www.auai.org/uai2012/papers/123.pdf">http://www.auai.org/uai2012/papers/123.pdf</a>.</p>
</div>
<div id="ref-Wald1945" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Wald, A.</span> 1945. Sequential tests of statistical hypotheses. <em>The Annals of Mathematical Statistics</em> <em>16</em>, 2, 117–186. DOI: <a href="https://doi.org/10.1214/aoms/1177731118">10.1214/aoms/1177731118</a>.</p>
</div>
<div id="ref-Wald1948" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Wald, A. and Wolfowitz, J.</span> 1948. Optimum character of the sequential probability ratio test. <em>The Annals of Mathematical Statistics</em> <em>19</em>, 3, 326–339. DOI: <a href="https://doi.org/10.1214/aoms/1177730197">10.1214/aoms/1177730197</a>.</p>
</div>
<div id="ref-Woodall1983" class="csl-entry" role="doc-biblioentry">
<p><span class="smallcaps">Woodall, W.H. and Reynolds, M.R.</span> 1983. A discrete markov chain representation of the sequential probability ratio test. <em>Communications in Statistics. Part C: Sequential Analysis</em> <em>2</em>, 1, 27–44. DOI: <a href="https://doi.org/10.1080/07474948308836025">10.1080/07474948308836025</a>.</p>
</div>
</div>


<p class="categories">
This entry 

 was last updated on 17 Dec 2021
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/pomdp">
  POMDP</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/pomdp">pomdp</a>,
<a href="https://adityam.github.io/stochastic-control/tags/belief-state">belief state</a>,
<a href="https://adityam.github.io/stochastic-control/tags/hypothesis-testing">hypothesis testing</a>.</p>



    </div>
  </body>
</html>


