<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Theory on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/theory/</link>
    <description>Recent content in Theory on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://adityam.github.io/stochastic-control/theory/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Stochastic optimization</title>
      <link>https://adityam.github.io/stochastic-control/theory/stochastic-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/stochastic-optimization/</guid>
      <description>Letâ€™s start with a simple stochastic optimization problem. Suppose \(\ALPHABET X\), \(\ALPHABET U\), \(\ALPHABET W\) are finite sets and let \(X \in \ALPHABET X\) and \(W \in \ALPHABET W\) be random variables defined on a common probability space. A decision maker observes the realization of \(X\) and chooses and action \(U \in \ALPHABET U\) according to a decison rule \(g\), i.e., \[ U = g(X) \] and incurs a cost \[J(g) = \EXP[ c(X, g(X), W) ],\] where the expectation is taken with respect to the joint probability distribution of \((X,W)\).</description>
    </item>
    
    <item>
      <title>Markov decision processes</title>
      <link>https://adityam.github.io/stochastic-control/theory/mdp-functional/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/mdp-functional/</guid>
      <description>Markov decision processes (MDP) model model the simplest stochastic control architecture. The dynamic behavior of an MDP is modeled by an equation of the form \[ X_{t+1} = f_t(X_t, U_t, W_t) \] where \(X_t \in \ALPHABET X\) is the state, \(U_t \in \ALPHABET U\) is the control input, and \(W_t \in \ALPHABET W\) is the noise. An agent/controller observes the state and chooses the control input \(U_t\).
The controller can be as sophisticated as we want.</description>
    </item>
    
    <item>
      <title>Stochastic dominance</title>
      <link>https://adityam.github.io/stochastic-control/theory/stochastic-dominance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/stochastic-dominance/</guid>
      <description>Stochastic dominance is a partial order on random variables. Let \(\ALPHABET X\) be a totally ordered finite set, say \(\{1, \dots, n\}\) and let \(\Delta(\ALPHABET X)\) denote the state of pmfs on \(\ALPHABET X\).
 Definition Suppose \(X\) and \(Y\) are \(\ALPHABET X\) valued random variables where \(X \sim \pi\) and \(Y \sim \mu\). We say \(X\) stochastically dominates \(Y\) if for any \(x \in \ALPHABET X\), \[ \PR(X \ge x) \ge \PR(Y \ge x).</description>
    </item>
    
    <item>
      <title>Blackwell&#39;s principle of irrelevant information</title>
      <link>https://adityam.github.io/stochastic-control/theory/principle-of-irrelevant-information/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/principle-of-irrelevant-information/</guid>
      <description>Theorem Let \(\ALPHABET X\), \(\ALPHABET Y\), and \(\ALPHABET U\) be standard Borel spaces and \(X \in \ALPHABET X\) and \(Y \in \ALPHABET Y\) be random variables defined on a common probability space.
A decision maker observes \((X,Y)\) and chooses \(U\) to minimize \(\EXP[c(X,U)]\), where \(c \colon \ALPHABET X \times \ALPHABET U \to \reals\) is a measurable function.
Then, there is no loss of optimality in choosing \(U\) only as a function of \(X\).</description>
    </item>
    
  </channel>
</rss>