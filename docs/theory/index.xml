<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Theory on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/theory/</link>
    <description>Recent content in Theory on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://adityam.github.io/stochastic-control/theory/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Stochastic optimization</title>
      <link>https://adityam.github.io/stochastic-control/theory/stochastic-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/stochastic-optimization/</guid>
      <description>Let’s start with a simple stochastic optimization problem. Suppose \(\ALPHABET X\), \(\ALPHABET U\), \(\ALPHABET W\) are finite sets and let \(X \in \ALPHABET X\) and \(W \in \ALPHABET W\) be random variables defined on a common probability space. A decision maker observes the realization of \(X\) and chooses and action \(U \in \ALPHABET U\) according to a decison rule \(g\), i.e., \[ U = g(X) \] and incurs a cost \[J(g) = \EXP[ c(X, g(X), W) ],\] where the expectation is taken with respect to the joint probability distribution of \((X,W)\).</description>
    </item>
    
    <item>
      <title>Blackwell&#39;s principle of irrelevant information</title>
      <link>https://adityam.github.io/stochastic-control/theory/principle-of-irrelevant-information/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/principle-of-irrelevant-information/</guid>
      <description>Theorem Let \(\ALPHABET X\), \(\ALPHABET Y\), and \(\ALPHABET U\) be standard Borel spaces and \(X \in \ALPHABET X\) and \(Y \in \ALPHABET Y\) be random variables defined on a common probability space.
A decision maker observes \((X,Y)\) and chooses \(U\) to minimize \(\EXP[c(X,U)]\), where \(c \colon \ALPHABET X \times \ALPHABET U \to \reals\) is a measurable function.
Then, there is no loss of optimality in choosing \(U\) only as a function of \(X\).</description>
    </item>
    
    <item>
      <title>Markov decision processes</title>
      <link>https://adityam.github.io/stochastic-control/theory/mdp-functional/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/mdp-functional/</guid>
      <description>Markov decision processes (MDP) model model the simplest stochastic control architecture. The dynamic behavior of an MDP is modeled by an equation of the form \[ X_{t+1} = f_t(X_t, U_t, W_t) \] where \(X_t \in \ALPHABET X\) is the state, \(U_t \in \ALPHABET U\) is the control input, and \(W_t \in \ALPHABET W\) is the noise. An agent/controller observes the state and chooses the control input \(U_t\).
The controller can be as sophisticated as we want.</description>
    </item>
    
    <item>
      <title>Matrix formulation of Markov decision processes</title>
      <link>https://adityam.github.io/stochastic-control/theory/mdp-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/mdp-matrix/</guid>
      <description>In this section, we present a matrix formulation for finite state finite action MDPs, which is useful for computing the solutions numerically. Let’s start with the function model described earlier and assume that \(\ALPHABET X\) and \(\ALPHABET U\) are finite sets and that the cost function and the probability distribution of \(\{W_t\}_{t \ge 1}\) are time-homogeneous. Then, the following is a fundamental property of MDPs:
 Lemma For any \(x_1, x_2, \dots, x_T \in \ALPHABET X\) and \(u_1, \dots, u_T \in \ALPHABET U\), we have \[ \PR(X_{t+1} = x_{t+1} | X_{1:t} = x_{1:t}, U_{1:t} = u_{1:t}) = \PR(X_{t+1} = x_{t+1} | X_{t} = x_t , U_t = u_t).</description>
    </item>
    
    <item>
      <title>Linear Quadratic Regulation (LQR)</title>
      <link>https://adityam.github.io/stochastic-control/theory/lqr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/lqr/</guid>
      <description>Note: To be consistent with the notation used in linear systems, we denote the state and action by lowercase \(x\) and \(u\), even for stochastic systems (unlike the notation used for other models where we use uppercase \(X\) and \(U\) for state and actions to emphasize the fact they are random variables.
We start by considering a determinisitc linear system with state \(x_t \in \reals^n\) and control actions \(u_t \in \reals^m\) and dynamics \[ x_{t+1} = A_t x_t + B_t u_t,\] where \(A_t \in \reals^{n \times n}\) and \(B_t \in \reals^{n \times m}\) are known matrices.</description>
    </item>
    
    <item>
      <title>Stochastic dominance</title>
      <link>https://adityam.github.io/stochastic-control/theory/stochastic-dominance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/stochastic-dominance/</guid>
      <description>Stochastic dominance is a partial order on random variables. Let \(\ALPHABET X\) be a totally ordered finite set, say \(\{1, \dots, n\}\) and let \(\Delta(\ALPHABET X)\) denote the state of pmfs on \(\ALPHABET X\).
 Definition Suppose \(X\) and \(Y\) are \(\ALPHABET X\) valued random variables where \(X \sim \pi\) and \(Y \sim \mu\). We say \(X\) stochastically dominates \(Y\) if for any \(x \in \ALPHABET X\), \[ \PR(X \ge x) \ge \PR(Y \ge x).</description>
    </item>
    
    <item>
      <title>Monotone value functions and policies</title>
      <link>https://adityam.github.io/stochastic-control/theory/monotonicity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/monotonicity/</guid>
      <description>Consider the matrix formulation of MDPs and suppose the state space \(\ALPHABET X\) is totally ordered. In many applications, it is useful to know if the value function is increasing (or decreasing) in state.
 Theorem Consider an MDP where the state space \(\ALPHABET X\) is totally ordered. Suppose the following conditions are satisfied.
C1. For every \(u \in \ALPHABET U\), the per-step cost \(c_t(x,u)\) is weakly inceasing in \(x\).</description>
    </item>
    
    <item>
      <title>Optimality of threshold policies in optimal stopping</title>
      <link>https://adityam.github.io/stochastic-control/theory/monotonicity-optimal-stopping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/monotonicity-optimal-stopping/</guid>
      <description>Consider an optimal stopping problem and define the benefit function as the expected benefit1 of delaying the stopping decision at time \(t\), i.e., \[ B_t(x) = c_t(x) + \EXP[ V_{t+1}( X_{t+1} | X_t = x] - s_t(x). \](1) Thus, it is optimal to stop whenever \(B_t(x) \ge 0\).
Note that, we can write the value function in terms of the benefit function as follows: \[\begin{align} V_t(x) &amp;amp;= \min\{ s_t(x), B_t(x) + s_t(x) \} \\ &amp;amp;= s_t(x) + [ B_t(x) ]^-, \end{align}\](2) where \([y]^-\) is a short hand for \(\min\{y, 0\}\).</description>
    </item>
    
    <item>
      <title>Infinite horizon discounted MDP</title>
      <link>https://adityam.github.io/stochastic-control/theory/discounted-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/theory/discounted-mdp/</guid>
      <description>Throughout this section, we assume that \(\ALPHABET X\) and \(\ALPHABET U\) are finite and \(|\ALPHABET X|= n\) and \(|\ALPHABET U| = m\).
Performance of a time-homogeneous Markov policy For any \(g \colon \ALPHABET X \to \ALPHABET U\), consider the time homogeneous policy \((g, g, \dots)\). For ease of notation, we denote this policy simply by \(g\). The expected discounted cost under this policy is given by \[ V_g(x) = \EXP^g\bigg[ \sum_{t=1}^∞ β^{t-1} c(X_t, U_t) \biggm| X_1 = x \bigg].</description>
    </item>
    
  </channel>
</rss>