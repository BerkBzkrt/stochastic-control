<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="information state" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" />
  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/github.css" type="text/css" />
  <script src="https://adityam.github.io/stochastic-control//js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script type="text/javascript"
    src="https://cdn.plot.ly/plotly-1.2.0.min.js">
  </script>
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_SVG">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "HTML-CSS": { 
          fonts: ["TeX"]
      }, 
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
    },
      TeX : {
          Macros : {
            PR: "\\mathbb{P}",
            EXP: "\\mathbb{E}",
            IND: "\\mathbb{I}",
            reals: "\\mathbb{R}",
            integers: "\\mathbb{Z}",
            TRANS: "\\intercal",
            VEC: "\\operatorname{vec}",
            TR:  "\\operatorname{Tr}",
            // mathcal: "\\mathscr",
            ALPHABET: ["\\mathcal{#1}", 1],
            MATRIX: ["\\begin{bmatrix} #1 \\end{bmatrix}", 1],
            NORM: ["\\left\\lVert #1 \\right\\rVert", 1],
            ABS: ["\\left\\lvert #1 \\right\\rvert", 1],
          }
      }
    });
  </script>
</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2018
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<h1>Dynamic programming for general models</h1>

<p>Consider a general controlled input output system with two inputs—control action <span class="math inline">\(U_t\)</span> and disturbance <span class="math inline">\(W_t\)</span>—and two outputs—observation <span class="math inline">\(Y_t\)</span> and cost <span class="math inline">\(C_t\)</span>. We assume that the system has an initial state, which we denote by <span class="math inline">\(X_1\)</span>, and assume to be random. <em>We do not assume a state-space model for the system, but simply assume an input-ouput model</em>, i.e., there exist functions <span class="math inline">\(\{f_t\}_{t \ge 1}\)</span> and <span class="math inline">\(\{h_t\}_{t \ge 1}\)</span> such that<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> <a name="eq:Y"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[ Y_t = f_t(X_1, U_{1:t-1}, W_{1:t-1}) \]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(1)</span></span>  and <a name="eq:C"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[ C_t = h_t(X_1, U_{1:t}, W_{1:t-1}). \]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(2)</span></span> </p>
<p>For simplicity, we assume that all variables are finite valued and <span class="math inline">\((X_1, W_1, W_2, \dots)\)</span> are independent random variables that are defined on a common probability space.</p>
<p>A decision-maker observes the output process<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> <span class="math inline">\(\{Y_t\}_{t \ge 1}\)</span> and chooses the control input <span class="math inline">\(U_t\)</span> based on all the information <span class="math inline">\(I_t = \{ Y_{1:t}, U_{1:t-1} \}\)</span> available to it, i.e., <span class="math display">\[ U_t = g_t(I_t). \]</span></p>
<p>The decision-maker is interested in choosing a <em>control strategy</em> <span class="math inline">\(g := (g_1, \dots, g_T)\)</span> to minimize the total expected cost over horizon <span class="math inline">\(T\)</span>, which is given by <span class="math display">\[ J(g) = \EXP^g\bigg[ \sum_{t=1}^T C_t \bigg], \]</span> where the expectation is taken with respect to the joint measure on <span class="math inline">\((C_1, \dots, C_T)\)</span> induced by the choice of the strategy <span class="math inline">\(g\)</span>.</p>
<h2 id="dynamic-programming-decomposition">Dynamic programming decomposition</h2>
<p>It is always possible to write a dynamic program in terms of the information <span class="math inline">\(I_t\)</span>. To show this, we follow the same idea that we used to establish the <a href="../MDP-functional#DP">dynamic programming decomposition</a> for MDPs. Given any strategy <span class="math inline">\(g\)</span>, let <span class="math inline">\(J_t(i_t; g)\)</span> denote <em>the cost-to-go function</em> from time <span class="math inline">\(t\)</span> onwards when the information <span class="math inline">\(I_t\)</span> is <span class="math inline">\(i_t\)</span>, i.e., <span class="math display">\[
J_t(i_t; g) = \EXP^g \bigg[ \sum_{s=t}^T C_s \biggm| I_t = i_t \bigg].
\]</span> Since the past inputs <span class="math inline">\(U_{1:t-1}\)</span> are a part of the information <span class="math inline">\(I_t\)</span>, <span class="math inline">\(J_t(i_t;g)\)</span> only depends on the future strategy <span class="math inline">\((g_t, \dots, g_T)\)</span> and does not depend on the past strategy <span class="math inline">\((g_1, \dots, g_{t-1})\)</span>. These functions can be computed recursively as follows. <span class="math display">\[ \begin{align}
J_t(i_t; g) &amp;= \EXP^g\bigg[ \sum_{s=t}^T C_s \biggm| I_t = i_t \bigg] \\
&amp;\stackrel{(a)}= \EXP^g \bigg[ C_t + \EXP^g\bigg[ \sum_{s = t+1}^T C_s \biggm| I_{t+1}
\bigg] \biggm| I_t = i_t \bigg] \\
&amp;= \EXP^g \big[ C_s + J_{t+1}(I_{t+1}; g) \bigm| I_t = i_t \big]
\end{align} \]</span> where <span class="math inline">\((a)\)</span> follows from the towering property of conditional expectation and the fact that the decision maker has perfect recall, i.e., <span class="math inline">\(I_t \subseteq I_{t+1}\)</span>.</p>
<p>Now, as was the case for MDP, we can write a dynamic programming decomposition for a general decision problem.</p>
<div class="highlight">
<dl>
<dt><span id="thm:DP" class="pandoc-numbering-text thm"><strong>Theorem 1</strong></span></dt>
<dd><p><strong>(Dynamic Program)</strong><br />
Recursively define <em>value function</em> <span class="math inline">\(\{V_t\}_{t=1}^{T}\)</span>, where <span class="math inline">\(V_t \colon I_t \mapsto \reals\)</span> as follows: <a name="eq:DP-1"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[ V_{T+1}(i_{T+1}) = 0 \]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(3)</span></span>  and for <span class="math inline">\(t \in \{T, \dots, 1\}\)</span>: <a name="eq:DP-Q"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[ Q_t(i_t, u_t) = \EXP\big[ C_t + \EXP[ V_{t+1}(I_{t+1}) \bigm| I_t = i_t,
U_t = u_t \big]\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(4)</span></span>  and define <a name="eq:DP-2"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[V_t(i_t) = \min_{u_t \in \ALPHABET U} Q_t(i_t, u) \]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(5)</span></span>  and <a name="eq:DP-policy"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[ g^*_t(i_t) = \arg\min_{u_t \in \ALPHABET U} Q_t(i_t, u_t). \]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(6)</span></span>  Then, a policy is optimal if and only if it satisfies (<a href="#eq:DP-policy">10</a>).</p>
</dd>
</dl>
</div>
<p>Note that the expectation in the right hand side of (<a href="#eq:DP-Q">4</a>) does not depend on the choice of policy. This is because (i) <span class="math inline">\(I_{t+1} = \{I_t, Y_{t+1}, U_t\}\)</span>, and (ii) <span class="math inline">\(\PR( Y_{t+1}, C_t | I_t, U_t)\)</span> does not depend on the policy <span class="math inline">\(g\)</span> due to (<a href="#eq:Y">1</a>) and (<a href="#eq:C">2</a>).</p>
<p>As was the case for MDPs, instead of proving the above result, we prove a related result.</p>
<div class="highlight">
<dl>
<dt><span id="thm:CP" class="pandoc-numbering-text thm"><strong>Theorem 2</strong></span></dt>
<dd><p><strong>(The comparison principle)</strong><br />
For any strategy <span class="math inline">\(g\)</span> <span class="math display">\[ J_t(i_t; g) \ge V_t(i_t) \]</span> with equality at <span class="math inline">\(t\)</span> if and only if the <em>future strategy</em> <span class="math inline">\((g_t, \dots, g_T)\)</span> satisfies the verification step (<a href="#eq:DP-policy">10</a>).</p>
</dd>
</dl>
</div>
<p>Note that the comparison principle immediately implies that the strategy obtained using dynamic programming is optimal.</p>
<h4 id="proof-of-the-comparison-principle">Proof of the comparison principle</h4>
<p>The proof proceeds by backward induction. For <span class="math inline">\(t = T\)</span> <span class="math display">\[ \begin{align}
J_T(i_T; g) &amp;= \EXP[ C_T | I_T = i_T, U_T = g_T(i_T) ] \\
&amp;= Q_T(i_T, g_T(i_T) ) \\
&amp;\ge V_T(i_t).
\end{align} \]</span> This forms the basis of induction. Now assume that the state statement of the theorem is true for <span class="math inline">\(t+1\)</span>. Then for time <span class="math inline">\(t\)</span> <span class="math display">\[ \begin{align}
J_t(i_i; g) &amp;= \EXP^g\big[ C_t + J_{t+1}(I_{t+1}) \bigm| I_t = i_t, U_t =
g_t(i_t) \big] \\
&amp;\stackrel{(a)}\ge \EXP^g\big[ C_t + V_{t+1}(I_{t+1}) \bigm| I_t = i_t, U_t = g_t(i_t) \big]
\\
&amp;= Q_t(i_t, g_t(i_t)) \\
&amp;\ge V_t(i_t),
\end{align} \]</span> where <span class="math inline">\((a)\)</span> follows from the induction hypothesis. Note that we have equality in <span class="math inline">\((a)\)</span> if and only if <span class="math inline">\(g_{t+1:T}\)</span> satisfy (<a href="#eq:DP-policy">10</a>) (this is part of the induction hypothesis) and we have equality in the last step if and only if <span class="math inline">\(g_t\)</span> satisfies (<a href="#eq:DP-policy">10</a>).</p>
<h1 id="information-state">Information state</h1>
<p>An information state <span class="math inline">\(\{Z_t\}_{t \ge 1}\)</span> is a function of the information <span class="math inline">\(I_t\)</span> (more precisely is measurable wrt the observation sigma algebra) which we denote by <span class="math inline">\(Z_t = F_t(I_t)\)</span> that satisfies the following properties:</p>
<ol type="1">
<li><p><strong>Is sufficient for predicting itselt</strong>, i.e., <span class="math display">\[ \PR(Z_{t+1} = z_{t+1} | I_t = i_t, U_t = u_t) 
 = \PR(Z_{t+1} = z_{t+1} | Z_t = F_t(i_t), U_t = u_t). \]</span></p></li>
<li><p><strong>Is sufficient for performance evaluation</strong>, i.e., <span class="math display">\[ \EXP[ C_t | I_t = i_t, U_t = u_t ] = 
   \EXP[ C_t | Z_t = F_t(i_t), U_t = u_t ]. \]</span></p></li>
</ol>
<p>In certain models, instead of 1, it is easier to establish the following weaker conditions:</p>
<ol type="a">
<li><p><strong>Evolves in a state-like manner</strong>, i.e., there exists a function <span class="math inline">\(H_t\)</span> such that <span class="math display">\[ Z_{t+1} = H_t(Z_t, Y_{t+1}, U_t),\]</span> or, more precisely, for any realization of the system variables, <span class="math display">\[ F_{t+1}(y_{1:t+1}, u_{1:t}) = H_t( F_t(y_{1:t}, u_{1:t-1}), y_{t+1},
u_t). \]</span></p></li>
<li><p><strong>Is sufficient for predicting future observations</strong>, i.e., <span class="math display">\[ \PR(Y_{t+1} = y_{t+1} | I_t = i_t, U_t = u_t)
 = \PR(Y_{t+1} = y_{t+1} | Z_t = F_t(i_t), U_t = u_t). \]</span></p></li>
</ol>
<p>An information state is useful because it is sufficient for dynamic programming.</p>
<div class="highlight">
<dl>
<dt>Theorem</dt>
<dd><p>Let <span class="math inline">\(\{Z_t\}_{t \ge 1}\)</span> be an information state of the system. Recursively define value function <span class="math inline">\(\{ \tilde V_t \}_{t \ge 1}\)</span>, where <span class="math inline">\(\tilde V_t \colon Z_t \mapsto \reals\)</span> as follows: <a name="eq:DPI-1"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[ \tilde V_{T+1}(z_{T+1}) = 0 \]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(7)</span></span>  and for <span class="math inline">\(t \in \{T, \dots, 1\}\)</span>: <a name="eq:DPI-Q"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[ \tilde Q_t(z_t, u_t) = \EXP\big[ C_t + \EXP[ \tilde V_{t+1}(Z_{t+1}) \bigm| Z_t = z_t,
U_t = u_t \big]\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(8)</span></span>  and define <a name="eq:DPI-2"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[\tilde V_t(z_t) = \min_{u_t \in \ALPHABET U} \tilde Q_t(z_t, u)  \]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(9)</span></span>  and <a name="eq:DP-policy"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[ \tilde g^*_t(z_t) = \arg\min_{u_t \in \ALPHABET U} \tilde Q_t(z_t, u_t). \]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(10)</span></span> </p>
<p>Then,</p>
<ul>
<li><span class="math inline">\(Q_t(i_t, u_t) = \tilde Q_t(F_t(i_t), u_t)\)</span>.<br />
</li>
<li><span class="math inline">\(V_t(i_t) = \tilde V_t(F_t(i_t))\)</span>.</li>
<li><span class="math inline">\(g^*(i_t) = \tilde g^*(F_t(i_t))\)</span>.</li>
</ul>
</dd>
</dl>
</div>
<h4 id="proof">Proof</h4>
<p>As usual, we proceed by backward induction. At <span class="math inline">\(t = T+1\)</span>, both value functions are equal. This forms the basis of induction. Suppose that at time <span class="math inline">\(t+1\)</span>, <span class="math inline">\(V_{t+1} = \tilde V_{t+1} \circ F_{t+1}\)</span>. Now consider the system at time <span class="math inline">\(t\)</span>. <span class="math display">\[ \begin{align}
Q_t(i_t, u_t) &amp;= \EXP\big[ C_t + V_{t+1}(I_{t+1}) \bigm| I_t = i_t, U_t = u_t
\big]
\\
&amp;\stackrel{(a)}= \EXP\big[ C_t + \tilde V_{t+1}(Z_{t+1}) \bigm| I_t = i_t, U_t
= u_t \big] \\
&amp;\stackrel{(b)}= \EXP\big[ C_t + \tilde V_{t+1}(Z_{t+1}) \bigm| Z_t =
F_t(i_t), U_t = u_t ] \\
&amp;\stackrel{(c)}= \tilde Q( F_t(i_t), u_t).
\end{align} \]</span> where <span class="math inline">\((a)\)</span> follows from the induction hypothesis, <span class="math inline">\((b)\)</span> follows from the two properties of information state, and <span class="math inline">\((c)\)</span> follows from the definition of <span class="math inline">\(\tilde Q\)</span>. This shows that the <span class="math inline">\(Q\)</span>-functions are equal. Minimizing over the actions, we get that the value functions and the optimal policy are also the same.</p>
<!--
## Examples of information state

1. **Even MDP**: Under what conditions is $V_t(x) = V_t(|x|)$? Reverse
   engineer the conditions. $Z_t = |X_t|$. 

   $$\PR( |X_{t+1}| = y | X_t = x, U_t = u)
   = \PR( |X_{t+1}| = y | |X_t| = |x|, U_t = u). $$

   For $x, y > 0$, 
   $$P_{xy}(u) + P_{x(-y)}(u) = P_{(-x)y}(u) + P_{(-x)(-y)}(u) $$
   and
   $$ c(x,u) = c(|x|, u). $$

   Sufficient condition: $P_{xy}(u) = P_{(-x)(-y)}(u)$ for all $x,y$ and
   $c(x,u)$ is even. 

   Folded MDP

   $\tilde P_{xy}(u) = P_{xy}(u) + P_{x(-y)}(u)$. 


-->
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>We could have assume a simpler model with <span class="math display">\[ (Y_t, C_t) = f_t(X_1, U_{1:t-1}, W_{1:t-1}), \]</span> but we have assumed the slightly more elaborate model to be consistent with the standard model of MDP discussed earlier where we assumed that the cost is a function of the current control action.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Note that we are assuming that the cost process <span class="math inline">\(\{C_t\}_{t \ge 1}\)</span> is not observed. If the cost were observed (as is assumed in reinforcement learning for POMDPs), it should be included in <span class="math inline">\(I_t\)</span> and all the subsequent definitions need to be adapted accordingly.<a href="#fnref2" class="footnote-back">↩</a></p></li>
</ol>
</section>


<p class="categories">
This entry was last updated on
04 Apr 2018

and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/pomdp">
  POMDP</a> 
and tagged 

<a href="https://adityam.github.io/stochastic-control/tags/information-state">information state</a>.
 

</p>



      <script type="text/javascript">
      
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-6887174-4']);
            _gaq.push(['_trackPageview']);
            (function() {
                var ga   = document.createElement('script');
                ga.type  = 'text/javascript';
                ga.async = true;
                ga.src   = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
              })();
      
      </script>
    </div>
  </body>
</html>


