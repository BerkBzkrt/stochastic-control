<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" />
  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/github.css" type="text/css" />
  <script src="https://adityam.github.io/stochastic-control//js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <script type="text/javascript"
    src="https://cdn.plot.ly/plotly-1.2.0.min.js">
  </script>
  <script type="text/javascript"
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_SVG">
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      "HTML-CSS": { 
          fonts: ["TeX"]
      }, 
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
    },
      TeX : {
          Macros : {
            PR: "\\mathbb{P}",
            EXP: "\\mathbb{E}",
            IND: "\\mathbb{I}",
            reals: "\\mathbb{R}",
            integers: "\\mathbb{Z}",
            TRANS: "\\intercal",
            VEC: "\\operatorname{vec}",
            TR:  "\\operatorname{Tr}",
            // mathcal: "\\mathscr",
            ALPHABET: ["\\mathcal{#1}", 1],
            MATRIX: ["\\begin{bmatrix} #1 \\end{bmatrix}", 1],
          }
      }
    });
  </script>
</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2018
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<h1>Markov decision processes</h1>

<p>Markov decision processes (MDP) model model the simplest stochastic control architecture. The dynamic behavior of an MDP is modeled by an equation of the form <span class="math display">\[ X_{t+1} = f_t(X_t, U_t, W_t) \]</span> where <span class="math inline">\(X_t \in \ALPHABET X\)</span> is the state, <span class="math inline">\(U_t \in \ALPHABET U\)</span> is the control input, and <span class="math inline">\(W_t \in \ALPHABET W\)</span> is the noise. An agent/controller observes the state and chooses the control input <span class="math inline">\(U_t\)</span>.</p>
<p>The controller can be as sophisticated as we want. In principle, it can analyze all the entire history of observations and control actions to choose the current control action. Thus, the control action can be written as <span class="math display">\[ U_t = g_t(X_{1:t}, U_{1:t-1}),\]</span> where <span class="math inline">\(X_{1:t}\)</span> is a shorthand for <span class="math inline">\((X_1, \dots, X_t)\)</span> and a similar interpretation holds for <span class="math inline">\(U_{1:t-1})\)</span>. The function <span class="math inline">\(g_t\)</span> is called the <em>control law</em> at time <span class="math inline">\(t\)</span>.</p>
<p>At each time, the system incurs a cost that may depend on the current state and control action. This cost is denoted by <span class="math inline">\(c_t(X_t, U_t)\)</span>. The system operates for a time horizon <span class="math inline">\(T\)</span>. During this time, it incurs a total cost <span class="math display">\[ \sum_{t=1}^T c_t(X_t, U_t). \]</span></p>
<p>The initial state <span class="math inline">\(X_1\)</span> and the noise process <span class="math inline">\(\{W_t\}_{t \ge 1}\)</span> are random variables defined on a common probability space (these are called <em>primitive random variables</em>) and are mutually independent.</p>
<p>Suppose we have to design such a controller. We are told the probability distribution of the initial state and the noise. We are also told the system update functions <span class="math inline">\((f_1, \dots, f_T)\)</span> and the cost functions <span class="math inline">\((c_1, \dots, c_T)\)</span>. We are asked to choose a <em>control strategy</em> <span class="math inline">\(g = (g_1, \dots, g_T)\)</span> to minimize the expected total cost <span class="math display">\[ J(g) := \EXP\bigg[ \sum_{t=1}^T c_t(X_t, U_t) \bigg]. \]</span> How should we proceed?</p>
<p>At first glance, the problem looks intimidating. It appears that we have to design a very sophisticated controller; one that can analyzes all past data to choose a control input. However, this is not the case. A remarkable result is that even the optimal control station can discard all past data and choose the control input based only on the current state of the system. Formally, we have the following:</p>
<div class="highlight">
<dl>
<dt>Theorem</dt>
<dd><p><strong>(Optimality of Markov strategies)</strong> For the system model described above, there is no loss of optimality in chosing the control action according to <span class="math display">\[ U_t = g_t(X_t), \quad t=1, \dots, T.\]</span> Such a control strategy is called a <em>Markov strategy</em>.</p>
</dd>
</dl>
</div>
<p>The above result claims that the cost incurred by the best Markovian strategy is the same as the cost incurred by the best history dependent strategy. This appears to be a tall claim, so lets see how we can prove it. The main idea of the proof is to repeatedly apply <a href="../../theory/principle-of-irrelevant-information">Blackwell’s principle of irrelevant information</a> <span class="citation" data-cites="Blackwell1964">(<a href="#ref-Blackwell1964">1964</a>)</span></p>
<div class="proclaim">
<p><strong>Two-Step Lemma.</strong> Consider an MDP that operates for two steps (<span class="math inline">\(T=2\)</span>). Then there is no loss of optimality in restricting attention to a Markov control strategy at time <span class="math inline">\(t=2\)</span>.</p>
</div>
<p>Note that <span class="math inline">\(g_1\)</span> is Markov because it can only depend <span class="math inline">\(X_1\)</span>.</p>
<h4 id="proof">Proof</h4>
<p>Fix <span class="math inline">\(g_1\)</span> and look at the problem of optimizing <span class="math inline">\(g_2\)</span>. The total cost is <span class="math display">\[ \EXP[ c_1(X_1, g_1(X_1)) + c_2(X_2, g_2(X_{1:2}, U_1)) ]\]</span> The choice of <span class="math inline">\(g_2\)</span> does not influence the first term. So, for a fixed <span class="math inline">\(g_1\)</span>, minimizing the total cost is the equivalent to minimizing the second term. Now, from Blackwell’s principle of irrelevant information, there exists a <span class="math inline">\(g_2^* \colon X_2 \mapsto U_2\)</span> such that for any <span class="math inline">\(g_2\)</span> <span class="math display">\[\EXP[c_2(X_2, g_2^*(X_2) ] \le \EXP[c_2(X_2, g_2(X_{1:2}, U_2) ].\]</span></p>
<div class="proclaim">
<p><strong>Three-Step Lemma.</strong> Consider an MDP that operates for three steps (<span class="math inline">\(T=3\)</span>). Assume that the control law <span class="math inline">\(g_3\)</span> at time <span class="math inline">\(t=3\)</span> is Markov, i.e., <span class="math inline">\(U_3 = g_3(X_3)\)</span>. Then, there is no loss of optimality in restricting attention to Markov control law at time <span class="math inline">\(t=2\)</span>.</p>
</div>
<h4 id="proof-1">Proof</h4>
<p>Fix <span class="math inline">\(g_1\)</span> and <span class="math inline">\(g_3\)</span> and look at optimizing <span class="math inline">\(g_2\)</span>. The total cost is <span class="math display">\[ \EXP[ c_1(X_1, g_1(X_1)) + c_2(X_2, g_2(X_{1:2}, U_1)) + c_3(X_3, g_3(X_3)].\]</span></p>
<p>The choice of <span class="math inline">\(g_2\)</span> does not affect the first term. So, for a fixed <span class="math inline">\(g_1\)</span> and <span class="math inline">\(g_3\)</span>, minimizing the total cost is the same as minimizing the last two terms. Let us look at the last term carefully. Bu the law of iterated expectations, we have <span class="math display">\[ \EXP[ c_3(X_3, g_3(X_3) ] = \EXP[ \EXP[ c_3(X_3, g_3(X_3)) | X_2, U_2 ] ]. \]</span> Now, <span class="math display">\[\begin{align}
  \EXP[ c_3(X_3, g_3(X_3)) | X_2 = x_2, U_2 = u_2 ] &amp;= 
  \sum_{x_3 \in \ALPHABET X} c_3(x_3, g_3(x_3)) \\
  &amp;= \PR( w_2 \in \ALPHABET W : f_2(x_2, u_2, w_2) = x_3 )
  \\
  &amp;=: h_2(x_2, u_2).
\end{align}\]</span> The key point is that <span class="math inline">\(h_2(x_2, u_2)\)</span> does not depend on <span class="math inline">\(g_1\)</span> or <span class="math inline">\(g_2\)</span>.</p>
<p>Thus, the total expected cost affected by the choice of <span class="math inline">\(g_2\)</span> can be written as <span class="math display">\[\begin{align}
  \EXP[ c_2(X_2, U_2) + c_3(X_3, U_3) ] &amp;= \EXP[ c_2(X_2, U_2) + h_2(X_2, U_2)
  ] \\
  &amp;=: \EXP[ \tilde c_2(X_2, U_2) ].
\end{align}\]</span> Now, by Blackwell’s principle of irrelevant information, there exists a <span class="math inline">\(g_2^* : X_2 \mapsto U_2\)</span> such that for any <span class="math inline">\(g_2\)</span>, we have <span class="math display">\[ \EXP[ \tilde c_2(X_2, g_2^*(X_2))] \le  \EXP[ \tilde c_2(X_2, g_2(X_{1:2},
U_1) ].\]</span></p>
<hr />
<p>Now we have enough background to present the proof of optimality of Markov strategies.</p>
<h4 id="proof-of-optimality-of-markov-strategies">Proof of optimality of Markov strategies</h4>
<p>The main idea is that any system can be thought of as a two- or three-step system by aggregating time. Suppose that the system operates for <span class="math inline">\(T\)</span> steps. It can be thought of as a two-step system where <span class="math inline">\(t \in \{1, \dots, T - 1\}\)</span> corresponds to step 1 and <span class="math inline">\(t = T\)</span> corresponds to step 2. From the two-step lemma, there is no loss of optimality in restricting attention to Markov control law at step 2 (i.e., at time <span class="math inline">\(t=T\)</span>), i.e., <span class="math display">\[ U_T = g_T(X_T). \]</span></p>
<p>Now consider a system where we are using a Markov strategy at time <span class="math inline">\(t=T\)</span>. This system can be thought of as a three-step system where <span class="math inline">\(t \in \{1, \dots, T-2\}\)</span> corresponds to step 1, <span class="math inline">\(t = T-1\)</span> corresponds to step 2, and <span class="math inline">\(t=T\)</span> corresponds to step 3. Since the controller at time <span class="math inline">\(T\)</span> is Markov, the assumption of the three step lemma is satisfied. Thus, by that lemma, there is no loss of optimality in restricting attention to Markov controllers at step 2 (i.e., at time <span class="math inline">\(t=T-1\)</span>), i.e., <span class="math display">\[U_{T-1} = g_{T-1}(X_{T-1}).\]</span></p>
<p>Now consider a system where we are using a Markov strategy at time <span class="math inline">\(t \in \{T-1, T\}\)</span>. This can be thought of as a three-step system where <span class="math inline">\(t \in \{1, \dots, T - 3\}\)</span> correspond to step 1, <span class="math inline">\(t = T-2\)</span> correspond to step 2, and <span class="math inline">\(t \in \{T-1, T\}\)</span> correspond to step 3. Since the controllers at time <span class="math inline">\(t \in \{T-1, T\}\)</span> are Markov, the assumption of the three-step lemma is satisfied. Thus, by that lemma, there is no loss of optimality in restricting attention to Markov controllers at step 2 (i.e., at time <span class="math inline">\(t=T-2\)</span>), i.e., <span class="math display">\[U_{T-2} = g_{T-2}(X_{T-2}).\]</span></p>
<p>Proceeding this way, we continue to think of the system as a three step system by different relabeling of time. Once we have shown that the controllers at times <span class="math inline">\(t \in \{s+1, s+2, \dots, T\}\)</span> are Markov, we relabel time as follows: <span class="math inline">\(t=\{1, \dots, s-1\}\)</span> corresponds to step 1, <span class="math inline">\(t = s\)</span> corresponds to step 2, and <span class="math inline">\(t \in \{s+1, \dots, T\}\)</span> corresponds to step 3. Since the controllers at time <span class="math inline">\(t \in \{s+1, \dots, T\}\)</span> are Markov, the assumption of the three-step lemma is satisfied. Thus, by that lemma, there is no loss of optimality in restricting attention to Markov controllers at stage 2 (i.e. at time <span class="math inline">\(s\)</span>), i.e., <span class="math display">\[U_s = g_s(X_s).\]</span></p>
<p>Proceeding until <span class="math inline">\(s=2\)</span>, completes the proof.</p>
<h2 id="performance-of-markov-strategies">Performance of Markov strategies</h2>
<p>We have shown that there is no loss of optimality to restrict attention to Markov strategies. One of the advantages of Markov strategies is that it is easy to recursively compute their performance. In particular, given any Markov strategy <span class="math inline">\(g = (g_1, \dots, g_T)\)</span>, define <em>the cost-to-go functions</em> as follows: <span class="math display">\[J_t(x; g) = \EXP^g \bigg[ \sum_{s = t}^{T} c_s(X_s, g_s(X_s)) \biggm| X_t =
x\bigg]. \]</span> Note that <span class="math inline">\(J_t(x; g)\)</span> only depends on the future strategy <span class="math inline">\((g_t, \dots, g_T)\)</span>. These functions can be computed recursively as follows: <span class="math display">\[\begin{align}
  J_t(x; g) &amp;= \EXP^g \bigg[ \sum_{s = t}^{T} c_s(X_s, g_s(X_s)) \biggm| X_t =
  x \bigg] \\
  &amp;= \EXP^g \bigg[ c_t(x, g_t(x)) + \EXP^g \bigg[ \sum_{s = t+1}^T
    c_s(X_s, g_s(X_s)) \biggm| X_{t+1} \bigg] \biggm| X_t = x \bigg]
  \\
  &amp;= \EXP^g\big[ c_t(x, g_t(x)) + J_{t+1}(X_{t+1}; g) \big| X_t = x \big].
\end{align}\]</span></p>
<h2 id="dynamic-programming-decomposition">Dynamic Programming Decomposition</h2>
<p>Now we are ready to state the main result of MDPs</p>
<div class="highlight">
<dl>
<dt>Theorem</dt>
<dd><p><strong>(Dynamic program)</strong> Recursive define <em>value functions</em> <span class="math inline">\(\{V_t\}_{t = 1}^{T+1} \colon \ALPHABET X \to \reals\)</span> as follows: <a name="eq:DP-1"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[ V_{T+1}(x) = 0 \]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(1)</span></span>  and for <span class="math inline">\(t \in \{T, \dots, 1\}\)</span>: <a name="eq:DP-2"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[\begin{align}
   Q_t(x,u) &amp;= c(x,u) + \EXP[ V_{t+1}(X_{t+1}) | X_t = x, U_t = u] \\
   &amp;= c(x,u) + \EXP[ V_{t+1}(f_t(x,u,W_t)) ],
\end{align}\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(2)</span></span>  and define <a name="eq:DP-3"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[V_t(x) = \min_{u \in \ALPHABET U} Q_t(x,u), \]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(3)</span></span>  and <a name="eq:verification"></a><span style="display: inline-block; position: relative; width: 100%"><span class="math display">\[ g_t^*(x) = \arg \min_{u \in \ALPHABET U} Q_t(x,u).\]</span><span style="position: absolute; right: 0em; top: 50%; line-height:0; text-align: right">(4)</span></span> </p>
<p>Then, a Markov policy is optimal if and only if it satisfies (<a href="#eq:verification">4</a>).</p>
</dd>
</dl>
</div>
<p>Instead of proving the above result, we prove a related result</p>
<div class="highlight">
<dl>
<dt>Theorem</dt>
<dd><strong>(The comparison principle)</strong> For any Markov strategy <span class="math inline">\(g\)</span> <span class="math display">\[ J_t(x; g) \ge V_t(x) \]</span> with equality at <span class="math inline">\(t\)</span> if and only if the <em>future strategy</em> <span class="math inline">\(g_{t:T}\)</span> satisfies the verification step (<a href="#eq:verification">4</a>).
</dd>
</dl>
</div>
<p>Note that the comparison principle immediately implies that the strategy obtained using dynamic programming is optimal.</p>
<p>The comparison principle also allows us to interpret the value functions. The value function at time <span class="math inline">\(t\)</span> is the minimum of all the cost-to-go functions over all future strategies. The comparison principle also allows us to interpret the optimal policy (the interpretation is due to Bellman and is colloquially called Bellman’s principle of optimality).</p>
<div class="proclaim">
<p><strong>Bellman’s principle of optimality.</strong> An optimal policy has the property that whatever the initial state and the initial decisions are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.</p>
</div>
<h4 id="proof-of-the-comparison-principle">Proof of the comparison principle</h4>
<p>The proof proceeds by backward induction. Consider any Markov strategy <span class="math inline">\(g = (g_1, \dots, g_T)\)</span>. For <span class="math inline">\(t = T\)</span>, <span class="math display">\[\begin{align}
  V_T(x) &amp;= \min_{u \in \ALPHABET U} Q_T(x,u) \\
  &amp;\stackrel{(a)}= \min_{u \in \ALPHABET U} c_T(x,u) \\
  &amp;\stackrel{(b)}\le c_T(x, g_T(x)) \\
  &amp;\stackrel{(c)}= J_T(x; g),
\end{align}\]</span> where <span class="math inline">\((a)\)</span> follows from the definition of <span class="math inline">\(Q_T\)</span>, <span class="math inline">\((b)\)</span> follows from the definition of minimization, and <span class="math inline">\((c)\)</span> follows from the definition of <span class="math inline">\(J_T\)</span>. Equality holds in <span class="math inline">\((b)\)</span> iff the policy <span class="math inline">\(g_T\)</span> is optimal. This result forms the basis of induction.</p>
<p>Now assume that the statement of the theorem is true for <span class="math inline">\(t+1\)</span>. Then, for <span class="math inline">\(t\)</span> <span class="math display">\[\begin{align}
  V_t(x) &amp;= \min_{u \in \ALPHABET U} Q_t(x,u) \\
  &amp;\stackrel{(a)}= \min_{u \in \ALPHABET U} \Big\{
  c_t(x,u) + \EXP[ V_{t+1}(X_{t+1}) | X_t = x, U_t = u] 
  \Big\}
  \\
  &amp;\stackrel{(b)}\le  \Big\{
  c_t(x,g_t(x)) + \EXP[ V_{t+1}(X_{t+1}) | X_t = x, U_t = g_t(x)] 
  \Big\} \\
  &amp;\stackrel{(c)}\le  \Big\{
  c_t(x,g_t(x)) + \EXP[ J_{t+1}(X_{t+1}; g) | X_t = x, U_t = g_t(x)] 
  \Big\} \\
  &amp;\stackrel{(d)}= J_t(x; g),
\end{align}\]</span> where <span class="math inline">\((a)\)</span> follows from the definition of <span class="math inline">\(Q_t\)</span>, <span class="math inline">\((b)\)</span> follows from the definition of minimization, <span class="math inline">\((c)\)</span> follows from the induction hypothesis, and <span class="math inline">\((d)\)</span> follows from the definition of <span class="math inline">\(J_t\)</span>. We have equality in step <span class="math inline">\((b)\)</span> iff <span class="math inline">\(g_t\)</span> satisfies the verification step (<a href="#eq:verification">4</a>) and have equality in step <span class="math inline">\((c)\)</span> iff <span class="math inline">\(g_{t+1:T}\)</span> is optimal (this is part of the induction hypothesis). Thus, the result is true for time <span class="math inline">\(t\)</span> and, by the principle of induction, is true for all time.</p>
<h1 id="variations-of-a-theme">Variations of a theme</h1>
<h2 id="cost-depends-on-next-state">Cost depends on next state</h2>
<h2 id="exponential-cost-function">Exponential cost function</h2>
<h2 id="multiplicative-cost">Multiplicative cost</h2>
<h2 id="discounted-cost">Discounted cost</h2>
<h2 id="optimal-stopping">Optimal stopping</h2>
<p>Let <span class="math inline">\(\{X_t\}_{t \ge 1}\)</span> be a Markov chain. At each time <span class="math inline">\(t\)</span>, a decision maker observes the state <span class="math inline">\(X_t\)</span> of the Markov chain and decides whether to continue or stop the process. If the decision maker decides to continue, he incurs a <em>continuation cost</em> <span class="math inline">\(c_t(X_t)\)</span> and the state evolves. If the DM decides to stop, he incurs a <em>stopping cost</em> of <span class="math inline">\(s_t(X_t)\)</span> and the problem is terminated. The objective is to determine an optimal <em>stopping time</em> <span class="math inline">\(\tau\)</span> to minimize <span class="math display">\[J(\tau) := \EXP\bigg[ \sum_{t=1}^{\tau-1} c_t(X_t) + s_\tau(X_\tau)
\bigg].\]</span></p>
<p>Such problems are called <em>Optimal stopping problems</em>.</p>
<p>Define the <em>cost-to-go function</em> of any stopping rule as <span class="math display">\[J_t(x; \tau) = \EXP\bigg[ \sum_{s = t}^{\tau - 1} c_{\tau}(X_t) +
s_\tau(X_\tau) \,\bigg|\, \tau &gt; t \bigg]\]</span> and the <em>value function</em> as <span class="math display">\[V_t(x) = \inf_{\tau} J_t(x; \tau). \]</span> Then, it can be shown that the value functions satisfy the following recursion:</p>
<div class="highlight">
<p><strong>Dynamic Program for optimal stopping</strong> <span class="math display">\[ \begin{align}
V_T(x) &amp;= s_T(x) \\
V_t(x) &amp;= \min\{ s_t(x), c_t(x) + \EXP[ V_{t+1}(X_{t+1}) | X_t = x].
\end{align}\]</span></p>
</div>
<p>For more details on the optimal stopping problems, see <span class="citation" data-cites="Ferguson:book">Ferguson (<a href="#ref-Ferguson:book">2008</a>)</span>.</p>
<h2 id="minimax-setup">Minimax setup</h2>
<h2 id="references">References</h2>
<p>The proof idea for the optimality of Markov strategies is based on a proof by <span class="citation" data-cites="Witsenhausen1979">Witsenhausen (<a href="#ref-Witsenhausen1979">1979</a>)</span> on the structure of optimal coding strategies for real-time communication. Note that the proof does not require us to find a dynamic programming decomposition of the problem. This is in contrast with the standard textbook proof where the optimality of Markov strategies is proves as part of the dynamic programming decomposition.</p>
<hr />
<div id="refs" class="references">
<div id="ref-Blackwell1964">
<p><span class="smallcaps">Blackwell, D.</span> 1964. Memoryless strategies in finite-stage dynamic programming. <em>The Annals of Mathematical Statistics</em> <em>35</em>, 2, 863–865. DOI: <a href="https://doi.org/doi:10.1214/aoms/1177703586">doi:10.1214/aoms/1177703586</a>.</p>
</div>
<div id="ref-Ferguson:book">
<p><span class="smallcaps">Ferguson, T.S.</span> 2008. Optimal stopping and applications.. Available at: <a href="http://www.math.ucla.edu/~tom/Stopping/Contents.html" class="uri">http://www.math.ucla.edu/~tom/Stopping/Contents.html</a>.</p>
</div>
<div id="ref-Witsenhausen1979">
<p><span class="smallcaps">Witsenhausen, H.S.</span> 1979. On the structure of real-time source coders. <em>Bell System Technical Journal</em> <em>58</em>, 6, 1437–1451.</p>
</div>
</div>


<p class="categories">
This entry was last updated on
08 Feb 2018

and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/mdp">
  MDP</a> 
and tagged 

<a href="https://adityam.github.io/stochastic-control/tags/markov-strategies">markov strategies</a>,
<a href="https://adityam.github.io/stochastic-control/tags/dynamic-programming">dynamic programming</a>,
<a href="https://adityam.github.io/stochastic-control/tags/comparison-principle">comparison principle</a>,
<a href="https://adityam.github.io/stochastic-control/tags/principle-of-irrelevant-information">principle of irrelevant information</a>.
 

</p>



      <script type="text/javascript">
      
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-6887174-4']);
            _gaq.push(['_trackPageview']);
            (function() {
                var ga   = document.createElement('script');
                ga.type  = 'text/javascript';
                ga.async = true;
                ga.src   = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
              })();
      
      </script>
    </div>
  </body>
</html>


