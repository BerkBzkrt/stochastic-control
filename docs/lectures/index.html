<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
          src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_SVG,https://adityam.github.io/stochastic-control/js/mathjax-local.js">
  </script>
</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2020
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Lectures</div>

<p>Whenever possible, I will post notes on some of the material covered in class, but that is not guaranteed. This is a graduate class and you are responsible for taking notes in class and reading the appropriate chapters of the textbooks.</p>
<p>The notes will be updated as we move along in the course. Please check the dates on the first page to keep track. If you find any typos/mistakes in the notes, please let me know. <a href="https://github.com/adityam/stochastic-control/tree/master/content">Pull requests welcome</a>.</p>
<dl>
<dt>Tue, Jan 7</dt>
<dd><p><strong>Introduction and course overview</strong>.</p>
<ul>
<li>Reading: Kumar and Varaiya (Ch 1, 2); Bertsekas (Ch 1).</li>
<li><a href="../stochastic/stochastic-optimization">stochastic optimization</a></li>
<li><a href="../assignments/01">Assignment 1</a> posted.</li>
</ul>
</dd>
<dt>Thu, Jan 9</dt>
<dd><p><strong>Introduction to Markov decision processes (MDPs)</strong></p>
<ul>
<li><a href="../stochastic/newsvendor">Newsvendor problem</a></li>
<li>Introduction to <a href="../mdp/mdp-functional">MDP theory</a></li>
</ul>
</dd>
<dt>Tue, Jan 14</dt>
<dd><p><strong>Examples of MDPs</strong></p>
<ul>
<li><a href="../assignments/01">Assignment 1</a> due.</li>
<li><a href="../assignments/02">Assignment 2</a> posted.</li>
<li>Example of <a href="../mdp/optimal-gambling">optimal gambling</a></li>
<li>Example of <a href="../mdp/inventory-management">inventory management</a></li>
</ul>
</dd>
<dt>Thu, Jan 16</dt>
<dd><p><strong>Proof of the main results for MDPs</strong></p>
<ul>
<li>See notes on <a href="../mdp/mdp-functional">MDP theory</a></li>
<li><a href="../mdp/mdp-matrix">Matrix formulation</a> of MDPs</li>
</ul>
</dd>
<dt>Tue, Jan 21</dt>
<dd><p><strong>Variations of MDPs</strong></p>
<ul>
<li><a href="../assignments/02">Assignment 2</a> due.</li>
<li><a href="../assignment/03">Assignment 3</a> posted.</li>
<li>Class cancelled</li>
</ul>
</dd>
<dt>Thu, Jan 23</dt>
<dd><p><strong>Proof of optimality of dynamic programming</strong></p>
<ul>
<li>See notes on <a href="../mdp/mdp-functional">MDP theory</a></li>
</ul>
</dd>
<dt>Tue, Jan 28</dt>
<dd><p><strong>Monotonicty in Markov decision processes</strong></p>
<ul>
<li><a href="../assignment/03">Assignment 3</a> due.</li>
<li><a href="../mdp/stochastic-dominance">Stochastic dominance, monotonicity</a>, and submodularity</li>
<li><a href="../mdp/monotonicity">Sufficient conditions for value function and optimal policy to be monotone</a>.</li>
</ul>
</dd>
<dt>Thu, Jan 30</dt>
<dd><p><strong>Example of monotone policies</strong></p>
<ul>
<li><a href="../mdp/power-delay-tradeoff">power-delay trade-off</a> in wireless communication</li>
</ul>
</dd>
<dt>Tue, Feb 4</dt>
<dd><p><strong>Optimal stopping</strong></p>
<ul>
<li>Monotonicity of <a href="../optimal-stopping/monotonicity-optimal-stopping">optimal stopping policies</a></li>
<li>Examples of <a href="../optimal-stopping/call-options">call options</a> and <a href="../optimal-stopping/optimal-choice">optimal choice</a></li>
</ul>
</dd>
<dt>Thu, Feb 6</dt>
<dd><p><strong>Infinite horizon discounted cost problems</strong></p>
<ul>
<li><a href="../mdp/reward-shaping">Reward shaping</a></li>
<li>Infinite horizon <a href="../inf-mdp/inventory-management">inventory management</a></li>
</ul>
</dd>
<dt>Tue, Feb 11</dt>
<dd><p><strong>Properties of the Bellman operator</strong></p>
<ul>
<li>Notes on <a href="../inf-mdp/discounted-mdp">infinite horizon MDPs</a></li>
</ul>
</dd>
<dt>Thu, Feb 13</dt>
<dd><p><strong>Value and policy iteration algorithms</strong></p>
<ul>
<li>Notes on <a href="../inf-mdp/discounted-mdp">infinite horizon MDPs</a></li>
</ul>
</dd>
<dt>Tue, Feb 18</dt>
<dd><p><strong>Lipschitz continuity of value function</strong></p>
<ul>
<li><a href="../inf-mdp/lipschitz-mdp">Lipschitz continuity of MDPs</a></li>
</ul>
</dd>
<dt>Thu, Feb 20 <br /> Tue, Feb 25</dt>
<dd><p><strong>State aggregation</strong></p>
<ul>
<li>State quantization (see notes on <a href="../inf-mdp/state-aggregation">state aggregation</a>)</li>
<li>State compression (see notes on <a href="../inf-mdp/state-aggregation">state aggregation</a>)</li>
</ul>
</dd>
<dt>Tue, March 31 <br /> Thu, Apr 2</dt>
<dd><p><strong>POMDPs</strong></p>
<ul>
<li>Basic model of POMDPs (see notes on <a href="../pomdp/pomdp">POMDPs</a>)</li>
<li>Notion of information state</li>
<li><a href="../pomdp/sequential-hypothesis.pdc">Sequential hypothesis testing</a></li>
</ul>
</dd>
</dl>
<p><br />
</p>
<!--

:   * Infinite horizon discounted cost problems
    * Bellman operator, contraction mappings, and Banach fixed point Theorem.

Tue, Feb 11

:   * Bellman operators and their properties. 
    * Introduction to value iteration algorithm.

Thu, Feb 13

:   * Value iteration, policy iteration, and linear programming for discounted
MDPs

Tue, Feb 18

:   * Approximate MDP

Thu, Feb 20

:   * Average cost MDPs

Tue, Feb 25

:   * Relative value iteration and policy iteration
    * Partially observed models, information state

Thu, Feb 27

:   * LQR with exchangeable components

-->


<p class="categories">
This entry 

 was last updated on 31 Mar 2020</p>



      <script type="text/javascript">
      
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-6887174-4']);
            _gaq.push(['_trackPageview']);
            (function() {
                var ga   = document.createElement('script');
                ga.type  = 'text/javascript';
                ga.async = true;
                ga.src   = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
              })();
      
      </script>
    </div>
  </body>
</html>


