<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="Linear systems,Adaptive control,LQR" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://cdn.plot.ly/plotly-basic-latest.min.js">
  </script><script type="text/javascript"
    src="https://cdn.jsdelivr.net/npm/mathjs@7.0.1/dist/math.min.js">
  </script>
  <script src="https://adityam.github.io/stochastic-control//js/random-0.26.js"></script><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2022
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Overview of adaptive control for linear systems</div>

<p>Adaptive control is a umbrella term which is often used to describe algorithms which are able to control an unknown system (i.e., a system with unknown dynamics). In this section, we focus on adaptive control of linear systems. The simplest setup is as follows.</p>
<p>Consider a standard linear quadratic regulator where the dynamics are <span class="math display">\[ 
  x_{t+1} = A_θ x_t + B_θ u_t + w_t
\]</span> where the matrices <span class="math inline">\(A_θ\)</span> and <span class="math inline">\(B_θ\)</span> are parameterized by a parameter <span class="math inline">\(θ\)</span> which takes values in a set <span class="math inline">\(Θ\)</span>. The per-step cost is <span class="math inline">\(c(x_t, u_t) = x_t^\TRANS Q x_t + u_t^\TRANS R u_t\)</span>. For simplicity, we assume that the <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span> matrices are known but all we know about the <span class="math inline">\(A_θ\)</span> and <span class="math inline">\(B_θ\)</span> matrices is that <span class="math inline">\(θ\)</span> lies in the set <span class="math inline">\(Θ\)</span>. The objective is the design a controller which will minimize the long term average cost <span class="math display">\[
  \lim_{T \to ∞} \frac{1}{T}
  \EXP\Bigl[
    \sum_{t=1}^T \bigl[ x_t^\TRANS Q x_t + u_t^\TRANS R u_t \bigr]
  \Bigr].
\]</span> Had we known <span class="math inline">\(A_θ\)</span> and <span class="math inline">\(B_θ\)</span>, we know that the optimal controller is given by <span class="math display">\[ u_t = - L_θ x_t \]</span> where <span class="math inline">\(L_θ = [R + B_θ^\TRANS S_θ B_θ]^{-1} B_θ^\TRANS S_θ A_θ\)</span> and <span class="math inline">\(S_θ\)</span> is the solution to the discrete-time algebric Riccati equation: <span class="math display">\[
  S_θ = A_θ^\TRANS S_θ A_θ + Q -
  A_θ^\TRANS S_θ^\TRANS B_θ[ R + B_θ^\TRANS S_θ B_θ]^{-1}
  B_θ^\TRANS S_θ A_θ
\]</span> and the corresponding optimal performance is <span class="math inline">\(J_θ = \TR(S_θΣ)\)</span>.</p>
<p>But what do we do if the system model is unknown? In this section, we overview the basic ideas for adaptive control (or reinforcement learning).</p>
<h1 data-number="1" id="certainty-equivalence"><span class="header-section-number">1</span> Certainty equivalence</h1>
<p>The simplest idea in adaptive control is <strong>certainty equivalence</strong>: at each time generate an estimate <span class="math inline">\(\hat θ_t\)</span> the unknown parameters <span class="math inline">\(θ\)</span> of the model based on the observed data and use the controller <span class="math display">\[
  u_t = -L_{\hat θ_t} x_t.
\]</span></p>
<p>The idea is that if <span class="math inline">\(\hat θ_t\)</span> converges to <span class="math inline">\(θ\)</span> (or more weakly <span class="math inline">\(L_{\hat θ_t}\)</span> converges to <span class="math inline">\(L_θ\)</span>, then the performance of the certainty equivalence controller will converge to the optimal performance.</p>
<h2 data-number="1.1" id="a-simple-example"><span class="header-section-number">1.1</span> A simple example</h2>
<p>We present a simple example to illustrate that it is possible to control a linear system with unknown dymamics. Consider a scalar linear system <span class="math display">\[ x_{t+1} = a x_t + b u_t + w_t \]</span> where <span class="math inline">\(x_t, u_t, w_t \in \reals\)</span> and <span class="math inline">\(\{w_t\}_{t \ge 1}\)</span> is an i.i.d. process with zero mean and variance <span class="math inline">\(σ^2\)</span>. Suppse the per-step cost is <span class="math inline">\(c(x_t, u_t) = x_t^2\)</span> and our objective is to minimize the long-term average cost <span class="math display">\[ 
  \lim_{T \to ∞} \frac{1}{T} \EXP\Bigl[ \sum_{t=1}^T x_t^2 \Bigr].
\]</span> It is easy to verify that the solution of the discrete-time algebric Riccati equation is given by <span class="math inline">\(S = 1\)</span> and therefore the optimal control action is <span class="math display">\[
  u_t = - \frac{a}{b} x_t
\]</span> and the optimal average cost is <span class="math inline">\(σ^2\)</span>.</p>
<p>Now suppose the parameters <span class="math inline">\(θ = \VEC(a,b)\)</span> are unknown. We assume that <span class="math inline">\(θ \in \reals^2\)</span>. Suppose we use linear least squares estimates to identify <span class="math inline">\(θ\)</span>. Let <span class="math inline">\(z_t\)</span> denote <span class="math inline">\(\VEC(x_t, u_t)\)</span>. Note that we can write <span class="math display">\[ x_{t+1} = z_t^\TRANS θ + w_t. \]</span></p>
<p>Combining oberrvations until time <span class="math inline">\(t\)</span>, we have <span class="math display">\[ \MATRIX{x_2 \\ \vdots \\ x_{t}} = 
   \MATRIX{z_1^\TRANS \\ \vdots \\ z_{t-1}^\TRANS } θ + 
   \MATRIX{w_1 \\ \vdots \\ w_{t-1}} \]</span> which may be written in vector form as <span class="math display">\[ X_t = Z_t θ + W_t, \]</span> where <span class="math display">\[ 
 X_t = \MATRIX{x_2 \\ \vdots \\ x_{t}}, \quad
 Z_t = \MATRIX{z_1^\TRANS \\ \vdots \\ z_{t-1}^\TRANS }, \quad
 W_t = \MATRIX{w_1 \\ \vdots \\ w_{t-1}}. \]</span></p>
<p>Thus, the linear least squares estimator of <span class="math inline">\(θ\)</span> is given by <span class="math display">\[ \hat θ_t = (Z_t^\TRANS Z_t)^{-1} Z_t^\TRANS X_t
    = \bigl[ \sum_{τ = 1}^{t-1} z_τ z_τ^\TRANS \bigr]^{-1}
              \sum_{τ=1}^{t-1} z_τ x_{τ+1}. \]</span></p>
<p>The linear least squares estimate can be written in recursive form as: <span class="math inline">\(\hat θ_0 = \VEC(0, 1)\)</span>, <span class="math inline">\(Σ_0 = I\)</span>, and for <span class="math inline">\(t \ge 0\)</span>: <span class="math display">\[\begin{align*}
  \hat θ_{t+1} &amp;= \hat θ_t + 
  \frac{ Σ_t z_t ( x_{t+1} - z_t^\TRANS \hat θ_t) }{σ^2 + z_t^\TRANS Σ_t z_t },
  \\
  Σ_{t+1} &amp;= Σ_t - 
  \frac{ Σ_t z_t z_t^\TRANS Σ_t }{ σ^2 + z_t^\TRANS Σ_t z_t }.
\end{align*}\]</span></p>
<p>Note that it can also be shown that the covariance <span class="math inline">\(Σ_t\)</span> satisfies the recusion <span class="math display">\[ 
  Σ_{t+1}^{-1} = Σ_t^{-1} + \frac{1}{σ^2} z_t z_t^\TRANS.
\]</span></p>
<p>Now, let <span class="math inline">\(\hat θ_t = (\hat a_t, \hat b_t)\)</span>. The certainty equivalent control law is given by <span class="math display">\[
  u_t = - \frac{\hat a_t}{\hat b_t} x_t.
\]</span></p>
<p>It can be shown that under some conditions on the parameters of the model, the sequence <span class="math inline">\(\{ \hat a_t/ \hat b_t\}_{t \ge 1}\)</span> converges to <span class="math inline">\(a/b\)</span>, so asymptotically we will use the right control law, even though <span class="math inline">\(\hat θ_t\)</span> does not converge to <span class="math inline">\(θ\)</span>.</p>
<p>Let’s run a simple simulation to test this. Suppose $a = $ <input id="ex1a"
       type="number"    step = 0.1
       maxlength = "3"  size = "2"
       min = "-10"      max  = "10"
       onchange  = "ex1Plot()"
       value = "" />, $b = $ <input id="ex1b"
       type="number"    step = 0.1
       maxlength = "3"  size = "2"
       min = "-10"      max  = "10"
       onchange  = "ex1Plot()"
       value = "" />, and $σ = $ <input id="ex1σ"
       type="number"    step = 0.1
       maxlength = "3"  size = "2"
       min = "0"      max  = "10"
       onchange  = "ex1Plot()"
       value = "" />. The resulting plot for a single sample path is shown below. You can generate another sample path by clicking <button onClick="ex1Plot()">Re-run</button></p>
<div class="flex">
<div id="ex1:cost" style="width:800px; height:250px;">

</div>
<div id="ex1:parameters" style="width:800px; height:250px;">

</div>
</div>
<script>
parser1 = math.parser()

if (pagedata == undefined) { var pagedata = { } }

pagedata.adaptiveControl = { a : 0.8, b : 1, σ : 1, T : 500 }

// Set initial values
document.getElementById('ex1a').value = pagedata.adaptiveControl.a;
document.getElementById('ex1b').value = pagedata.adaptiveControl.b;
document.getElementById('ex1σ').value = pagedata.adaptiveControl.σ;

function show_plot(dom, x, y, xtitle, ytitle) {
  var data = [{
      x : x,
      y : y,
      line : {
        shape : "spline",
      }, 
      mode : "lines" , 
  }];

  var layout = { 
      paper_bgcolor: "#fffff8",
      plot_bgcolor:  "#fffff8",
      xaxis : {
        title: xtitle,
        showgrid: true,
        showline: true,
        zeroline: false,
        showticklabels: true,
      },
      yaxis : {
        title: ytitle,
        showgrid: true,
        showline: true,
        zeroline: false,
        showticklabels: true,
      },
      automargin: true,
      margin: {
        l : 75,
        r : 15,
        b : 75,
        t : 15,
        pad : 15
      } ,
      hovermode: false,
  };

  var options = {
      staticPlot: true,
      displayModeBar: false,
  };

  Plotly.newPlot(dom, data, layout, options);
}

function ex1Plot() { 
  var T = pagedata.adaptiveControl.T ; 

  var stream = new Random();

  var a = document.getElementById('ex1a').valueAsNumber;
  var b = document.getElementById('ex1b').valueAsNumber;
  var σ = document.getElementById('ex1σ').valueAsNumber;

  parser1.set('a' , a)
  parser1.set('b' , b)
  parser1.set('σ' , σ)
  
  var time = Array(T);
  var cost = Array(T);
  var avg  = Array(T);

  var hat_a = Array(T);
  var hat_b = Array(T); 

  var gain  = Array(T);

  var x = 0;

  parser1.evaluate('x = 0; u = 0; w = 0')
  parser1.evaluate('z = zeros(2); θ = [0, 1]; Σ = identity(2)')


    for (var t = 0; t < T; t++) {

      x = parser1.get('x')
      
      time[t] = t;
      if(t == 0) {
        cost[t] = x*x;
      } else {
        cost[t] = cost[t-1] + x*x;
      }
      avg[t] = cost[t]/(t+1);

      gain[t] = parser1.evaluate('θ[1]/θ[2]')

      parser1.evaluate('u = -θ[1]/θ[2] * x')
      parser1.set('w', stream.normal(0, σ))

      parser1.evaluate('z = [x, u]')
      parser1.evaluate('x = a*x + b*u + w')
      parser1.evaluate("normalize = σ^2 + z'*Σ*z")

      parser1.evaluate("θ = θ + Σ*z*(x - z'*θ)/normalize")
      parser1.evaluate("Σ = Σ - Σ*z*z'*Σ/normalize")

    }

  show_plot('ex1:cost', time, avg, "Time", "Average cost");
  show_plot('ex1:parameters', time, gain, "Time", "Controller gain");
}

(async () => { await ex1Plot() })();

</script>
<h2 data-number="1.2" id="certainty-equivalence-can-fail"><span class="header-section-number">1.2</span> Certainty equivalence can fail</h2>
<p>Now, we present a simple example to show that certainty equivalence can fail. Again, consider the simple scalar example from before <span class="math display">\[ x_{t+1} = a x_t + b u_t + w_t \]</span> where <span class="math inline">\(x_t, u_t, w_t \in \reals\)</span> and <span class="math inline">\(\{w_t\}_{t \ge 1}\)</span> is an i.i.d. process with zero mean and variance <span class="math inline">\(σ^2\)</span>. The per-step cost is <span class="math inline">\(c(x_t, u_t) = x_t^2 + 2 u_t^2\)</span>. So, our objective is to minimize the long-term average cost <span class="math display">\[
  \lim_{T \to ∞} \frac{1}{T} \EXP\Bigl[ \sum_{t=1}^T x_t^2 + 2 u_t^2 \Bigr].
\]</span></p>
<p>Now suppose that <span class="math inline">\(θ = (a,b)\)</span> are unknown but we know that <span class="math inline">\(θ\)</span> lies in the set <span class="math inline">\(Θ = \{ (0, -1), (1, 1) \}\)</span>. When <span class="math inline">\((a,b) = (0, -1)\)</span>, <span class="math inline">\(S = 1\)</span> and the optimal control law is <span class="math inline">\(u_t = 0\)</span>. When <span class="math inline">\((a,b) = (1,1)\)</span>, <span class="math inline">\(S = 2\)</span> and the optimal control law is <span class="math inline">\(u_t = -\tfrac 12 x_t)\)</span>. Thus, the optimal control law is <span class="math display">\[
  u_t = \begin{cases} 
    0, &amp; \text{if } (a, b) = (0, -1), \\
    -\tfrac12 x_t, &amp; \text{if } (a, b) = (1, 1).
  \end{cases}
\]</span></p>
<p>Suppose, as before, we use a certainty equivalence control law where we estimate the parameters <span class="math inline">\(θ\)</span> using least-squares (or equivalently, in this case, a maximum-likelihood) estimate based on the observed data up to time <span class="math inline">\(t\)</span>. The least-squares estimate is <span class="math display">\[
  \hat θ_t = \begin{cases}
    (0, -1), &amp; \text{if } \sum_{τ=1}^{t-1} (x_{τ+1} + u_τ)^2 
    \le \sum_{τ=1}^{t-1}(x_{τ+1} - x_τ - u_τ)^2, \\
    (1, 1), &amp; \text{otherwise}.
  \end{cases}
\]</span></p>
<p>Now suppose at some point of time, the estimate is <span class="math inline">\((1,1)\)</span>. Which means that <span class="math display">\[
    \sum_{τ=1}^{t-1} (x_{τ+1} + u_τ)^2 
    &gt; \sum_{τ=1}^{t-1}(x_{τ+1} - x_τ - u_τ)^2, 
\]</span> and by using the certainty equivalence controller, we chose <span class="math inline">\(u_t = -\tfrac12 x_t\)</span>. This implies that <span class="math display">\[ (x_{t+1} + u_t)^2 = (x_{t+1} - x_t - u_t)^2. \]</span> Thus, at time <span class="math inline">\(t+1\)</span>, we will have <span class="math display">\[
    \sum_{τ=1}^{t} (x_{τ+1} + u_τ)^2 
    &gt; \sum_{τ=1}^{t}(x_{τ+1} - x_τ - u_τ)^2 
\]</span> and, therefore, the estimate <span class="math inline">\(\hat θ_{t+1} = (1,1)\)</span>. Repeating the above argument at time <span class="math inline">\(t+1\)</span>, we get that if at <em>any</em> time the parameters <span class="math inline">\(θ\)</span> are estimated to be <span class="math inline">\((1,1)\)</span>, then the parameter estimates will thereafter remain <em>unchanged</em> and the adaptive control law will “stick” at <span class="math inline">\(u_τ = -\tfrac12 x_τ\)</span> for all <span class="math inline">\(τ \ge t\)</span>. This is clearly undesirable if the true value of the parameters is <span class="math inline">\((0, -1)\)</span>.</p>
<p>To see that this can indeed happen with positive probability, suppose that <span class="math inline">\((a,b) = (0, -1)\)</span> is indeed the true system and we start initially with <span class="math inline">\(x_1 = 1\)</span> and <span class="math inline">\(u_1 = 0\)</span>. Then, <span class="math display">\[ \PR( (x_2 + u_1)^2 &gt; (x_2 - x_1 - u_1)^2 ) =
   \PR( w_1^2 &gt; (w_1 - 1)^2 ) = \PR(w_1 &gt; \tfrac12).
\]</span> Since <span class="math inline">\(w_1 \sim {\cal N}(0,1)\)</span>, the event <span class="math inline">\(\{ w_1 &gt; \tfrac12\}\)</span> occurs with probability <span class="math inline">\(0.31\)</span> and the adaptive control law will “stick” with probability <em>at least</em> 0.31 at the <em>non-optimal</em> (cost = 2 versus cost = 1) control law.</p>
<p>To circumvent “sticking” to the bad controller, we need to make sure that a condition known as “persistent excitation” is satisfied, which ensures that there is sufficient exploration. More on that later.</p>
<h1 data-number="2" id="thompson-sampling"><span class="header-section-number">2</span> Thompson sampling</h1>
<p>One of the oldest and simplest algorithms to ensure sufficient exploration is Thompson sampling, which is Bayesian approach that relies on a very simple idea. Maintain a posterior belief <span class="math inline">\(π_t\)</span> over the parameter <span class="math inline">\(θ\)</span>. At time <span class="math inline">\(t\)</span>, sample a value <span class="math inline">\(\tilde θ_t\)</span> from the posterior <span class="math inline">\(π_t\)</span> and choose the control law <span class="math display">\[ 
  u_t = - L_{\tilde θ_t} x_t.
\]</span> The intuition behind the approach is that if we take enough samples, <span class="math inline">\(π_t\)</span> will converge to a dirac delta distribution centered at the true <span class="math inline">\(θ\)</span> and therefore <span class="math inline">\(θ_t\)</span> will be the same as <span class="math inline">\(θ\)</span>.</p>
<h2 data-number="2.1" id="a-simple-example-1"><span class="header-section-number">2.1</span> A simple example</h2>
<p>We present a simple example to illustrate how Thompson sampling works. As before, let’s consider a scalar system <span class="math display">\[ x_{t+1} = a x_t + b u_t + w_t \]</span> where <span class="math inline">\(x_t, u_t, w_t \in \reals\)</span> and <span class="math inline">\(\{w_t\}_{t \ge 1}\)</span> is an i.i.d. process with zero mean and variance <span class="math inline">\(σ^2\)</span>. The per-step cost is <span class="math inline">\(c(x_t, u_t) = x_t^2\)</span> and our objective is to minimize the long-term average cost <span class="math display">\[
  \lim_{T \to ∞} \frac{1}{T} \EXP\Bigl[ \sum_{t=1}^T x_t^2 \Bigr].
\]</span> As we have seen before, if we knew the model parameters, the optimal control law in this case is <span class="math display">\[ u_t = -\frac{a}{b} x_t. \]</span></p>
<p>Now suppose we had an initial prior <span class="math inline">\(π_0 \sim {\cal N}(θ_0, Σ_0)\)</span> over <span class="math inline">\(θ\)</span>. where <span class="math inline">\(θ_0 = \VEC(0,1)\)</span> and <span class="math inline">\(Σ_0 = I\)</span>. Let <span class="math inline">\(z_t\)</span> denote <span class="math inline">\(\VEC(x_t, u_t)\)</span>. As before, we can write <span class="math display">\[ x_{t+1} = z_t^\TRANS θ + w_t. \]</span> Thus, at each time we get a noisy observation of the unknown parameter <span class="math inline">\(θ\)</span> though the “observation channel” <span class="math inline">\(z_t^\TRANS\)</span>. This is the standard estimation problem with Gaussian observations. From standard results in Kalman filtering, we know that the posterior <span class="math inline">\(π_t(dθ) = \PR( θ \in dθ | z_{1:t})\)</span> is Gaussian, say <span class="math inline">\({\cal N}(θ_t, Σ_t)\)</span>, where <span class="math inline">\((θ_t, Σ_t)\)</span> satisfy the recursion</p>
<p><span class="math display">\[\begin{align*}
  θ_{t+1} &amp;= θ_t + \frac{ Σ_t z_t (x_{t+1} - z_t^\TRANS θ_t) }
  { σ^2 + z_t^\TRANS Σ_t z_t } \\
  Σ_{t+1} &amp;= Σ_t - \frac{ Σ_t z_t z_t^\TRANS Σ_t }{ σ^2 + z_t^\TRANS Σ_t z_t }.
\end{align*}\]</span></p>
<p>Note that the update above is identical to the update for the recursive least squares estimates. Then, at time <span class="math inline">\(t\)</span>, we sample <span class="math display">\[
  \tilde θ_t \sim {\cal N}(θ_t, Σ_t).
\]</span> Now, let <span class="math inline">\(\tilde θ_t = (\tilde a_t, \tilde b_t)\)</span>. Then, the Thompson sampling control law is given by <span class="math display">\[
  u_t = - \frac{\tilde a_t}{\tilde b_t} x_t.
\]</span> It can be shown that under some conditions on the parameters of the model, the posterior <span class="math inline">\(θ_t\)</span> converges to a <span class="math inline">\(θ^*\)</span> such that <span class="math inline">\(L_{θ^*} = L_{θ}\)</span> and the covariance <span class="math inline">\(Σ_t\)</span> converges to <span class="math inline">\(0\)</span>.</p>
<p>Let’s run a simple simulation to test this. Suppose $a = $ <input id="ex2a"
       type="number"    step = 0.1
       maxlength = "3"  size = "2"
       min = "-10"      max  = "10"
       onchange  = "ex2Plot()"
       value = "" />, $b = $ <input id="ex2b"
       type="number"    step = 0.1
       maxlength = "3"  size = "2"
       min = "-10"      max  = "10"
       onchange  = "ex2Plot()"
       value = "" />, and $σ = $ <input id="ex2σ"
       type="number"    step = 0.1
       maxlength = "3"  size = "2"
       min = "0"      max  = "10"
       onchange  = "ex2Plot()"
       value = "" />. The resulting plot for a single sample path is shown below. You can generate another sample path by clicking <button onClick="ex2Plot()">Re-run</button></p>
<div class="flex">
<div id="ex2:cost" style="width:800px; height:250px;">

</div>
<div id="ex2:parameters" style="width:800px; height:250px;">

</div>
</div>
<script>
parser2 = math.parser()

if (pagedata == undefined) { var pagedata = { } }

pagedata.thompsonSampling = { a : 0.8, b : 1, σ : 1, T : 500 }

// Set initial values
document.getElementById('ex2a').value = pagedata.thompsonSampling.a;
document.getElementById('ex2b').value = pagedata.thompsonSampling.b;
document.getElementById('ex2σ').value = pagedata.thompsonSampling.σ;

function ex2Plot() { 
  var T = pagedata.thompsonSampling.T ; 

  var stream = new Random();
  var thompson = new Random();

  var a = document.getElementById('ex2a').valueAsNumber;
  var b = document.getElementById('ex2b').valueAsNumber;
  var σ = document.getElementById('ex2σ').valueAsNumber;

  parser2.set('a' , a)
  parser2.set('b' , b)
  parser2.set('σ' , σ)
  
  var time = Array(T);
  var cost = Array(T);
  var avg  = Array(T);

  var hat_a = Array(T);
  var hat_b = Array(T); 

  var gain  = Array(T);

  var x = 0;

  parser2.evaluate('x = 0; u = 0; w = 0')
  parser2.evaluate('z = zeros(2); θ = [0, 1]; Σ = identity(2)')


    for (var t = 0; t < T; t++) {

      x = parser2.get('x')
      
      time[t] = t;
      if(t == 0) {
        cost[t] = x*x;
      } else {
        cost[t] = cost[t-1] + x*x;
      }
      avg[t] = cost[t]/(t+1);

      parser2.set('N1', thompson.normal(0,1))
      parser2.set('N2', thompson.normal(0,1))
      parser2.evaluate('W = [N1, N2]')
      parser2.evaluate('E = eigs(Σ)')
      parser2.evaluate('Λ = [sqrt(E.values[1]), 0; 0, sqrt(E.values[2])]')
      parser2.evaluate('V = E.vectors')
      parser2.evaluate("θ_hat = θ + W'*Λ*V")

      gain[t] = parser2.evaluate('θ_hat[1]/θ_hat[2]')

      parser2.evaluate('u = -θ_hat[1]/θ_hat[2] * x')
      parser2.set('w', stream.normal(0, σ))

      parser2.evaluate('z = [x, u]')
      parser2.evaluate('x = a*x + b*u + w')
      parser2.evaluate("normalize = σ^2 + z'*Σ*z")

      parser2.evaluate("θ = θ + Σ*z*(x - z'*θ)/normalize")
      parser2.evaluate("Σ = Σ - Σ*z*z'*Σ/normalize")

    }

  show_plot('ex2:cost', time, avg, "Time", "Average cost");
  show_plot('ex2:parameters', time, gain, "Time", "Controller gain");
}

(async () => { await ex2Plot() })();

</script>
<h1 class="unnumbered" id="references">References</h1>
<p>The material for <a href="#certainty-equivalence-can-fail">the example that certainty equivalence can fail</a> is taken from <span class="citation" data-cites="Kumar1983">Kumar (<a href="#ref-Kumar1983" role="doc-biblioref">1983</a>)</span>.</p>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Kumar1983" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Kumar, P.R.</span> 1983. Optimal adaptive control of linear-quadratic-gaussian systems. <em><span>SIAM</span> Journal on Control and Optimization</em> <em>21</em>, 2, 163–178. DOI: <a href="https://doi.org/10.1137/0321009">10.1137/0321009</a>.
</div>
</div>


<p class="categories">
This entry 

 was last updated on 01 Aug 2021
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/rl">
  RL</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/linear-systems">linear systems</a>,
<a href="https://adityam.github.io/stochastic-control/tags/adaptive-control">adaptive control</a>,
<a href="https://adityam.github.io/stochastic-control/tags/lqr">lqr</a>.</p>



    </div>
  </body>
</html>


