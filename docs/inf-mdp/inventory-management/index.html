<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="inventory management,reward shaping,structural results" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2022
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Example: Inventory Management (revisited)</div>

<p>TL;DR <em>One of the potential benefits of modeling a system as infinite horizon discounted cost MDP is that it can be simpler to identify an optimal policy. We illustrate this using the inventory management example</em>.</p>
<p>Consider the model for <a href="../../mdp/inventory-management">inventory management</a> and assume that it runs for an infinite horizon. We assume that the per-step cost is given by <span class="math display">\[c(s,a,s_{+}) = p a + γ h(s), \]</span> where <span class="math display">\[ h(s) = \begin{cases}
  c_h s, &amp; \text{if $s \ge 0$} \\
  -c_s s, &amp; \text{if $s &lt; 0$},
\end{cases}\]</span> where <span class="math inline">\(a\)</span> is the per-unit holding cost, <span class="math inline">\(b\)</span> is the per-unit shortage cost, and <span class="math inline">\(p\)</span> is the per-unit procurement cost. Note that in the per-step cost, we are assuming that the holding or shortage cost is dicounted because this cost is incurred at the end of the time period.</p>
<p>Recall that in the finite horizon setting, the optimal policy was a <em>base-stock</em> policy characterized by thresholds <span class="math inline">\(\{s^*_t\}_{t \ge 1}\)</span>. In the infinite horizon discounted setting, we expect the optimal policy to be time-homogeneous, i.e., the thresholds <span class="math inline">\(\{s^*_t\}_{t \ge 1}\)</span> be a constant <span class="math inline">\(s^*\)</span> and not to depend on time.</p>
<p>As an illustration, let’s reconsider the example used for the <a href="../../mdp/inventory-management#fig1">finite horizon setting</a> (where <span class="math inline">\(p=5\)</span>, <span class="math inline">\(c_h = 4\)</span>, <span class="math inline">\(c_s = 2\)</span>, and the demand is Binomial(10,0.4)). We consider the discount factor <span class="math inline">\(γ = 0.9\)</span>. The value function and optimal policy is this case are shown below.</p>
<figure style='max-width:40em;'>
<div id="observablehq-viewof-discount-11b15b95">

</div>
<table>
<tr>
<td id="observablehq-inf_value_plot-11b15b95">
</td>
<td id="observablehq-inf_action_plot-11b15b95">
</td>
</tr>
<tr>
<td>
Value function
</td>
<td>
Optimal policy
</td>
</table>
<figcaption>
<b>Figure 1</b><br/> The value function and the optimal policy for the example
</figcaption>
</figure>
<script type="module">
import {Runtime, Inspector} from "https://cdn.jsdelivr.net/npm/@observablehq/runtime@4/dist/runtime.js";
import define from "https://api.observablehq.com/d/b2eb5b0d068edd4a.js?v=3";
new Runtime().module(define, name => {
  if (name === "inf_value_plot") return new Inspector(document.querySelector("#observablehq-inf_value_plot-11b15b95"));
  if (name === "inf_action_plot") return new Inspector(document.querySelector("#observablehq-inf_action_plot-11b15b95"));
  if (name === "viewof discount") return new Inspector(document.querySelector("#observablehq-viewof-discount-11b15b95"));
  return ["infDP"].includes(name);
});
</script>
<p>We are interested in the following question: <em>Is it possible to identify the optimal threshold of the base-stock policy without explicitly solving the dynamic program?</em> In this section, we show that the answer is affirmative.</p>
<p>As a first step, we modify the per-step cost using <a href="../../mdp/reward-shaping">reward shaping</a>. In particular, we consider the following potential function</p>
<p><span class="math display">\[\varphi(s) = h(s) + \frac1{γ} p s - \frac{1}{1-γ}p\mu,\]</span> where <span class="math inline">\(\mu = \EXP[W]\)</span> is the expected number of arrivals at each time period.</p>
<p>Now consider a new cost function <span class="math display">\[\begin{align*}
  c&#39;(s,a,s_{+}) &amp;= c(s,a,s_{+}) + \varphi(s) - γ \varphi(s_{+}) \\
  &amp;= pa + γ h(s_{+}) + h(s) + \frac{1}{γ} p s - \frac{1}{1-γ} p \mu
  - γ h(s_{+}) - p s_{+} - \frac{γ}{1-γ} p \mu \\
  &amp;= h(s) + \frac{1-γ}{γ} ps + p w - p \mu.
\end{align*} \]</span> Note that <span class="math display">\[ \EXP[ c&#39;(s,a,S_{+}) | S = s, A = a ] = h(s) + \frac{1-γ}{γ} ps 
=: c^*(s). \]</span> Thus, the optimal policy of the original model is the same as that in which the per-step cost is given by <span class="math inline">\(c^*(s)\)</span>.</p>
<p>Recall that the optimal policy in the original model was a control limit policy. For the infinite horizon model, the threshold will become time-invariant. Thus, the optimal policy will be of the form <span class="math display">\[
  π(s) = \begin{cases}
  s^* - s, &amp; \text{if $s \le s^*$} \\
  0, &amp; \text{otherwise}.
\end{cases}\]</span></p>
<p>The infinite horizon dynamic programming with this modified cost is given by <span class="math display">\[ V(s) = \min_{a \in \reals_{\ge 0}} 
   \bigl\{ c^*(s) + γ \EXP[ V(s + a - W) ] \bigr\}. \]</span></p>
<p>Using the structure of the optimal policy identified above, we have that <span class="math display">\[\begin{equation}\label{eq:opt}
V(s) = c^*(s) + γ \EXP[ V(s^* - W) ], \qquad s \le s^*.
\end{equation}\]</span> Let <span class="math inline">\(F(s^*)\)</span> denote <span class="math inline">\(\EXP[V(s^*-W)]\)</span>. Then, substituting <span class="math inline">\(s = s^* - W\)</span> in \eqref{eq:opt} and taking expectations, we get <span class="math display">\[F(s^*) = \EXP[ c^*(s^* - W) ] + γ F(s^*).\]</span> Thus, <span class="math display">\[ F(s^*) = \frac{1}{1-γ} \EXP[ c^*(s^*-W) ]. \]</span></p>
<p>Substituting the above in \eqref{eq:opt} for <span class="math inline">\(s = 0\)</span>, (note that <span class="math inline">\(c^*(0) = 0\)</span>) we get <span class="math display">\[ s^* = \arg\min_{s^* \ge 0} \frac{γ}{1-γ} \EXP[ c^*(s^* - W) ].\]</span> Consequently, we have the following:</p>
<div class="highlight">
<dl>
<dt><span id="thm:1"></span><span id="thm:inventory" class="pandoc-numbering-text thm"><strong>Theorem 1</strong></span></dt>
<dd>
<p>The optimal threshold <span class="math inline">\(s^*\)</span> is given by the value of <span class="math inline">\(s^*\)</span> which minimizes <span class="math inline">\(\EXP[ c^*(s^*-W) ]\)</span>.</p>
</dd>
</dl>
</div>
<p>An almost closed form characterization of the optimal policy may be obtained using the same approach as that used for the <a href="../../stochastic/newsvendor">newsvendor problem</a>. In particular, let <span class="math inline">\(f\)</span> denote the distribution of <span class="math inline">\(W\)</span>, <span class="math inline">\(F\)</span> denote the CDF, and <span class="math inline">\(μ = \EXP[W]\)</span>. Then, <span class="math display">\[\begin{align}
\EXP[c^*(s^*-W)] &amp;= \EXP[h(s^*-W)] + \frac{1-γ}{γ}p(s^* - μ) 
\notag \\
&amp;= \int_{0}^{s^*} c_s(w-s^*)f(w)dw + 
   \int_{s^*}^{∞} c_h(s^*-w)f(w)dw + \frac{1-γ}{γ}p(s^* - μ) \notag
\end{align}\]</span> Thereore, <span class="math display">\[\begin{align}
\frac{∂ \EXP[c^*(s^*-W)]}{∂s^*} &amp;=
  \int_{0}^{s^*} [-c_s] f(w)dw
+ \int_{s^*}^{\infty}c_h f(w)dw 
+ \frac{1-γ}{γ}p \notag \\
&amp;= -c_s F(s^*) + c_h(1 - F(s^*))
+ \frac{1-γ}{γ}p \notag \\
\end{align}\]</span> To find the optimal threshold, we set the derivative to <span class="math inline">\(0\)</span> and simplify: <span class="math display">\[
 s^* = F^{-1}\left( \frac{c_h + p(1-γ)/γ}{c_h + c_s} \right).
\]</span></p>
<hr />
<h1 class="unnumbered" id="exercises">Exercises</h1>
<ol type="1">
<li><p>Suppose that the arrival process is exponential with rate <span class="math inline">\(1/\mu\)</span>, i.e., the density of <span class="math inline">\(W\)</span> is given by <span class="math inline">\(e^{-s/\mu}/\mu\)</span>. Show that the optimal threshold is given by <span class="math display">\[ s^* = \mu \log \left[ \frac{ c_h + c_s} { c_h + p (1-γ)/γ} \right]. \]</span></p>
<p><em>Hint</em>: Recall that the CDF the exponential distribution is <span class="math inline">\(F(s) = 1 - e^{-s/μ}\)</span>. <!--
_Hint_: Show that for exponential distribution
$$ \EXP[ (s^*-W)^+ ] = (s^* - \mu) + \mu e^{-s^*/\mu} $$
and
$$ \EXP[ (s^*-W)^- ] = - \mu e^{-s^*/\mu}. $$
--></p></li>
</ol>
<hr />
<h1 class="unnumbered" id="references">References</h1>
<p>The idea of using reward shaping to derive a closed form expression for inventory management is taken from <span class="citation" data-cites="Whittle1982">Whittle (<a href="#ref-Whittle1982" role="doc-biblioref">1982</a>)</span>. It is interesting to note that <span class="citation" data-cites="Whittle1982">Whittle (<a href="#ref-Whittle1982" role="doc-biblioref">1982</a>)</span> uses the idea of reward shaping more than 17 years before the paper by <span class="citation" data-cites="Ng1999">Ng et al. (<a href="#ref-Ng1999" role="doc-biblioref">1999</a>)</span> on reward shaping.</p>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Ng1999" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Ng, A.Y., Harada, D., and Russell, S.</span> 1999. Policy invariance under reward transformations: Theory and application to reward shaping. <em>ICML</em>, 278–287. Available at: <a href="http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf">http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf</a>.
</div>
<div id="ref-Whittle1982" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Whittle, P.</span> 1982. <em>Optimization over time: Dynamic programming and stochastic control. Vol. 1 and 2</em>. Wiley.
</div>
</div>


<p class="categories">
This entry 

 was last updated on 26 Feb 2022
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/mdp">
  MDP</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/inventory-management">inventory management</a>,
<a href="https://adityam.github.io/stochastic-control/tags/reward-shaping">reward shaping</a>,
<a href="https://adityam.github.io/stochastic-control/tags/structural-results">structural results</a>.</p>



    </div>
  </body>
</html>


