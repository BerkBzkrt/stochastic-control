<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="infinite horizon,discounted cost,Bellman operator,value iteration,policy iteration" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2022
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Theory: Infinite horizon discounted MDP</div>

<!-- FIXME: Temp while the mathjax definition propogaes to server -->
<p><span class="math inline">\(\def\ONES{\mathbb{1}}\)</span></p>
<p>A common way to approximate systems that run for a very large horizon is to assume that they run for an infinite horizon. There is an inherent homogeneity over time for infinite horizon system: the future depends only on the current state and not on the current time. Due to this homogeneity over time, we expect that the optimal policy should also be time-homogeneous. Therefore, the optimal policy for an infinite-horizon system should be easier to implement than the optimal policy for a finite horizon system, especially so when the horizon is large. This is one of the motivations for studying infinite horizon models.</p>
<p>The most common formulation for infinite horizon models is the discounted setup, where the cost function is assumed to be <span class="math display">\[ J(π) = \EXP\Bigl[ \sum_{t=1}^\infty γ^{t-1} c_t(S_t, A_t) \Bigr] \]</span> where <span class="math inline">\(γ \in (0,1)\)</span> is called the discount factor.</p>
<p>There are two interpretations of the discount factor <span class="math inline">\(γ\)</span>. The first interpretation is an economic interpretation to determine the <em>present value</em> of a utility that will be received in the future. For example, suppose a decision maker is indifferent between receiving 1 dollar today or <span class="math inline">\(s\)</span> dollars tomorrow. This means that the decision maker discounts the future at a rate <span class="math inline">\(1/s\)</span>, so <span class="math inline">\(γ = 1/s\)</span>.</p>
<p>The second interpretation is that of an absorbing state. Suppose we are operating a machine that generates a value of $1 each day. However, there is a probability <span class="math inline">\(p\)</span> that the machine will break down at the end of the day. Thus, the expected return for today is $1 while the expected return for tomorrow is <span class="math inline">\((1-p)\)</span> (which is the probability that the machine is still working tomorrow). In this case, the discount factor is defined as <span class="math inline">\((1-p)\)</span>. See <span class="citation" data-cites="Shwartz2001">Shwartz (<a href="#ref-Shwartz2001" role="doc-biblioref">2001</a>)</span> for a detailed discussion of this alternative.</p>
<p>In the remainder of this section, we will study how to obtain a solution for such infinite horizon discounted cost models.</p>
<p><em>Note</em>: Throughout this section, we assume that <span class="math inline">\(\ALPHABET S\)</span> and <span class="math inline">\(\ALPHABET A\)</span> are finite and <span class="math inline">\(|\ALPHABET S|= n\)</span> and <span class="math inline">\(|\ALPHABET A| = m\)</span>.</p>
<h1 data-number="1" id="performance-of-a-time-homogeneous-markov-policy"><span class="header-section-number">1</span> Performance of a time-homogeneous Markov policy</h1>
<p>For any <span class="math inline">\(π \colon \ALPHABET S \to \ALPHABET A\)</span>, consider the time homogeneous policy <span class="math inline">\((π, π, \dots)\)</span>. For ease of notation, we denote this policy simply by <span class="math inline">\(π\)</span>. The expected discounted cost under this policy is given by <span class="math display">\[ V_π(s) = \EXP^π\bigg[ \sum_{t=1}^∞ γ^{t-1} c(S_t, A_t) \biggm| S_1 = s
\bigg].\]</span></p>
<p>To get a compact expression for this, define a <span class="math inline">\(n × 1\)</span> vector <span class="math inline">\(c_π\)</span> and a <span class="math inline">\(n × n\)</span> matrix <span class="math inline">\(P_π\)</span> as follows: <span class="math display">\[ [c_π]_s = c(s, π(s))
   \quad\text{and}\quad
   [P_π]_{ss&#39;} = P_{ss&#39;}(π(s)).
\]</span> Then the dynamics under policy <span class="math inline">\(π\)</span> are Markovian with transition probability matrix <span class="math inline">\(P_π\)</span> and a cost function <span class="math inline">\(c_π\)</span>. Then <span class="math display">\[ \begin{align*}
\EXP^π\big[ c(S_t, π(S_t)) \bigm| S_1 = s \big]
  &amp;= \sum_{s&#39; \in \ALPHABET S} \PR^π(S_t = s&#39; | S_1 = s) c(s&#39;, π(s&#39;))
  \\
  &amp;= \sum_{s&#39; \in \ALPHABET S} [P_π^{t-1}]_{ss&#39;} [c_π]_y
  \\
  &amp;= δ_s P_π^{t-1} c_π.
\end{align*} \]</span></p>
<p>Let <span class="math inline">\(V_π\)</span> denote the <span class="math inline">\(n × 1\)</span> vector given by <span class="math inline">\([V_π]_s = V_π(s)\)</span>. Then, <span class="math display">\[ \begin{align*}
V_π &amp;= c_π + γ P_π c_π + γ^2 P_π^2 c_π + \cdots \\
    &amp;= c_π + γ P_π \big( c_π + γ P_π c_π + \cdots \big) \\
    &amp;= c_π + γ P_π V_π,
\end{align*} \]</span> which can be rewritten as <span class="math display">\[ (I - γ P_π) V_π = c_π. \]</span></p>
<p>The <a href="https://en.wikipedia.org/wiki/Spectral_radius">spectral radius</a> <span class="math inline">\(ρ(γ P_d)\)</span> of a matrix is upper bounded by its <a href="https://en.wikipedia.org/wiki/Matrix_norm">spectral norm</a> <span class="math inline">\(\lVert γ P_d \rVert = γ &lt; 1\)</span>. Therefore, the matrix <span class="math inline">\((I - γ P_π)\)</span> has an inverse and by left multiplying both sides by <span class="math inline">\((I - γ P_π)^{-1}\)</span>, we get <span class="math display">\[ V_π = (I - γP_π)^{-1} c_π. \]</span></p>
<p>The equation <span class="math display">\[ V_π = c_π + γ P_π V_π \]</span> is sometimes also written as <span class="math display">\[ V_π = \mathcal B_π V_π \]</span> where the operator <span class="math inline">\(\mathcal B_π\)</span>, which is called the <em>Bellman operator</em>, is an operator from <span class="math inline">\(\reals^n\)</span> to <span class="math inline">\(\reals^n\)</span> given by <span class="math display">\[ \mathcal B_π v = c_π + γ P_π v.\]</span></p>
<h1 data-number="2" id="bellman-operators"><span class="header-section-number">2</span> Bellman operators</h1>
<dl>
<dt>Definition</dt>
<dd>
<p>Define the <em>Bellman operator</em> <span class="math inline">\(\mathcal B : \reals^n \to \reals^n\)</span> as follows: for any <span class="math inline">\(v \in \reals^n\)</span> <span class="math display">\[ [\mathcal B v]_s = \min_{a \in \ALPHABET A}
\Big\{ c(s,a) + γ \sum_{s&#39; \in \ALPHABET S} P_{ss&#39;}(a) v_y \Big\}.
\]</span></p>
</dd>
</dl>
<p>Note that the above may also be written as<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="math display">\[ \mathcal B v = \min_{π \in \Pi} \mathcal B_π v, \]</span> where <span class="math inline">\(\Pi\)</span> denotes the set of all deterministic Markov policies.</p>
<div class="highlight">
<dl>
<dt><span id="prop:1"></span><span id="prop:contraction" class="pandoc-numbering-text prop"><strong>Prop. 1</strong></span></dt>
<dd>
<p>For any <span class="math inline">\(v \in \reals^n\)</span>, define the norm <span class="math inline">\(\NORM{V} := \sup_{s \in \ALPHABET S} \ABS{V_s}\)</span>. Then, the Bellman operator is a contraction, i.e., for any <span class="math inline">\(v, w \in \reals^n\)</span>, <span class="math display">\[ \NORM{\mathcal B v - \mathcal B w} \le γ \NORM{v - w}. \]</span></p>
</dd>
</dl>
</div>
<details>
<summary>
<h4 class="unnumbered" id="proof">Proof</h4>
</summary>
<div>
<p>Fix a state <span class="math inline">\(s \in \ALPHABET S\)</span> and consider <span class="math inline">\([\mathcal B v](s) - [\mathcal B w](s)\)</span>. In particular, let <span class="math inline">\(a^*\)</span> be the optimal action in the right hand side of <span class="math inline">\([\mathcal B w](s)\)</span>. Then, <span class="math display">\[\begin{align*}
  [\mathcal B v - \mathcal B w](s) &amp;= 
  \min_{a \in \ALPHABET A}\bigl\{ c(s,a) + γ \sum_{s&#39; \in \ALPHABET S}
  P_{ss&#39;}(a) v(s&#39;) \bigr\} - 
  \min_{a \in \ALPHABET A}\bigl\{ c(s,a) + γ \sum_{s&#39; \in \ALPHABET S}
  P_{ss&#39;}(a) w(s&#39;) \bigr\} 
  \\
  &amp;\le c(s,a^*) + γ \sum_{s&#39;\in \ALPHABET S} P_{ss&#39;}(a^*) v(s&#39;) -
       c(s,a^*) - γ \sum_{s&#39;\in \ALPHABET S} P_{ss&#39;}(a^*) w(s&#39;)
  \\
  &amp;\le γ \sum_{s&#39; \in \ALPHABET S} P_{ss&#39;}(a^*) \| v - w \|
  \\
  &amp;= γ \| v - w \|.
\end{align*} \]</span></p>
<p>By a similar argument, we can show that <span class="math inline">\([\mathcal B w - \mathcal B v](s) \le γ \| v - w \|\)</span>, which proves the other side of the inequality.</p>
</div>
</details>
<p>An immediate consequence of the contraction property is the following.</p>
<div class="highlight">
<dl>
<dt><span id="thm:1"></span><span id="thm:fixed-point" class="pandoc-numbering-text thm"><strong>Theorem 1</strong></span></dt>
<dd>
<p>There is a unique bounded <span class="math inline">\(V^* \in \reals^n\)</span> that satisfies the <em>Bellman equation</em> <span class="math display">\[ V = \mathcal B V \]</span></p>
<p>Moreover, if we start from any <span class="math inline">\(V_0 \in \reals^n\)</span> and recursively define <span class="math display">\[ V_n = \mathcal B V_{n-1} \]</span> then <span class="math display">\[ \lim_{n \to ∞} V_n = V^*. \]</span></p>
</dd>
</dl>
</div>
<details>
<summary>
<h4 class="unnumbered" id="proof-1">Proof</h4>
</summary>
<div>
<p>This follows immediately from the <a href="https://proofwiki.org/wiki/Banach_Fixed-Point_Theorem">Banach fixed point theorem</a>.</p>
</div>
</details>
<h1 data-number="3" id="optimal-time-homogeneous-policy"><span class="header-section-number">3</span> Optimal time-homogeneous policy</h1>
<!--
::: highlight :::
Prop.

:   Let $μ$ be any (possibly randomized) history dependent policy. Then,      
    there exists a randomized Markov policy $π$ that has the same performance
    as $μ$.
:::

#### Proof {-}

For any policy $μ$, define the _occupation measure_
$$ \mu(s,a | s^∘; μ) 
  := \EXP^μ \bigg[ \sum_{t=1}^∞ γ^{t-1} \IND\{S_t = s, A_t = a\} 
  \biggm| S_1 = s^∘ \bigg]
$$
Then 
$$ J(s^∘; μ) = \sum_{s \in \ALPHABET S, a \in \ALPHABET A}
   μ(s,a | s^∘; μ) c(s,a).
$$ {#eq:performance}

Now, define a randomized Markov policy $π$ as follows:
$$\PR(A_t = a | S_t = s; π) = \frac{ μ(s, a| s^∘; μ) } { μ(s | s^∘; μ) }$$
where $μ(s|s^∘;μ) = \sum_{a \in \ALPHABET A} μ(s,a | s^∘; μ)$. Then, by
construction, 
$ μ(s,a | s^∘; π) = μ(s,a | s^∘; μ) $
and therefore by \\eqref{eq:performance}, we have that $J(s^∘; π) = J(s^∘; μ).$

-->
<div class="highlight">
<dl>
<dt><span id="prop:2"></span><span id="prop:optimal" class="pandoc-numbering-text prop"><strong>Prop. 2</strong></span></dt>
<dd>
<p>Define <span class="math display">\[ V^{opt}_∞(s) := \min_{π} \EXP^π \bigg[ \sum_{t=1}^∞ γ^{t-1} c(S_t, A_t) 
\biggm| S_1 = s \bigg], \]</span> where the minimum is over all (possibly randomized) history dependent policies. Then, <span class="math display">\[ V^{opt}_∞ = V^*, \]</span> where <span class="math inline">\(V^*\)</span> is the solution of the Bellman equation.</p>
</dd>
</dl>
</div>
<details>
<summary>
<h4 class="unnumbered" id="proof-2">Proof</h4>
</summary>
<div>
<p>Since the state and action space are finite, without loss of optimality, we can assume that <span class="math inline">\(0 \le c(s,a) \le M\)</span>.</p>
<p>Consider the finite horizon truncation <span class="math display">\[ V^{opt}_T(s) =  \min_{π} \EXP^π\bigg[ \sum_{t=1}^T γ^{t-1} c(S_t, A_t) | S_1 = s \bigg].
\]</span> From the results for finite horizon MDP, we have that <span class="math display">\[ V^{opt}_T = \mathcal B^T V_0 \]</span> where <span class="math inline">\(V_0\)</span> is the all zeros vector.</p>
<p>Now by construction, <span class="math display">\[V^{opt}_∞(s) \ge V^{opt}_T(s) = [\mathcal B^T V_0](s). \]</span> Taking limit as <span class="math inline">\(T \to ∞\)</span>, we get that <span class="math display">\[\begin{equation}\label{eq:1}
  V^{opt}_∞(s) \ge \lim_{T \to ∞} [\mathcal B^T V_0](s) = V^*(s). 
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(0 \le c(s,a) \le M\)</span>, for any <span class="math inline">\(T\)</span>, <span class="math display">\[ \begin{align*}
V^{opt}_∞(s) &amp;\le \min_π \EXP^π \bigg[ \sum_{t=1}^T γ^{t-1} c(S_t, A_t) 
\biggm| S_1 = s \bigg] + \sum_{t=T+1}^∞ γ^{t-1} M \\
&amp;= V^{opt}_T(s) + γ^T M / (1 - γ) \\
&amp;= [\mathcal B^T V_0](s) + γ^T M / (1-γ). 
\end{align*} \]</span> Taking limit as <span class="math inline">\(T \to ∞\)</span>, we get that <span class="math display">\[\begin{equation}\label{eq:2}
  V^{opt}_∞(s) \le \lim_{T \to ∞}
  \big\{ [\mathcal B^T V_0](s) + γ^T M / (1-γ) \big\} = V^*(s). 
\end{equation}\]</span></p>
<p>From \eqref{eq:1} and \eqref{eq:2}, we get that <span class="math inline">\(V^{opt}_∞ = V^*\)</span>.</p>
</div>
</details>
<h1 data-number="4" id="properties-of-bellman-operator"><span class="header-section-number">4</span> Properties of Bellman operator</h1>
<div class="highlight">
<dl>
<dt><span id="prop:3"></span><span id="prop:properties" class="pandoc-numbering-text prop"><strong>Prop. 3</strong></span></dt>
<dd>
<p>The Bellman operator satisfies the following properties</p>
<ul>
<li><em>Monotonicity</em>. For any <span class="math inline">\(v, w \in \reals^n\)</span>, if <span class="math inline">\(v \le w\)</span>, then <span class="math inline">\(\mathcal B_π v \le \mathcal B_π w\)</span> and <span class="math inline">\(\mathcal B v \le \mathcal B w\)</span>.</li>
<li><em>Discounting</em>. For any <span class="math inline">\(v \in \reals^n\)</span> and <span class="math inline">\(m \in \reals\)</span>, <span class="math inline">\(\mathcal B_π (v + m \ONES) = \mathcal B_π v + γ m \ONES\)</span> and <span class="math inline">\(\mathcal B (v + m \ONES) = \mathcal B v + γ m \ONES\)</span>.</li>
</ul>
</dd>
</dl>
</div>
<details>
<summary>
<h4 class="unnumbered" id="proof-of-monotonicity-property">Proof of monotonicity property</h4>
</summary>
<div>
<p>Recall that <span class="math display">\[ \mathcal B_π v = c_π + γ P_π v. \]</span> So, monotonicity of <span class="math inline">\(\mathcal B_π\)</span> follows immediately from monotonicity of matrix multiplication for positive matrices.</p>
<p>Let <span class="math inline">\(μ\)</span> be such that <span class="math inline">\(\mathcal B w = \mathcal B_μ w\)</span>. Then, <span class="math display">\[ \mathcal B v \le \mathcal B_μ v 
\stackrel{(a)} \le \mathcal B_μ w = \mathcal B w,
\]</span> where <span class="math inline">\((a)\)</span> uses the monotonicity of <span class="math inline">\(\mathcal B_μ\)</span>.</p>
</div>
</details>
<details>
<summary>
<h4 class="unnumbered" id="proof-of-discounting-property">Proof of discounting property</h4>
</summary>
<div>
<p>Recall that <span class="math display">\[ \mathcal B_π v = c_π + γ P_π v. \]</span> Thus, <span class="math display">\[ \mathcal B_π(v+m \ONES) = c_π + γ P_π (v+m \ONES) = c_π + γ P_π v + γ m
\ONES = \mathcal B_π
v + γ m \ONES.\]</span> Thus, <span class="math inline">\(\mathcal B_π\)</span> is discounting. Now consider <span class="math display">\[ \mathcal B (v + m \ONES ) = \min_{π} \mathcal B_π (v+m \ONES)
= \min_π \mathcal (B_π v + γ m \ONES) = \mathcal B v + γ m \ONES.\]</span> Thus, <span class="math inline">\(\mathcal B\)</span> is discounting.</p>
</div>
</details>
<div class="highlight">
<dl>
<dt><span id="prop:4"></span><span id="prop:coarse-bounds" class="pandoc-numbering-text prop"><strong>Prop. 4</strong></span></dt>
<dd>
<p>For any <span class="math inline">\(V \in \reals^n\)</span>,</p>
<ul>
<li>If <span class="math inline">\(V \ge \mathcal B V\)</span>, then <span class="math inline">\(V \ge V^*\)</span>;</li>
<li>If <span class="math inline">\(V \le \mathcal B V\)</span>, then <span class="math inline">\(V \le V^*\)</span>;</li>
<li>If <span class="math inline">\(V = \mathcal B V\)</span>, then <span class="math inline">\(V\)</span> is the only vector with this property and <span class="math inline">\(V = V^*\)</span>.</li>
</ul>
<p>The same bounds are true when <span class="math inline">\((\mathcal B, V^*)\)</span> is replaced with <span class="math inline">\((\mathcal B_π, V_π)\)</span>.</p>
</dd>
</dl>
</div>
<details>
<summary>
<h4 class="unnumbered" id="proof-3">Proof</h4>
</summary>
<div>
<p>We prove the first part. The proof of the other parts is similar.</p>
<p>We are given that <span class="math display">\[V \ge \mathcal B V.\]</span> Then, by monotonicity of the Bellman operator, <span class="math display">\[ \mathcal B V \ge \mathcal B^2 V.\]</span> Continuing this way, we get <span class="math display">\[ \mathcal B^k V \ge \mathcal B^{k+1} V.\]</span> Adding the above equations, we get <span class="math display">\[ V \ge \mathcal B^{k+1} V.\]</span> Taking limit as <span class="math inline">\(k \to ∞\)</span>, we get <span class="math display">\[V \ge V^*.\]</span></p>
</div>
</details>
<div class="highlight">
<dl>
<dt><span id="prop:5"></span><span id="prop:bounds" class="pandoc-numbering-text prop"><strong>Prop. 5</strong></span></dt>
<dd>
<p>For any <span class="math inline">\(V \in \reals^n\)</span> and <span class="math inline">\(m \in \reals\)</span>,</p>
<ul>
<li>If <span class="math inline">\(V + m \ONES \ge \mathcal B V\)</span>, then <span class="math inline">\(V + m \ONES/(1-γ) \ge V^*\)</span>;</li>
<li>If <span class="math inline">\(V + m \ONES \le \mathcal B V\)</span>, then <span class="math inline">\(V + m \ONES/(1-γ) \le V^*\)</span>;</li>
</ul>
<p>The same bounds are true when <span class="math inline">\((\mathcal B, V^*)\)</span> is replaced with <span class="math inline">\((\mathcal B_π, V_π)\)</span>.</p>
</dd>
</dl>
</div>
<dl>
<dt>Remark</dt>
<dd>
<p>The above result can also be stated as follows:</p>
<ul>
<li><span class="math inline">\(\displaystyle \| V_π - V \| \le \frac{1}{1-γ}\| \mathcal B_π V - V \|\)</span>.</li>
<li><span class="math inline">\(\displaystyle \| V^* - V \| \le \frac{1}{1-γ}\| \mathcal B V - V \|\)</span>.</li>
</ul>
</dd>
</dl>
<details>
<summary>
<h4 class="unnumbered" id="proof-4">Proof</h4>
</summary>
<div>
<p>Again, we only prove the first part. The proof of the second part is the same. We have that <span class="math display">\[ V + m \ONES \ge \mathcal B V. \]</span> From discounting and monotonicity properties, we get <span class="math display">\[ \mathcal B V + γ m \ONES \ge \mathcal B^2 V. \]</span> Again, from discounting and monotonitiy properties, we get <span class="math display">\[ \mathcal B^2 V + γ^2 m \ONES \ge \mathcal B^3 V. \]</span> Continuing this way, we get <span class="math display">\[ \mathcal B^k V + γ^k m \ONES \ge \mathcal B^{k+1} V. \]</span> Adding all the above equations, we get <span class="math display">\[ V + \sum_{\ell = 0}^k γ^\ell m \ONES \ge \mathcal B^{k+1} V. \]</span> Taking the limit as <span class="math inline">\(k \to ∞\)</span>, we get <span class="math display">\[ V + m \ONES/(1-γ) \ge V^*. \]</span></p>
</div>
</details>
<h1 data-number="5" id="value-iteration-algorithm"><span class="header-section-number">5</span> Value Iteration Algorithm</h1>
<section id="value-iteration-algorithm-1" class="unnumbered highlight">
<h4 class="unnumbered">Value Iteration Algorithm</h4>
<ol type="1">
<li>Start with any <span class="math inline">\(V_0 \in \reals^n\)</span>.</li>
<li>Recursively compute <span class="math inline">\(V_{k+1} = \mathcal B V_k = \mathcal B_{π_k} V_k.\)</span></li>
<li>Define <span class="math display">\[ \begin{align*}
   \underline δ_k &amp;= \frac{γ}{1-γ} \min_s \{ V_k(s) - V_{k-1}(s) \}, \\
   \bar δ_k &amp;=       \frac{γ}{1-γ} \max_s \{ V_k(s) - V_{k-1}(s) \}.
 \end{align*} \]</span></li>
</ol>
<p>Then, for all <span class="math inline">\(k\)</span></p>
<ol type="1">
<li><span class="math inline">\(V_k + \underline δ_k \ONES \le V^* \le V_k + \bar δ_k \ONES\)</span>.</li>
<li><span class="math inline">\((\underline δ_k - \bar δ_k) \ONES \le V_{π_k} - V^* \le (\bar δ_k - \underline δ_k) \ONES.\)</span></li>
</ol>
</section>
<h4 class="unnumbered" id="proof-5">Proof</h4>
<p>By construction, <span class="math display">\[ \begin{align*} 
   \mathcal B V_k - V_k &amp;= \mathcal B V_k - \mathcal B V_{k-1} \\ 
   &amp; \le \mathcal B_{π_{k-1}} V_k - \mathcal B_{π_{k-1}} V_{k-1}\\
   &amp; \le γ P_{π_{k-1}}[ V_k - V_{k-1} ] \\
   &amp;= (1-γ) \bar δ_k \ONES.
\end{align*} \]</span> Thus, by <a href="#prop:bounds" title="Prop. 5"><span class="pandoc-numbering-link prop">Prop. 5</span></a>, we have <span class="math display">\[\begin{equation} \label{eq:VI-1}
  V^* \le V_k + \bar δ_k \ONES.
\end{equation}\]</span> Note that <span class="math inline">\(\mathcal B V_k = \mathcal B_{π_k} V_k\)</span>. So, we have also show that <span class="math inline">\(\mathcal B_{π_k} V_k - V_k \le (1-γ) \bar δ_k \ONES\)</span>. Thus, again by <a href="#prop:bounds" title="Prop. 5"><span class="pandoc-numbering-link prop">Prop. 5</span></a>, we have <span class="math display">\[\begin{equation}\label{eq:VI-2}
  V_{π_k} \le V_k + \bar δ_k \ONES. 
\end{equation}\]</span></p>
<p>By a similar argument, we can show <span class="math display">\[\begin{equation}\label{eq:VI-3}
  V^* \ge V_k + \underline δ_k \ONES
\quad\text{and}\quad
 V_{π_k} \ge V_k + \underline δ_k \ONES. 
\end{equation}\]</span></p>
<p>Eq. \eqref{eq:VI-1} and \eqref{eq:VI-3} imply the first relationship of the result. To establish the second relationship, note that the triangle inequality <span class="math display">\[ \max\{ V_{π_k} - V^* \} \le 
   \max\{ V_{π_k} - V_k \} + \max\{ V_{k} - V^* \}
   \le (\bar δ_k - \underline δ_k) \ONES.
\]</span> Similarly, <span class="math display">\[ 
  \max\{ V^* - V_{π_k} \} \le
   \max \{ V^* - V_k \} + \max\{ V_k - V_{π_k} \}
   \le (\bar δ_k - \underline δ_k) \ONES.
\]</span> Combining the above two equation, we get the second relationship of the result. <span class="math inline">\(\Box\)</span></p>
<h1 data-number="6" id="policy-iteration-algorithm"><span class="header-section-number">6</span> Policy Iteration Algorithm</h1>
<div class="highlight">
<dl>
<dt><span id="lemma:1"></span><span id="lemma:PI" class="pandoc-numbering-text lemma"><strong>Lemma 1</strong></span></dt>
<dd>
<p><strong>Policy Improvement</strong> Suppose <span class="math inline">\(V_π\)</span> is the fixed point of <span class="math inline">\(\mathcal B_π\)</span> and <span class="math display">\[\mathcal B_{μ} V_π = \mathcal B V_π. \]</span> Then, <span class="math display">\[V_{μ}(s) \le V_π(s), \quad \forall s \in \ALPHABET S. \]</span> Moreover, if <span class="math inline">\(π\)</span> is not optimal, then at least one inequality is strict.</p>
</dd>
</dl>
</div>
<h4 class="unnumbered" id="proof-6">Proof</h4>
<p><span class="math display">\[ V_π = \mathcal B_π V_π \ge \mathcal B V_π = \mathcal B_{μ} V_π.\]</span> Thus, <span class="math display">\[ V_π \ge V_{μ}. \]</span></p>
<p>Finally, suppose <span class="math inline">\(V_μ = V_π\)</span>. Then, <span class="math display">\[ V_μ = \mathcal B_μ V_μ = \mathcal B_μ V_π = \mathcal B V_π = \mathcal B
V_μ. \]</span> Thus, <span class="math inline">\(V_μ\)</span> (and <span class="math inline">\(V_π\)</span>) is the unique fixed point of <span class="math inline">\(\mathcal B\)</span>. Hence <span class="math inline">\(μ\)</span> and <span class="math inline">\(π\)</span> are optimal.  <span class="math inline">\(\Box\)</span></p>
<section id="policy-iteration-algorithm-1" class="unnumbered highlight">
<h4 class="unnumbered">Policy Iteration Algorithm</h4>
<ol type="1">
<li><p>Start with an arbitrary policy <span class="math inline">\(π_0\)</span>. Compute <span class="math inline">\(V_0 = \mathcal B_{π_0} V_0\)</span>.</p></li>
<li><p>Recursively compute a policy <span class="math inline">\(π_k\)</span> such that <span class="math display">\[\mathcal B V_{k-1} = \mathcal B_{π_k} V_{k-1}\]</span> and compute the performance of the policy using <span class="math display">\[ V_k = \mathcal B_{π_k} V_k.\]</span></p></li>
<li><p>Stop if <span class="math inline">\(V_k = V_{k-1}\)</span> (or <span class="math inline">\(π_k = π_{k-1}\)</span>).</p></li>
</ol>
</section>
<p>The policy improvement lemma (<a href="#lemma:PI" title="Lemma 1"><span class="pandoc-numbering-link lemma">Lemma 1</span></a>) implies that <span class="math inline">\(V_{k-1} \ge V_k\)</span>. Since the state and action spaces are finite, there are only a finite number of policies. The value function improves at each step. So the process must converge in finite number of iterations. At convergence, <span class="math inline">\(V_k = V_{k-1}\)</span> and the policy improvement lemma implies that the corresponding policies <span class="math inline">\(π_k\)</span> or <span class="math inline">\(π_{k-1}\)</span> are optimal.</p>
<h2 data-number="6.1" id="policy-iteration-as-newton-raphson-algoritm"><span class="header-section-number">6.1</span> Policy iteration as Newton-Raphson algoritm</h2>
<!-- Taken from Pg 58 of Whittle, Optimal Control: Basics and Beyond -->
<p>Recall that the main idea behind Newton-Raphson is as follows. Suppose we want to solve a fixed point equation <span class="math inline">\(V = \mathcal B V\)</span> and we have an approximate solution <span class="math inline">\(V_k\)</span>. Then we can search for an improved soluiton <span class="math inline">\(V_{k+1} = V_k + Δ_k\)</span> by setting <span class="math display">\[\begin{equation} \label{eq:NR}
V_k + Δ_k = \mathcal{B}( V_k + Δ_k ), 
\end{equation} \]</span> expanding the right-hand side as far as first-order terms in <span class="math inline">\(Δ_k\)</span> and solving the consequent linear equation for <span class="math inline">\(Δ_k\)</span>.</p>
<p>Now, let’s try to apply this idea to find the fixed point of the Bellman equation. Suppose we have identified a guess <span class="math inline">\(V_k\)</span> and <span class="math inline">\(\mathcal B V_k = \mathcal B_{π_{k+1}} V_k\)</span>. Because the choice of control action <span class="math inline">\(a\)</span> is optimization out in <span class="math inline">\(\mathcal B\)</span>, the varation of <span class="math inline">\(a\)</span> induced by the variation <span class="math inline">\(Δ_k\)</span> of <span class="math inline">\(V_k\)</span> has no first-order effect on the value of <span class="math inline">\(\mathcal B(V_k + Δ_k)\)</span>. Therefore, <span class="math display">\[
  \mathcal{B}(V_k + Δ_k) = \mathcal B_{π_{k+1}}(V_k + Δ_k) + o(Δ_k).
\]</span> It follows that the linearized version of \eqref{eq:NR} is just <span class="math display">\[ 
  V_{k+1} = \mathcal B_{π_{k+1}} V_{k+1}.
\]</span> That is, <span class="math inline">\(V_{k+1}\)</span> is just the value function for the policy <span class="math inline">\(π_{k+1}\)</span>, where <span class="math inline">\(π_{k+1}\)</span> was deduced from the value function <span class="math inline">\(V_k\)</span> exactly by the policy improvement procedure. Therefore, we can conclude the following.</p>
<div class="highlight">
<dl>
<dt><span id="theorem:1"></span><span id="theorem:NR" class="pandoc-numbering-text theorem"><strong>Theorem 1</strong></span></dt>
<dd>
<p>The policy improvement algorithm is equivalent to the application of Newton-Raphson algorithm to the fixed point equation <span class="math inline">\(V = \mathcal B V\)</span> of dynamic programming.</p>
</dd>
</dl>
</div>
<p>The equivalence between policy iteration and Newton-Raphson partily explains why policy iteration approaches converge in few iterations.</p>
<h1 data-number="7" id="optimistic-policy-iteration"><span class="header-section-number">7</span> Optimistic Policy Iteration</h1>
<section id="optimistic-policy-iteration-algorithm" class="unnumbered highlight">
<h4 class="unnumbered">Optimistic Policy Iteration Algorithm</h4>
<ol type="1">
<li><p>Fix a sequence of integers <span class="math inline">\(\{\ell_k\}_{k \in \integers_{\ge 0}}\)</span>.</p></li>
<li><p>Start with an initial guess <span class="math inline">\(V_0 \in \reals^n\)</span>.</p></li>
<li><p>For <span class="math inline">\(k=0, 1, 2, \dots\)</span>, recursively compute a policy <span class="math inline">\(π_k\)</span> such that <span class="math display">\[ \mathcal B_{π_k} V_k = \mathcal B V_k \]</span> and then update the value function <span class="math display">\[ V_{k+1} = \mathcal B_{π_k}^{\ell_k} V_k. \]</span></p></li>
</ol>
</section>
<p>Note that if <span class="math inline">\(\ell_k = 1\)</span>, the optimistic policy iteration is equivalent to value iteration and if <span class="math inline">\(\ell_k = \infty\)</span>, then optimistic policy iteration is equal to policy iteration.</p>
<p>In the remainder of this section, we state the modifications of the main results to establish the convergence bounds for optimistic policy iteration.</p>
<div class="highlight">
<dl>
<dt><span id="prop:6"></span><span id="prop:multi-step" class="pandoc-numbering-text prop"><strong>Prop. 6</strong></span></dt>
<dd>
<p>For any <span class="math inline">\(V \in \reals^n\)</span> and <span class="math inline">\(m \ONES \in \reals_{\ge 0}\)</span></p>
<ul>
<li>If <span class="math inline">\(V + m \ONES \ge \mathcal B V = \mathcal B_π V\)</span>, then for any <span class="math inline">\(\ell \in \integers_{&gt; 0}\)</span>, <span class="math display">\[ \mathcal B V + \frac{γ}{1 - γ} m \ONES \ge \mathcal B_π^\ell V \]</span> and <span class="math display">\[ \mathcal B_π^\ell V + γ^\ell m \ONES \ge \mathcal B( \mathcal B_π^\ell V). \]</span></li>
</ul>
</dd>
</dl>
</div>
<p>The proof is left as an exercise.</p>
<div class="highlight">
<dl>
<dt><span id="prop:7"></span><span id="prop:optimistic-bound" class="pandoc-numbering-text prop"><strong>Prop. 7</strong></span></dt>
<dd>
<p>Let <span class="math inline">\(\{(V_k, π_k)\}_{k \ge 0}\)</span> be generated as per the optimistic policy iteration algorithm. Define <span class="math display">\[ \alpha_k = \begin{cases}
  1, &amp; \text{if } k = 0 \\
  γ^{\ell_0 + \ell_1 + \dots + \ell_{k-1}}, &amp; \text{if } k &gt; 0.
 \end{cases}\]</span> Suppose there exists an <span class="math inline">\(m \in \reals\)</span> such that <span class="math display">\[ \| V_0 - \mathcal B V_0 \| \le m. \]</span> Then, for all <span class="math inline">\(k \ge 0\)</span>, <span class="math display">\[ \mathcal B V_{k+1} - \alpha_{k+1} m \le V_{k+1} \le 
 \mathcal B V_k + \frac{γ}{1-γ} \alpha_k m.
 \]</span> Moreover, <span class="math display">\[ V_{k} - \frac{(k+1) γ^k}{1-γ} m \le
    V^* \le 
    V_k + \frac{\alpha_k}{1 - γ} m \le 
    V_k + \frac{γ^k}{1 - γ} m. \]</span></p>
</dd>
</dl>
</div>
<h1 class="unnumbered" id="exercises">Exercises</h1>
<ol type="1">
<li><p>Show that the error bound for value iteration is monotone with the number of iterations, i.e, <span class="math display">\[ V_k + \underline δ_k \ONES \le V_{k+1} + \underline δ_{k+1} \ONES
\le V^* 
\le V_{k+1} + \bar δ_{k+1} \ONES \le V_k + \bar δ_k \ONES. \]</span></p></li>
<li><p><strong>One-step look-ahead error bounds.</strong></p>
<p>Given any <span class="math inline">\(V \in \reals^n\)</span>, let <span class="math inline">\(π\)</span> be such that <span class="math inline">\(\mathcal B V = \mathcal B_π V\)</span>. Moreover, let <span class="math inline">\(V^*\)</span> denote the unique fixed point of <span class="math inline">\(\mathcal B\)</span> and <span class="math inline">\(V_π\)</span> denote the unique fixed point of <span class="math inline">\(\mathcal B_π\)</span>. Then, show that</p>
<ol type="a">
<li><span class="math display">\[ \| V^* - V \| \le \frac{1}{1-γ} \| \mathcal B V - V \|. \]</span></li>
<li><span class="math display">\[ \| V^* - \mathcal B V \| \le \frac{γ}{1-γ} \| \mathcal B V - V \|. \]</span></li>
<li><span class="math display">\[ \| V_π - V \| \le \frac{1}{1-γ} \| \mathcal B_π V - V \|. \]</span></li>
<li><span class="math display">\[ \| V_π - \mathcal B_π V \| \le \frac{γ}{1-γ} \| \mathcal B_π V - V \|. \]</span></li>
<li><span class="math display">\[ \| V_π - V^* \| \le \frac{2}{1-γ} \| \mathcal B V - V \|. \]</span></li>
<li><span class="math display">\[ \| V_π - V^* \| \le \frac{2γ}{1 - γ} \| V - V^* \|. \]</span></li>
</ol></li>
</ol>
<!-- TODO: Add the example of policy improvement for the harvesting example
from Page 61 of Whittle, Optimal Control: Basics and Beyond -->
<h1 class="unnumbered" id="references">References</h1>
<p>The material included here is referenced from different sources. Perhaps the best sources to study this material are the books by <span class="citation" data-cites="Puterman2014">Puterman (<a href="#ref-Puterman2014" role="doc-biblioref">2014</a>)</span>, <span class="citation" data-cites="Whittle1982">Whittle (<a href="#ref-Whittle1982" role="doc-biblioref">1982</a>)</span>, and <span class="citation" data-cites="Bertsekas:book">Bertsekas (<a href="#ref-Bertsekas:book" role="doc-biblioref">2011</a>)</span>.</p>
<p>The techniques for value iteration and policy improvement were formalized by <span class="citation" data-cites="Howard1960">Howard (<a href="#ref-Howard1960" role="doc-biblioref">1960</a>)</span>. The equivalence of policy improvement and the Newton-Raphson algorithm was demonstrated in the LQ case by <span class="citation" data-cites="Whittle1988a">Whittle and Komarova (<a href="#ref-Whittle1988a" role="doc-biblioref">1988</a>)</span>, for which it holds in a tighter sense.</p>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Bertsekas:book" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Bertsekas, D.P.</span> 2011. <em>Dynamic programming and optimal control</em>. Athena Scientific. Available at: <a href="http://www.athenasc.com/dpbook.html">http://www.athenasc.com/dpbook.html</a>.
</div>
<div id="ref-Howard1960" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Howard, R.A.</span> 1960. <em>Dynamic programming and markov processes</em>. The M.I.T. Press.
</div>
<div id="ref-Puterman2014" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Puterman, M.L.</span> 2014. <em>Markov decision processes: Discrete stochastic dynamic programming</em>. John Wiley &amp; Sons. DOI: <a href="https://doi.org/10.1002/9780470316887">10.1002/9780470316887</a>.
</div>
<div id="ref-Shwartz2001" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Shwartz, A.</span> 2001. Death and discounting. <em><span>IEEE</span> Transactions on Automatic Control</em> <em>46</em>, 4, 644–647. DOI: <a href="https://doi.org/10.1109/9.917668">10.1109/9.917668</a>.
</div>
<div id="ref-Whittle1982" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Whittle, P.</span> 1982. <em>Optimization over time: Dynamic programming and stochastic control. Vol. 1 and 2</em>. Wiley.
</div>
<div id="ref-Whittle1988a" class="csl-entry" role="doc-biblioentry">
<span class="smallcaps">Whittle, P. and Komarova, N.</span> 1988. Policy improvement and the newton-raphson algorithm. <em>Probability in the Engineering and Informational Sciences</em> <em>2</em>, 2, 249–255. DOI: <a href="https://doi.org/10.1017/s0269964800000760">10.1017/s0269964800000760</a>.
</div>
</div>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>This is true for general models only when the arg min at each state exists.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>


<p class="categories">
This entry 

 was last updated on 10 Nov 2022
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/mdp">
  MDP</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/infinite-horizon">infinite horizon</a>,
<a href="https://adityam.github.io/stochastic-control/tags/discounted-cost">discounted cost</a>,
<a href="https://adityam.github.io/stochastic-control/tags/bellman-operator">bellman operator</a>,
<a href="https://adityam.github.io/stochastic-control/tags/value-iteration">value iteration</a>,
<a href="https://adityam.github.io/stochastic-control/tags/policy-iteration">policy iteration</a>.</p>



    </div>
  </body>
</html>


