<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="infinite horizon,discounted cost,Bellman operator,value iteration,policy iteration" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>
</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2020
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Theory: Infinite horizon discounted MDP</div>

<p>A common way to approximate systems that run for a very large horizon is to assume that they run for an infinite horizon. There is an inherent homogeneity over time for infinite horizon system: the future depends only on the current state and not on the current time. Due to this homogeneity over time, we expect that the optimal policy should also be time-homogeneous. Therefore, the optimal policy for an infinite-horizon system should be easier to implement than the optimal policy for a finite horizon system, especially so when the horizon is large. This is one of the motivations for studying infinite horizon models.</p>
<p>The most common formulation for infinite horizon models is the discounted setup, where the cost function is assumed to be <span class="math display">\[ J(g) = \EXP\Bigl[ \sum_{t=1}^\infty \beta^{t-1} c_t(X_t, U_t) \Bigr] \]</span> where <span class="math inline">\(\beta \in (0,1)\)</span> is called the discount factor.</p>
<p>There are two interpretations of the discount factor <span class="math inline">\(\beta\)</span>. The first interpretation is an economic interpretation to determine the <em>present value</em> of a utility that will be received in the future. For example, suppose a decision maker is indifferent between receiving 1 dollar today or <span class="math inline">\(x\)</span> dollars tomorrow. This means that the decision maker discounts the future at a rate <span class="math inline">\(1/x\)</span>, so <span class="math inline">\(\beta = 1/x\)</span>.</p>
<p>The second interpretation is that of an absorbing state. Suppose we are operating a machine that generates a value of $1 each day. However, there is a probability <span class="math inline">\(p\)</span> that the machine will break down at the end of the day. Thus, the expected return for today is $1 while the expected return for tomorrow is <span class="math inline">\((1-p)\)</span> (which is the probability that the machine is still working tomorrow). In this case, the discount factor is defined as <span class="math inline">\((1-p)\)</span>. See <span class="citation" data-cites="Shwartz2001">@Shwartz2001</span> for a detailed discussion of this alternative.</p>
<p>In the remainder of this section, we will study how to obtain a solution for such infinite horizon discounted cost models.</p>
<p><em>Note</em>: Throughout this section, we assume that <span class="math inline">\(\ALPHABET X\)</span> and <span class="math inline">\(\ALPHABET U\)</span> are finite and <span class="math inline">\(|\ALPHABET X|= n\)</span> and <span class="math inline">\(|\ALPHABET U| = m\)</span>.</p>
<h1 id="performance-of-a-time-homogeneous-markov-policy">Performance of a time-homogeneous Markov policy</h1>
<p>For any <span class="math inline">\(g \colon \ALPHABET X \to \ALPHABET U\)</span>, consider the time homogeneous policy <span class="math inline">\((g, g, \dots)\)</span>. For ease of notation, we denote this policy simply by <span class="math inline">\(g\)</span>. The expected discounted cost under this policy is given by <span class="math display">\[ V_g(x) = \EXP^g\bigg[ \sum_{t=1}^∞ β^{t-1} c(X_t, U_t) \biggm| X_1 = x
\bigg].\]</span></p>
<p>To get a compact expression for this, define a <span class="math inline">\(n × 1\)</span> vector <span class="math inline">\(c_g\)</span> and a <span class="math inline">\(n × n\)</span> matrix <span class="math inline">\(P_g\)</span> as follows: <span class="math display">\[ [c_g]_x = c(x, g(x))
   \quad\text{and}\quad
   [P_g]_{xy} = P_{xy}(g(x)).
\]</span> Then the dynamics under policy <span class="math inline">\(g\)</span> are Markovian with transition probability matrix <span class="math inline">\(P_g\)</span> and a cost function <span class="math inline">\(c_g\)</span>. Then <span class="math display">\[ \begin{align*}
\EXP^g\big[ c(X_t, g(X_t)) \bigm| X_1 = x \big]
  &amp;= \sum_{y \in \ALPHABET X} \PR^g(X_t = y | X_1 = x) c(y, g(y))
  \\
  &amp;= \sum_{y \in \ALPHABET X} [P_g^{t-1}]_{xy} [c_g]_y
  \\
  &amp;= δ_x P_g^{t-1} c_g.
\end{align*} \]</span></p>
<p>Let <span class="math inline">\(V_g\)</span> denote the <span class="math inline">\(n × 1\)</span> vector given by <span class="math inline">\([V_g]_x = V_g(x)\)</span>. Then, <span class="math display">\[ \begin{align*}
V_g &amp;= c_g + β P_g c_g + β^2 P_g^2 c_g + \cdots \\
    &amp;= c_g + β P_g \big( c_g + β P_g c_g + \cdots \big) \\
    &amp;= c_g + β P_g V_g,
\end{align*} \]</span> which can be rewritten as <span class="math display">\[ (I - β P_g) V_g = c_g. \]</span></p>
<p>The <a href="https://en.wikipedia.org/wiki/Spectral_radius">spectral radius</a> of <span class="math inline">\(ρ(β P_d)\)</span> is upper bounded by <span class="math inline">\(\lVert β P_d \rVert = β &lt; 1\)</span>. Therefore, the matrix <span class="math inline">\((I - β P_g)\)</span> has an inverse and by left multiplying both sides by <span class="math inline">\((I - β P_g)^{-1}\)</span>, we get <span class="math display">\[ V_g = (I - βP_g)^{-1} c_g. \]</span></p>
<p>The equation <span class="math display">\[ V_g = c_g + β P_g V_g \]</span> is sometimes also written as <span class="math display">\[ V_g = \mathcal B_g V_g \]</span> where the operator <span class="math inline">\(\mathcal B_g\)</span>, which is called the <em>Bellman operator</em>, is an operator from <span class="math inline">\(\reals^n\)</span> to <span class="math inline">\(\reals^n\)</span> given by <span class="math display">\[ \mathcal B_g v = c_g + β P_g v.\]</span></p>
<h1 id="bellman-operators">Bellman operators</h1>
<dl>
<dt>Definition</dt>
<dd><p>Define the <em>Bellman operator</em> <span class="math inline">\(\mathcal B : \reals^n \to \reals^n\)</span> as follows: for any <span class="math inline">\(v \in \reals^n\)</span> <span class="math display">\[ [\mathcal B v]_x = \min_{u \in \ALPHABET U}
\Big\{ c(x,u) + β \sum_{y \in \ALPHABET Y} P_{xy}(u) v_y \Big\}.
\]</span></p>
</dd>
</dl>
<p>Note that the above may also be written as<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="math display">\[ \mathcal B v = \min_{g \in \ALPHABET G} \mathcal B_g v, \]</span> where <span class="math inline">\(\ALPHABET G\)</span> denotes the set of all deterministic Markov policies.</p>
<div class="highlight">
<dl>
<dt>Prop. #prop:contraction</dt>
<dd><p>For any <span class="math inline">\(v \in \reals^n\)</span>, define the norm <span class="math inline">\(\NORM{V} := \sup_{x \in \ALPHABET X} \ABS{V_x}\)</span>. Then, the Bellman operator is a contraction, i.e., for any <span class="math inline">\(v, w \in \reals^n\)</span>, <span class="math display">\[ \NORM{\mathcal B v - \mathcal B w} \le β \NORM{v - w}. \]</span></p>
</dd>
</dl>
</div>
<h4 class="unnumbered" id="proof">Proof</h4>
<p>Fix a state <span class="math inline">\(x \in \ALPHABET X\)</span> and consider <span class="math inline">\([\mathcal B v](x) - [\mathcal B w](x)\)</span>. In particular, let <span class="math inline">\(u^*\)</span> be the optimal action in the right hand side of <span class="math inline">\([\mathcal B w](x)\)</span>. Then, <span class="math display">\[\begin{align*}
  [\mathcal B v - \mathcal B w](x) &amp;= 
  \min_{u \in \ALPHABET U}\bigl\{ c(x,u) + \beta \sum_{y \in \ALPHABET X}
  P_{xy}(u) v(y) \bigr\} - 
  \min_{u \in \ALPHABET U}\bigl\{ c(x,u) + \beta \sum_{y \in \ALPHABET X}
  P_{xy}(u) w(y) \bigr\} 
  \\
  &amp;\le c(x,u^*) + \beta \sum_{y\in \ALPHABET X} P_{xy}(u^*) v(y) -
       c(x,u^*) - \beta \sum_{y\in \ALPHABET X} P_{xy}(u^*) w(y)
  \\
  &amp;\le \beta \sum_{y \in \ALPHABET X} P_{xy}(u^*) \| v - w \|
  \\
  &amp;= \beta \| v - w \|.
\end{align*} \]</span></p>
<p>By a similar argument, we can show that <span class="math inline">\([\mathcal B w - \mathcal B v](x) \le \beta \| v - w \|\)</span>, which proves the other side of the inequality.  <span class="math inline">\(\Box\)</span></p>
<p>An immediate consequence of the contraction property is the following.</p>
<div class="highlight">
<dl>
<dt>Theorem #thm:fixed-point</dt>
<dd><p>There is a unique bounded <span class="math inline">\(V^* \in \reals^n\)</span> that satisfies the <em>Bellman equation</em> <span class="math display">\[ V = \mathcal B V \]</span></p>
<p>Moreover, if we start from any <span class="math inline">\(V_0 \in \reals^n\)</span> and recursively define <span class="math display">\[ V_n = \mathcal B V_{n-1} \]</span> then <span class="math display">\[ \lim_{n \to ∞} V_n = V^*. \]</span></p>
</dd>
</dl>
</div>
<h4 class="unnumbered" id="proof-1">Proof</h4>
<p>This follows immediately from the <a href="https://proofwiki.org/wiki/Banach_Fixed-Point_Theorem">Banach fixed point theorem</a>  <span class="math inline">\(\Box\)</span></p>
<h1 id="optimal-time-homogeneous-policy">Optimal time-homogeneous policy</h1>
<!--
::: highlight :::
Prop.

:   Let $h$ be any (possibly randomized) history dependent policy. Then,      
    there exists a randomized Markov policy $g$ that has the same performance
    as $h$.
:::

#### Proof {-}

For any policy $h$, define the _occupation measure_
$$ \mu(x,u | x^∘; h) 
  := \EXP^h \bigg[ \sum_{t=1}^∞ β^{t-1} \IND\{X_t = x, U_t = u\} 
  \biggm| X_1 = x^∘ \bigg]
$$
Then 
$$ J(x^∘; h) = \sum_{x \in \ALPHABET X, u \in \ALPHABET U}
   μ(x,u | x^∘; h) c(x,u).
$$ {#eq:performance}

Now, define a randomized Markov policy $g$ as follows:
$$\PR(U_t = u | X_t = x; g) = \frac{ μ(x, u| x^∘; h) } { μ(x | x^∘; h) }$$
where $μ(x|x^∘;h) = \sum_{u \in \ALPHABET U} μ(x,u | x^∘; h)$. Then, by
construction, 
$ μ(x,u | x^∘; g) = μ(x,u | x^∘; h) $
and therefore by \\eqref{eq:performance}, we have that $J(x^∘; g) = J(x^∘; h).$

-->
<div class="highlight">
<dl>
<dt>Prop. #prop:optimal</dt>
<dd><p>Define <span class="math display">\[ V^{opt}_∞(x) := \min_{g} \EXP^g \bigg[ \sum_{t=1}^∞ β^{t-1} c(X_t, U_t) 
\biggm| X_1 = x \bigg], \]</span> where the minimum is over all (possibly randomized) history dependent policies. Then, <span class="math display">\[ V^{opt} = V^*, \]</span> where <span class="math inline">\(V^*\)</span> is the solution of the Bellman equation.</p>
</dd>
</dl>
</div>
<h4 class="unnumbered" id="proof-2">Proof</h4>
<p>Since the state and action space are finite, without loss of optimality, we can assume that <span class="math inline">\(0 \le c(x,u) \le M\)</span>.</p>
<p>Consider the finite horizon truncation <span class="math display">\[ V^{opt}_T(x) =  \min_{g} \EXP^g\bigg[ \sum_{t=1}^T β^{t-1} c(X_t, U_t) | X_1 = x \bigg].
\]</span> From the results for finite horizon MDP, we have that <span class="math display">\[ V^{opt}_T = \mathcal B^T V_0 \]</span> where <span class="math inline">\(V_0\)</span> is the all zeros vector.</p>
<p>Now by construction, <span class="math display">\[V^{opt}_∞(x) \ge V^{opt}_T(x) = [\mathcal B^T V_0](x). \]</span> Taking limit as <span class="math inline">\(T \to ∞\)</span>, we get that <span class="math display">\[\begin{equation}\label{eq:1}
  V^{opt}_∞(x) \ge \lim_{T \to ∞} [\mathcal B^T V_0](x) = V^*(x). 
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(0 \le c(x,u) \le M\)</span>, for any <span class="math inline">\(T\)</span>, <span class="math display">\[ \begin{align*}
V^{opt}_∞(x) &amp;\le \min_g \EXP^g \bigg[ \sum_{t=1}^T β^{t-1} c(X_t, U_t) 
\biggm| X_1 = x \bigg] + \sum_{t=T+1}^∞ β^{t-1} M \\
&amp;= V^{opt}_T(x) + β^T M / (1 - β) \\
&amp;= [\mathcal B^T V_0](x) + β^T M / (1-β). 
\end{align*} \]</span> Taking limit as <span class="math inline">\(T \to ∞\)</span>, we get that <span class="math display">\[\begin{equation}\label{eq:2}
  V^{opt}_∞(x) \le \lim_{T \to ∞}
  \big\{ [\mathcal B^T V_0](x) + β^T M / (1-β) \big\} = V^*(x). 
\end{equation}\]</span></p>
<p>From \eqref{eq:1} and \eqref{eq:2}, we get that <span class="math inline">\(V^{opt}_∞ = V^*\)</span>.  <span class="math inline">\(\Box\)</span></p>
<h1 id="properties-of-bellman-operator">Properties of Bellman operator</h1>
<div class="highlight">
<dl>
<dt>Prop. #prop:properties</dt>
<dd><p>The Bellman operator satisfies the following properties</p>
<ul>
<li><em>Monotonicity</em>. For any <span class="math inline">\(v, w \in \reals^n\)</span>, if <span class="math inline">\(v \le w\)</span>, then <span class="math inline">\(\mathcal B_g v \le \mathcal B_g w\)</span> and <span class="math inline">\(\mathcal B v \le \mathcal B w\)</span>.</li>
<li><em>Discounting</em>. For any <span class="math inline">\(v \in \reals^n\)</span> and <span class="math inline">\(m \in \reals\)</span>, <span class="math inline">\(\mathcal B_g (v + m) = \mathcal B_g v + β m\)</span> and <span class="math inline">\(\mathcal B (v + m) = \mathcal B v + β m\)</span>.</li>
</ul>
</dd>
</dl>
</div>
<p>Note that we interpret <span class="math inline">\(v + m\)</span> as <span class="math inline">\([v + m]_x = v_x + m\)</span>.</p>
<h4 class="unnumbered" id="proof-3">Proof</h4>
<p>We first prove the monotonicity property. Recall that <span class="math display">\[ \mathcal B_g v = c_g + β P_g v. \]</span> So, monotonicity of <span class="math inline">\(\mathcal B_g\)</span> follows immediately from monotonicity of matrix multiplication for positive matrices.</p>
<p>Let <span class="math inline">\(h\)</span> be such that <span class="math inline">\(\mathcal B w = \mathcal B_h w\)</span>. Then, <span class="math display">\[ v = \mathcal B v \le \mathcal B_h v 
\stackrel{(a)} \le \mathcal B_h w = \mathcal B w = w,
\]</span> where <span class="math inline">\((a)\)</span> uses the monotonicity of <span class="math inline">\(\mathcal B_h\)</span>.</p>
<p>We now prove the discounting property. Recall that <span class="math display">\[ \mathcal B_g v = c_g + β P_g v. \]</span> Thus, <span class="math display">\[ \mathcal B_g(v+m) = c_g + β P_g (v+m) = c_g + β P_g v + β m = \mathcal B_g
v + β m.\]</span> Thus, <span class="math inline">\(\mathcal B_g\)</span> is discounting. Now consider <span class="math display">\[ \mathcal B (v + m ) = \min_{g} \mathcal B_g (v+m)
= \min_g \mathcal (B_g v + β m) = \mathcal B v + β m.\]</span> Thus, <span class="math inline">\(\mathcal B\)</span> is discounting.  <span class="math inline">\(\Box\)</span></p>
<div class="highlight">
<dl>
<dt>Prop. #prop:coarse-bounds</dt>
<dd><p>For any <span class="math inline">\(V \in \reals^n\)</span>,</p>
<ul>
<li>If <span class="math inline">\(V \ge \mathcal B V\)</span>, then <span class="math inline">\(V \ge V^*\)</span>;</li>
<li>If <span class="math inline">\(V \le \mathcal B V\)</span>, then <span class="math inline">\(V \le V^*\)</span>;</li>
<li>If <span class="math inline">\(V = \mathcal B V\)</span>, then <span class="math inline">\(V\)</span> is the only vector with this property and <span class="math inline">\(V = V^*\)</span>.</li>
</ul>
<p>The same bounds are true when <span class="math inline">\((\mathcal B, V^*)\)</span> is replaced with <span class="math inline">\((\mathcal B_g, V_g)\)</span>.</p>
</dd>
</dl>
</div>
<h4 class="unnumbered" id="proof-4">Proof</h4>
<p>We prove the first part. The proof of the other parts is similar.</p>
<p>We are given that <span class="math display">\[V \ge \mathcal B V.\]</span> Then, by monotonicity of the Bellman operator, <span class="math display">\[ \mathcal B V \ge \mathcal B^2 V.\]</span> Continuing this way, we get <span class="math display">\[ \mathcal B^k V \ge \mathcal B^{k+1} V.\]</span> Adding the above equations, we get <span class="math display">\[ V \ge \mathcal B^{k+1} V.\]</span> Taking limit as <span class="math inline">\(k \to ∞\)</span>, we get <span class="math display">\[V \ge V^*. \qquad \Box\]</span></p>
<div class="highlight">
<dl>
<dt>Prop. #prop:bounds</dt>
<dd><p>For any <span class="math inline">\(V \in \reals^n\)</span> and <span class="math inline">\(m \in \reals\)</span>,</p>
<ul>
<li>If <span class="math inline">\(V + m \ge \mathcal B V\)</span>, then <span class="math inline">\(V + m/(1-β) \ge V^*\)</span>;</li>
<li>If <span class="math inline">\(V + m \le \mathcal B V\)</span>, then <span class="math inline">\(V + m/(1-β) \le V^*\)</span>;</li>
</ul>
<p>The same bounds are true when <span class="math inline">\((\mathcal B, V^*)\)</span> is replaced with <span class="math inline">\((\mathcal B_g, V_g)\)</span>.</p>
</dd>
</dl>
</div>
<dl>
<dt>Remark</dt>
<dd><p>The above result can also be stated as follows:</p>
<ul>
<li><span class="math inline">\(\displaystyle \| V_g - V \| \le \frac{1}{1-β}\| \mathcal B_g V - V \|\)</span>.</li>
<li><span class="math inline">\(\displaystyle \| V^* - V \| \le \frac{1}{1-β}\| \mathcal B V - V \|\)</span>.</li>
</ul>
</dd>
</dl>
<h4 class="unnumbered" id="proof-5">Proof</h4>
<p>Again, we only prove the first part. The proof of the second part is the same. We have that <span class="math display">\[ V + m \ge \mathcal B V. \]</span> From discounting and monotonicity properties, we get <span class="math display">\[ \mathcal B V + β m \ge \mathcal B^2 V. \]</span> Again, from discounting and monotonitiy properties, we get <span class="math display">\[ \mathcal B^2 V + β^2 m \ge \mathcal B^3 V. \]</span> Continuing this way, we get <span class="math display">\[ \mathcal B^k V + β^k m \ge \mathcal B^{k+1} V. \]</span> Adding all the above equations, we get <span class="math display">\[ V + \sum_{\ell = 0}^k β^\ell m \ge \mathcal B^{k+1} V. \]</span> Taking the limit as <span class="math inline">\(k \to ∞\)</span>, we get <span class="math display">\[ V + m/(1-β) \ge V^*. \Box\]</span></p>
<h1 id="value-iteration-algorithm">Value Iteration Algorithm</h1>
<section id="value-iteration-algorithm-1" class="highlight">
<h4>Value Iteration Algorithm</h4>
<ol type="1">
<li>Start with any <span class="math inline">\(V_0 \in \reals^n\)</span>.</li>
<li>Recursively compute <span class="math inline">\(V_{k+1} = \mathcal B V_k = \mathcal B_{g_k} V_k.\)</span></li>
<li>Define <span class="math display">\[ \begin{align*}
   \underline δ_k &amp;= \frac{β}{1-β} \min_x \{ V_k(x) - V_{k-1}(x) \}, \\
   \bar δ_k &amp;=       \frac{β}{1-β} \max_x \{ V_k(x) - V_{k-1}(x) \}.
 \end{align*} \]</span></li>
</ol>
<p>Then, for all <span class="math inline">\(k\)</span></p>
<ol type="1">
<li><span class="math inline">\(V_k + \underline δ_k \le V^* \le V_k + \bar δ_k\)</span>.</li>
<li><span class="math inline">\(\underline δ_k - \bar δ_k \le \NORM{V_{g_k} - V^*} \le \bar δ_k - \underline δ_k.\)</span></li>
</ol>
</section>
<h4 class="unnumbered" id="proof-6">Proof</h4>
<p>By construction, <span class="math display">\[ \begin{align*} 
   \mathcal B V_k - V_k &amp;= \mathcal B V_k - \mathcal B V_{k-1} \\ 
   &amp; \le \mathcal B_{g_{k-1}} V_k - \mathcal B_{g_{k-1}} V_{k-1}\\
   &amp; \le β P_{g_{k-1}}[ V_k - V_{k-1} ] \\
   &amp;= (1-β) \bar δ_k.
\end{align*} \]</span> Thus, by the previous proposition, we have <span class="math display">\[ V^* \le V_k + \bar δ_k.\]</span> Note that <span class="math inline">\(\mathcal B V_k = \mathcal B_{g_k} V_k\)</span>. So, we have also show that <span class="math inline">\(\mathcal B_{g_k} V_k - V_k \le (1-\beta) \bar \delta_k\)</span>. Thus, <span class="math display">\[ V_{g_k} \le V_k + \bar δ_k. \]</span></p>
<p>By a similar argument, we can show <span class="math display">\[ V^* \ge V_k + \underline δ_k
\quad\text{and}\quad
 V_{g_k} \ge V_k + \underline δ_k. \]</span></p>
<p>Now, by the triangle inequality <span class="math display">\[ \max\{ V_{g_k} - V^* \} \le 
   \max\{ V_{g_k} - V_k \} + \max\{ V_{k} - V^* \}
   \le \bar δ_k - \underline δ_k.
\]</span> Similarly, <span class="math display">\[ \max\{ V^* - V_{g_k} \} \le
   \max \{ V^* - V_k \} + \max\{ V_k - V_{g_k} \}
   \le \bar δ_k - \underline δ_k.
\]</span></p>
<p>Thus, <span class="math display">\[ \NORM{ V_{g_k} - V^* } \le \bar δ_k - \underline δ_k. \]</span></p>
<p>By a similar argument, we can show that <span class="math display">\[ \NORM{ V_{g_k} - V^* } \ge \underline δ_k - \bar δ_k. 
\qquad \Box\]</span></p>
<h1 id="policy-iteration-algorithm">Policy Iteration Algorithm</h1>
<div class="highlight">
<dl>
<dt>Lemma #lemma:PI</dt>
<dd><p><strong>Policy Improvement</strong> Suppose <span class="math inline">\(V_g\)</span> is the fixed point of <span class="math inline">\(\mathcal B_g\)</span> and <span class="math display">\[\mathcal B_{h} V_g = \mathcal B V_g. \]</span> Then, <span class="math display">\[V_{h}(x) \le V_g(x), \quad \forall x \in \ALPHABET X. \]</span> Moreover, if <span class="math inline">\(g\)</span> is not optimal, then at least one inequality is strict.</p>
</dd>
</dl>
</div>
<h4 class="unnumbered" id="proof-7">Proof</h4>
<p><span class="math display">\[ V_g = \mathcal B_g V_g \ge \mathcal B V_g = \mathcal B_{h} V_g.\]</span> Thus, <span class="math display">\[ V_g \ge V_{h}. \]</span></p>
<p>Finally, suppose <span class="math inline">\(V_h = V_g\)</span>. Then, <span class="math display">\[ V_h = \mathcal B_h V_h = \mathcal B_h V_g = \mathcal B V_g = \mathcal B
V_h. \]</span> Thus, <span class="math inline">\(V_h\)</span> (and <span class="math inline">\(V_g\)</span>) is the unique fixed point of <span class="math inline">\(\mathcal B\)</span>. Hence <span class="math inline">\(h\)</span> and <span class="math inline">\(g\)</span> are optimal.  <span class="math inline">\(\Box\)</span></p>
<section id="policy-iteration-algorithm-1" class="unnumbered highlight">
<h4 class="unnumbered">Policy Iteration Algorithm</h4>
<ol type="1">
<li><p>Start with an arbitrary policy <span class="math inline">\(g_0\)</span>. Compute <span class="math inline">\(V_0 = \mathcal B_{g_0} V_0\)</span>.</p></li>
<li><p>Recursively compute a policy <span class="math inline">\(g_k\)</span> such that <span class="math display">\[\mathcal B V_{k-1} = \mathcal B_{g_k} V_{k-1}\]</span> and compute the performance of the policy using <span class="math display">\[ V_k = \mathcal B_{g_k} V_k.\]</span></p></li>
<li><p>Stop if <span class="math inline">\(V_k = V_{k-1}\)</span> (or <span class="math inline">\(g_k = g_{k-1}\)</span>).</p></li>
</ol>
</section>
<p>The policy improvement lemma (<span class="citation" data-cites="lemma:PI">@lemma:PI</span>) implies that <span class="math inline">\(V_{k-1} \ge V_k\)</span>. Since the state and action spaces are finite, there are only a finite number of policies. The value function improves at each step. So the process must converge in finite number of iterations. At convergence, <span class="math inline">\(V_k = V_{k-1}\)</span> and the policy improvement lemma implies that the corresponding policies <span class="math inline">\(g_k\)</span> or <span class="math inline">\(g_{k-1}\)</span> are optimal.</p>
<h1 id="optimistic-policy-iteration">Optimistic Policy Iteration</h1>
<section id="optimistic-policy-iteration-algorithm" class="unnumbered highlight">
<h4 class="unnumbered">Optimistic Policy Iteration Algorithm</h4>
<ol type="1">
<li><p>Fix a sequence of integers <span class="math inline">\(\{\ell_k\}_{k \in \integers_{\ge 0}}\)</span>.</p></li>
<li><p>Start with an initial guess <span class="math inline">\(V_0 \in \reals^n\)</span>.</p></li>
<li><p>For <span class="math inline">\(k=0, 1, 2, \dots\)</span>, recursively compute a policy <span class="math inline">\(g_k\)</span> such that <span class="math display">\[ \mathcal B_{g_k} V_k = \mathcal B V_k \]</span> and then update the value function <span class="math display">\[ V_{k+1} = \mathcal B_{g_k}^{\ell_k} V_k. \]</span></p></li>
</ol>
</section>
<p>Note that if <span class="math inline">\(\ell_k = 1\)</span>, the optimistic policy iteration is equivalent to value iteration and if <span class="math inline">\(\ell_k = \infty\)</span>, then optimistic policy iteration is equal to policy iteration.</p>
<p>In the remainder of this section, we state the modifications of the main results to establish the convergence bounds for optimistic policy iteration.</p>
<div class="highlight">
<dl>
<dt>Prop. #prop:multi-step</dt>
<dd><p>For any <span class="math inline">\(V \in \reals^n\)</span> and <span class="math inline">\(m \in \reals_{\ge 0}\)</span></p>
<ul>
<li>If <span class="math inline">\(V + m \ge \mathcal B V = \mathcal B_g V\)</span>, then for any <span class="math inline">\(\ell \in \integers_{&gt; 0}\)</span>, <span class="math display">\[ \mathcal B V + \frac{\beta}{1 - \beta} m \ge \mathcal B_g^\ell V \]</span> and <span class="math display">\[ \mathcal B_g^\ell V + \beta^\ell m \ge \mathcal B( \mathcal B_g^\ell V). \]</span></li>
</ul>
</dd>
</dl>
</div>
<p>The proof is left as an exercise.</p>
<div class="highlight">
<dl>
<dt>Prop. #prop:optimistic-bound</dt>
<dd><p>Let <span class="math inline">\(\{(V_k, g_k)\}_{k \ge 0}\)</span> be generated as per the optimistic policy iteration algorithm. Define <span class="math display">\[ \alpha_k = \begin{cases}
  1, &amp; \text{if } k = 0 \\
  \beta^{\ell_0 + \ell_1 + \dots + \ell_{k-1}}, &amp; \text{if } k &gt; 0.
 \end{cases}\]</span> Suppose there exists an <span class="math inline">\(m \in \reals\)</span> such that <span class="math display">\[ \| V_0 - \mathcal B V_0 \| \le m. \]</span> Then, for all <span class="math inline">\(k \ge 0\)</span>, <span class="math display">\[ \mathcal B V_{k+1} - \alpha_{k+1} m \le V_{k+1} \le 
 \mathcal B V_k + \frac{\beta}{1-\beta} \alpha_k m.
 \]</span> Moreover, <span class="math display">\[ V_{k} - \frac{(k+1) \beta^k}{1-\beta} m \le
    V^* \le 
    V_k + \frac{\alpha_k}{1 - \beta} m \le 
    V_k + \frac{\beta^k}{1 - \beta} m. \]</span></p>
</dd>
</dl>
</div>
<h1 class="unnumbered" id="exercises">Exercises</h1>
<ol type="1">
<li><p>Show that the error bound for value iteration is monotone with the number of iterations, i.e, <span class="math display">\[ V_k + \underline δ_k \le V_{k+1} + \underline δ_{k+1} 
\le V^* 
\le V_{k+1} + \bar δ_{k+1} \le V_k + \bar δ_k. \]</span></p></li>
<li><p><strong>One-step look-ahead error bounds.</strong></p>
<p>Given any <span class="math inline">\(V \in \reals^n\)</span>, let <span class="math inline">\(g\)</span> be such that <span class="math inline">\(\mathcal B V = \mathcal B_g V\)</span>. Moreover, let <span class="math inline">\(V^*\)</span> denote the unique fixed point of <span class="math inline">\(\mathcal B\)</span> and <span class="math inline">\(V_g\)</span> denote the unique fixed point of <span class="math inline">\(\mathcal B_g\)</span>. Then, show that</p>
<ol type="a">
<li><span class="math display">\[ \| V^* - V \| \le \frac{1}{1-\beta} \| \mathcal B V - V \|. \]</span></li>
<li><span class="math display">\[ \| V^* - \mathcal B V \| \le \frac{\beta}{1-\beta} \| \mathcal B V - V \|. \]</span></li>
<li><span class="math display">\[ \| V_g - V \| \le \frac{1}{1-\beta} \| \mathcal B_g V - V \|. \]</span></li>
<li><span class="math display">\[ \| V_g - \mathcal B_g V \| \le \frac{\beta}{1-\beta} \| \mathcal B_g V - V \|. \]</span></li>
<li><span class="math display">\[ \| V_g - V^* \| \le \frac{2}{1-\beta} \| \mathcal B V - V \|. \]</span></li>
<li><span class="math display">\[ \| V_g - V^* \| \le \frac{2\beta}{1 - \beta} \| V - V^* \|. \]</span></li>
</ol></li>
</ol>
<h1 class="unnumbered" id="references">References</h1>
<p>The material included here is referenced from different sources. Perhaps the best sources to study this material are the books by <span class="citation" data-cites="Puterman2014">@Puterman2014</span>, <span class="citation" data-cites="Whittle1982">@Whittle1982</span>, and <span class="citation" data-cites="Bertsekas:book">@Bertsekas:book</span>.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>This is true for general models only when the arg min at each state exists.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>


<p class="categories">
This entry 

 was last updated on 31 Mar 2020
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/mdp">
  MDP</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/infinite-horizon">infinite horizon</a>,
<a href="https://adityam.github.io/stochastic-control/tags/discounted-cost">discounted cost</a>,
<a href="https://adityam.github.io/stochastic-control/tags/bellman-operator">bellman operator</a>,
<a href="https://adityam.github.io/stochastic-control/tags/value-iteration">value iteration</a>,
<a href="https://adityam.github.io/stochastic-control/tags/policy-iteration">policy iteration</a>.</p>



      <script type="text/javascript">
      
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-6887174-4']);
            _gaq.push(['_trackPageview']);
            (function() {
                var ga   = document.createElement('script');
                ga.type  = 'text/javascript';
                ga.async = true;
                ga.src   = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
                var s = document.getElementsByTagName('script')[0];
                s.parentNode.insertBefore(ga, s);
              })();
      
      </script>
    </div>
  </body>
</html>


