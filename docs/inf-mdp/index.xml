<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inf-MDP on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/inf-mdp/</link>
    <description>Recent content in Inf-MDP on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="https://adityam.github.io/stochastic-control/inf-mdp/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Infinite horizon discounted MDP</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/discounted-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/discounted-mdp/</guid>
      <description>Note: Throughout this section, we assume that \(\ALPHABET X\) and \(\ALPHABET U\) are finite and \(|\ALPHABET X|= n\) and \(|\ALPHABET U| = m\).
Performance of a time-homogeneous Markov policy For any \(g \colon \ALPHABET X \to \ALPHABET U\), consider the time homogeneous policy \((g, g, \dots)\). For ease of notation, we denote this policy simply by \(g\). The expected discounted cost under this policy is given by \[ V_g(x) = \EXP^g\bigg[ \sum_{t=1}^∞ β^{t-1} c(X_t, U_t) \biggm| X_1 = x \bigg].</description>
    </item>
    
    <item>
      <title>Example: Inventory Management (revisited)</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/inventory-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/inventory-management/</guid>
      <description>TL;DR One of the potential benefits of modeling a system as infinite horizon discounted cost MDP is that it can be simpler to identify an optimal policy. We illustrate this using the inventory management example.
Consider the model for inventory management and assume that it runs for an infinite horizon. We assume that the per-step cost is given by \[c(x,u,x_{+}) = p u + \beta h(x), \] where \[ h(x) = \begin{cases} ax, &amp;amp; \text{if $x \ge 0$} \\ -bx, &amp;amp; \text{if $x &amp;lt; 0$}, \end{cases}\] where \(a\) is the per-unit holding cost, \(b\) is the per-unit shortage cost, and \(p\) is the per-unit procurement cost.</description>
    </item>
    
    <item>
      <title>Lipschitz MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/lipschitz-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/lipschitz-mdp/</guid>
      <description>Preliminaries Given two metric spaces \((\ALPHABET X, d_X)\) and \((\ALPHABET Y, d_Y)\), the Lipschitz constant of function \(f \colon \ALPHABET X \to \ALPHABET Y\) is defined by \[ \| f\|_{L} = \sup_{x_1 \neq x_2} \left\{ \frac{ d_Y(f(x_1), f(x_2)) } { d_X(x_1, x_2) } : x_1, x_2 \in \ALPHABET X \right\} \in [0, ∞]. \] The function is called Lipschitz continuous if its Lipschitz constant is finite.
Let \(\ALPHABET U\) be an arbitrary set.</description>
    </item>
    
    <item>
      <title>Approximate dynamic programming</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/adp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/adp/</guid>
      <description>The value and policy iteration algorithms for discounted cost MDPs rely on exact computation of the Bellman update \(W = \mathcal B V\) and the corresponding optimal policy \(g\) such that \(\mathcal B V = \mathcal B_g V\). Suppose we cannot compute these updates exactly, but can find approximate solutions \(W\) and \(g\) such that \[ \NORM{W - \mathcal B V} \le δ \quad\text{and}\quad \NORM{\mathcal B_g V - \mathcal B V} \le ε\] where \(δ\) and \(ε\) are positive constants.</description>
    </item>
    
  </channel>
</rss>