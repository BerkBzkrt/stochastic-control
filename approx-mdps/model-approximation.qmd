---
title: "Model approximation"
keywords:
  - infinite horizon
  - discounted cost
  - Lipschitz continuity
  - approximation bounds
  - state aggregation
---

There are many instances where approximate models are used. For example, in many applications with large state spaces, one often construct a simulation model of the system and use it to identify an optimal policy using simulation based methods (such as reinforcement learning). Often, the simulation model is only an approximation of the true model. In such instances, we want to know the error in using the policy obtained from the simulation model in the real world. In this section, we present bounds on such _sim-to-real_ transfer. 

Consider an MDP $\ALPHABET M = \langle \ALPHABET S, \ALPHABET A, P, c, γ
\rangle$. Suppose the components $\langle \ALPHABET S, \ALPHABET A, γ \rangle$
are known exactly but the components $(P,c)$ are known approximately. Consider the
approximate MDP $\widehat {\ALPHABET M} = \langle \ALPHABET S, \ALPHABET A,
\hat P, \hat c, γ \rangle$. We will call $\ALPHABET M$ to be the _true model_
and $\widehat {\ALPHABET M}$ to be the _approximate model_. 

Let $V^*$ and $\hat V^*$ denote the optimal value functions of the true model
$\ALPHABET M$ and the approximate model $\widehat {\ALPHABET M}$,
respectively. Moreover, let $π^*$ and $\hat π^*$ be optimal policies for the
true model $\ALPHABET M$ and the approximate model $\widehat {\ALPHABET M}$,
respectively. 

We are interested in the following questions:

1. **Policy error bounds:** Given a policy $π$, what is the error if $\hat V^π$ is used as an approximation for $V^π$?

2. **Value error bounds:** What is the error if $\hat V^*$ is used
   as an approximation for $V^*$?

3. **Model error approximation:** What is the error if the policy $\hat
   π^*$ is used instead of the optimal policy $π^*$

## Bounds for model approximation

### Policy and value error bounds {#value-error}

Let $\BELLMAN^π$ and $\BELLMAN^*$ denote the Bellman operator for policy
$π$ and the optimality Bellman operator for model $\ALPHABET M$. Let $\hat
{\BELLMAN}^π$ and $\hat {\BELLMAN}^*$ denote the corresponding
quantities for model $\widehat {\ALPHABET M}$. Define the _Bellman mismatch
functionals_ $\MISMATCH^π$ and $\MISMATCH^*$ as follows: 
\begin{align*}
  \MISMATCH^π v &= \| \BELLMAN^π v - \hat {\BELLMAN}^π v \|_∞, 
  \\
  \MISMATCH^* v &= \| \BELLMAN^* v - \hat {\BELLMAN}^* v \|_∞ .
\end{align*}

Also define the _maximum Bellman mismatch_ as
\begin{align*}
  \MISMATCH^{\max} v &= 
  \max_{(s,a) \in \ALPHABET S, A} \biggl\lvert
    c(s,a) + γ \sum_{s' \in \ALPHABET S} P(s'|s,a)v(s')  \notag \\
  & \hskip 6em 
   -\hat c(s,a) - γ \sum_{s' \in \ALPHABET S} \hat P(s'|s,a) v(s')
   \biggr\rvert.
\end{align*}

:::{#lem-mismatch-bellman}
The following inequalities hold:

* $\sup_{π \in Π} \MISMATCH^π v = \MISMATCH^{\max} v$
* $\MISMATCH^* v \le \MISMATCH^{\max} v$.
:::

The Bellman mismatch functional can be used to bound the performance difference of
a policy between the true and approximate models.

:::{#prp-policy-error}
#### Policy error

For any (possibly randomized) policy $π$, 
\begin{equation}\label{eq:policy-error}
   \| V^{π} - \hat V^{π} \|_∞ \le 
   \frac{1}{1-γ} \min\{ \MISMATCH^π V^{π}, \MISMATCH^π \hat V^{π} \}. 
\end{equation}
:::

:::{.callout-note collapse="true"}
#### Proof {-}
We bound the left hand side of \\eqref{eq:policy-error} in two ways. The first way
is as follows:
\begin{align}
  \| V^{π} - \hat V^{π} \|_∞ 
  &=
  \| \BELLMAN^π V^π - \hat {\ALPHABET  B}^π \hat V^π \|_∞ 
  \notag \\
  &\le
  \| \BELLMAN^π V^π - \hat {\ALPHABET  B}^π V^π \|_∞ 
  + 
  \| \hat {\BELLMAN}^π V^π - \hat {\ALPHABET  B}^π \hat V^π \|_∞ 
  \notag \\
  &\le
  \MISMATCH^π V^π + γ \| V^π - \hat V^π \|_∞
  \label{eq:ineq-3}
\end{align}
where the first inequality follows from the triangle inequality, and the
second inequality follows from the definition of the Bellman mismatch functional
and the contraction property of Bellman operators. Rearranging terms
in \\eqref{eq:ineq-3} gives us
\begin{equation}
\| V^{π} - \hat V^{π} \|_∞ \le \frac{ \MISMATCH^π V^{π}}{1 - γ}.
\label{eq:ineq-4}\end{equation}
This gives the first bound.

The second bound is symmetric and obtained by interchanging the roles of $V^π$
and $\hat V^π$.
\begin{align}
  \| V^{π} - \hat V^{π} \|_∞ 
  &=
  \| \BELLMAN^π V^π - \hat {\ALPHABET  B}^π \hat V^π \|_∞ 
  \notag \\
  &\le
  \| \BELLMAN^π V^π - \ALPHABET  B^π \hat V^π \|_∞ 
  + 
  \| \BELLMAN^π \hat V^π - \hat {\ALPHABET  B}^π \hat V^π \|_∞ 
  \notag \\
  &\le
  γ \| V^π - \hat V^π \|_∞
  +
  \MISMATCH^π \hat V^π 
  \label{eq:ineq-13}
\end{align}
Rearranging terms in \\eqref{eq:ineq-13} gives
us
$$\begin{equation}
\| V^{π} - \hat V^{π} \|_∞ \le \frac{ \MISMATCH^π \hat V^{π}}{1 - γ}.
\label{eq:ineq-14}\end{equation}$$
This gives the second bound.
:::


Similar to the above, we can also bound the difference between the optimal
value function of the true and approximate models. 

:::{#prp-value-error}
#### Value error

Let $V^*$ and $\hat V^*$ denote the optimal value functions for $\ALPHABET
M$ and $\widehat {\ALPHABET M}$, respectively. Then,
\begin{equation}\label{eq:value-error}
    \| V^* - \hat V^* \|_∞ \le 
    \frac{1}{1-γ} \min\{ \MISMATCH^* V^*, \MISMATCH^* \hat V^* \} 
\end{equation}
:::

:::{.callout-note collapse="true"} 
#### Proof {-}
The proof argument is almost the same as the proof argument for
@prp-policy-error. The first was is as follows:
\begin{align}
  \| V^{*} - \hat V^{*} \|_∞ 
  &=
  \| \BELLMAN^* V^* - \hat {\ALPHABET  B}^* \hat V^* \|_∞ 
  \notag \\
  &\le
  \| \BELLMAN^* V^* - \hat {\ALPHABET  B}^* V^* \|_∞ 
  + 
  \| \hat {\BELLMAN}^* V^* - \hat {\ALPHABET  B}^* \hat V^* \|_∞ 
  \notag \\
  &\le
  \MISMATCH^* V^* + γ \| V^* - \hat V^* \|_∞
  \label{eq:ineq-1}
\end{align}
where the first inequality follows from the triangle inequality, and the
second inequality follows from the definition of the Bellman mismatch functional
and the contraction property of Bellman operators. Rearranging terms
in \\eqref{eq:ineq-1} gives us
\begin{equation}
\| V^* - \hat V^* \|_∞ \le \frac{  \MISMATCH^* V^*}{1 - γ}.
\label{eq:ineq-2}\end{equation}
This gives the first bound.

The second bound is symmetric and obtained by interchanging the roles of $V^*$
and $\hat V^*$.
\begin{align}
  \| V^{*} - \hat V^{*} \|_∞ 
  &=
  \| \BELLMAN^* V^* - \hat {\ALPHABET  B}^* \hat V^* \|_∞ 
  \notag \\
  &\le
  \| \BELLMAN^* V^* - \ALPHABET  B^* \hat V^* \|_∞ 
  + 
  \| \BELLMAN^* \hat V^* - \hat {\ALPHABET  B}^* \hat V^* \|_∞ 
  \notag \\
  &\le
  γ \| V^* - \hat V^* \|_∞
  +
  \MISMATCH^* \hat V^* 
  \label{eq:ineq-11}
\end{align}
Rearranging terms in \\eqref{eq:ineq-11} gives us
\begin{equation}
\| V^{*} - \hat V^{*} \|_∞ \le \frac{ \MISMATCH^* \hat V^{*}}{1 - γ}.
\label{eq:ineq-12}\end{equation}
This gives the second bound.
:::

### Model error

To bound the model error, we observe that from triangle inequality we have
\begin{equation} \label{eq:triangle-1}
  \| V^* - V^{\hat π^*} \|_∞ \le
  \| V^* - \hat V^{\hat π^*} \|_∞ 
  + 
  \| V^{\hat π^*} - \hat V^{\hat π^*} \|_∞.
\end{equation}

@prp-policy-error and @prp-value-error provide bounds of both of the terms of \eqref{eq:triangle-1}. Choosing appropriate values for both terms gives us the following

:::{#thm-model-error-hat-V-star}
#### Model error

The policy $\hat π^*$ is an $α$-optimal policy of $\ALPHABET M$ where
$$
    α := \| V^* - V^{\hat π^*} \|_∞ \le
    \frac{1}{1-γ} \bigl[ \MISMATCH^* \hat V^* + \MISMATCH^{\hat
    π^*} \hat V^* \bigr]. 
$$
Moreover, since $\MISMATCH^{\max} \hat V^*$ is an upper bound for both
$\MISMATCH^{\hat π^*} \hat V^*$ and $\MISMATCH^*
\hat V^*$, we have
$$
    α \le \frac{2}{(1-γ)}  \MISMATCH^{\max}  \hat V^*. 
$$
::::

In some applications, it is useful to have a model-error bound that depends on $V^*$ rather than $\hat V^*$. We provide such a bound below.

:::{#thm-model-error-V-star}
#### Model error

The policy $\hat π^*$ is an $α$-optimal policy of $\ALPHABET M$ where
$$
    α := \| V^* - V^{\hat π^*} \|_∞ \le
    \frac{1}{1-γ} \MISMATCH^{\hat π^*} V^*
    + 
    \frac{(1+γ)}{(1-γ)^2} \MISMATCH^* V^* .
$$

Moreover, since $\MISMATCH^{\max} V^*$ is an upper bound for
both $\MISMATCH^{\hat π^*} V^*$ and $\MISMATCH^* V^*$, we have
$$
    α \le \frac{2}{(1-γ)^2}  \MISMATCH^{\max}  V^*. 
$$
:::

:::{.callout-note collapse="true"}
#### Proof {-}
We bound the first term of \eqref{eq:triangle-1} by @prp-value-error
But instead of bounding the second term of \eqref{eq:triangle-1} by
@prp-policy-error, we consider the following:
\begin{align}
  \| V^{\hat π^*} - \hat V^{\hat π^*} \|_∞ 
  &= 
  \| V^{\hat π^*} - \hat V^{*} \|_∞ 
  = \| \BELLMAN^{\hat π^*} V^{\hat π^*} - 
       \hat {\BELLMAN}^{\hat π^*} \hat V^{*} \|_∞ 
  \notag \\
  &\le \| \BELLMAN^{\hat π^*} V^{\hat π^*} - 
          \BELLMAN^{\hat π^*} V^{*} \|_∞
    +  \| \BELLMAN^{\hat π^*} V^{*} -
       \hat {\BELLMAN}^{\hat π^*} V^{*} \|_∞ 
    + 
       \| \hat {\BELLMAN}^{\hat π^*} V^{*} - 
       \hat {\BELLMAN}^{\hat π^*} \hat V^{*} \|_∞ 
  \notag \\
  &\le γ \| V^* - V^{\hat π^*} \|_∞ + \MISMATCH^{\hat π^*} V^* 
  + γ \| V^* - \hat V^* \|_∞
  \label{eq:ineq-21}.
\end{align}
where the first inequality follows from the triangle inequality and the second
inequality follows from the definition of Bellman mismatch functional and
contraction property of Bellman operator. 

Substituting \eqref{eq:ineq-21} in \eqref{eq:triangle-1} and rearranging
terms, we get
\begin{align}
  \| V^* - V^{\hat π^*} \|_∞ 
  &\le
  \frac{1}{1-γ} \MISMATCH^{\hat π^*} V^*
  + 
  \frac{1+γ}{1-γ} \| V^* - \hat V^* \|_∞
  \notag \\
  &\le
  \frac{1}{1-γ} \MISMATCH^{\hat π^*} V^*
  + 
  \frac{(1+γ)}{(1-γ)^2} \MISMATCH^* V^* .
\end{align}
where the second inequality follows from @prp-value-error.
:::

:::{.callout-warning}
### Remark:
Note that the bound of @thm-model-error-V-star is tighter by a factor of $1/(1-γ)$ but that bound is in terms of $\hat V^*$. In some settings, 
a bound in terms of $V^*$ is more desirable. Using @thm-model-error-hat-V-star in such settings leads to scaling by $1/(1-γ)$. 
:::

<!--
## Example: State aggregation (or discretization or quantization)

Consider an MDP $\ALPHABET M = \langle \ALPHABET S, \ALPHABET A, p, c, γ
\rangle$, with large (or possibly continuous) state space. For simplicity, we
assume that $\ALPHABET S$ is continuous (and compact), and that the $p$ is the
density of the transition kernel. Note that we are using the term "probability
density" in the engineering sense (so, it may be a combination of a continuous
function and delta functions) rather than in the precise measure theoretic
sense (where it is the Radon-Nikodym derivative with respect to the Lebesgue
measure).

If exact computations were possible, we can find an optimal solution by
solving a dynamic program. However, since the state space is continuous, we
cannot compute the value functions exactly. The simplest way to proceed is to
discretize or quantize the state space $\ALPHABET S$. In particular, let
$\{\ALPHABET S_1, \dots \ALPHABET S_n\}$ denote a partition of $\ALPHABET S$
(i.e., $\bigcup_{i=1}^n \ALPHABET S_i = \ALPHABET S$ and for any $i \neq j$,
$\ALPHABET S_i \cap \ALPHABET S_j = \emptyset$). Pick a representative point
$\hat s_i \in \ALPHABET S_i$. We can think of the "grid points" $\hat
{\ALPHABET S} = \{\hat s_1, \dots, \hat s_n\}$ as quantization of the state
space $\ALPHABET S$. To simplify the notation, we define a quantization
function $\phi \colon \ALPHABET S \to \hat {\ALPHABET S}$ which maps all
points in $\ALPHABET S_i$ to the representative element $\hat s_i$. 

We consider a finite state MDP $\widehat {\ALPHABET M} = \langle \hat
{\ALPHABET S}, \ALPHABET A, \hat P, \hat c, γ\rangle$, where $\hat c$ is the
restriction of $c$ onto $\hat {\ALPHABET S}$, and $\hat P$ is given by 

$$\hat P(\hat s_j | \hat s_i, a) =
\int_{\ALPHABET S_j} P(s' | \hat s_i, a) ds' = P(\ALPHABET S_j | \hat s_i, a),
\quad \forall \hat s_i, \hat s_j \in \hat {\ALPHABET S}.
$$


Since model $\widehat {\ALPHABET M}$ is discrete, we can find the optimal
value function and policy using dynamic programming. 
We are interested in the questions of quantifying the errors in value
approximation and policy approximation in this setup. Since the quantized
model is defined on a state space $\hat {\ALPHABET S}$ that is different from
the original state space, the approximation bounds derived above are not
directly applicable. Below we present two intermediate models, to be able to
compare models $\ALPHABET M$ and $\widehat {\ALPHABET M}$

### An intermediate model

Define a model $\overline {\ALPHABET M} = \langle \ALPHABET S, \ALPHABET A, 
\bar p, \bar c, γ \rangle$, where
$$
  \bar c(s,a) = c(\phi(s),a)
$$
and
$$
  \bar p(s'|s,a) = p(s'|\phi(s),a).
$$

We have the following property for the $\bar p$ dynamics:

:::{#lem-aggregation-models}
For any $\hat V \colon \hat {\ALPHABET S} \to \reals$, let $V \colon
\ALPHABET S \to \reals$ be its piecewise constant extrapolation from
$\hat {\ALPHABET S}$ to $\ALPHABET S$ (i.e., $V = \hat V \circ \phi$).
Then, for any $\hat s \in \ALPHABET S$ and $a \in \ALPHABET A$, we have
$$
\int_{\ALPHABET S_j} \bar p(s'|\hat s, a) V(s') ds' 
=
\hat P(\hat s_j | \hat s, a) \hat V(\hat s_j).
$$
Therefore, 
$$
\int_{\ALPHABET S} \bar p(s'|\hat s, a) V(s') ds' 
=
\sum_{\hat s' \in \hat {\ALPHABET S}} \hat P(\hat s' | \hat s, a) \hat V(\hat s').
$$
:::

:::{.callout-note collapse="true"} 
#### Proof {-}
Since $V$ is constant over $\ALPHABET S_j$, we have
\begin{align*}
  \int_{\ALPHABET S_j} \bar p(s'|\hat s,a) V(s') ds' 
  &=
  V(\hat s_j) \int_{\ALPHABET S_j} \bar p(s'|\hat s,a) ds' 
  \notag \\
  &= V(\hat s_j) \bar p(\ALPHABET S_j | \hat s, a)
\end{align*}
The first result now follows from observing that $\bar p(\ALPHABET S_j | \hat
s, a) = p(\ALPHABET S_j | \hat s, a) = \hat P(\hat s_j | \hat s, a)$. The
second result follows from the fact that for any $f \colon \ALPHABET S \to
\reals$, 
$$
  \int_{\ALPHABET S} f(s')  ds' = \int_{\ALPHABET S_1} f(s') ds' + \cdots +
  \int_{\ALPHABET S_n} f(s') d(s').
$$
:::


Let $\bar V^*$, and $\hat V^*$ denote the optimal value functions of models
$\overline {\ALPHABET M}$ and $\widehat {\ALPHABET M}$, respectively.
Moreover, let $\bar π^*$ and $\hat π^*$ denote optimal policies of models
$\widehat {\ALPHABET M}$, and $\overline {\ALPHABET M}$, respectively. Then,
these are related as follows.

:::{#lem-aggregation-equiv}
$\bar V^*$ is a piecewise constant extrapolation of $\hat V^*$ from
$\hat {\ALPHABET S}$ to $\ALPHABET S$. Moreover, the piecewise constant
extrapolation of $\hat π^*$ is optimal for model $\overline {\ALPHABET
M}$. 
:::

:::{.callout-note collapse="true"}  
#### Proof {-}
Let $\bar V$ be the piecewise constant extrapolation of $\hat V^*$ from $\hat
{\ALPHABET S}$ to $\ALPHABET S$. @lem-aggregation-models implies that for any $s \in
\ALPHABET S$, the one step Bellman update of $\bar V$ (in model $\overline
{\ALPHABET M}$) is given by
\begin{align*}
  \min_{a \in \ALPHABET S} \biggl\{
    c(\phi(s),a) + γ \int_{\ALPHABET S} \bar p(s'|\phi(s),a) \bar V(s') ds'
  \biggr\}
  &= 
  \min_{a \in \ALPHABET S} \biggl\{
    \hat c(\phi(s),a) + γ \sum_{\hat s' \in \hat {\ALPHABET S}} 
    \hat P(\hat s' | \phi(s), a) \hat V^*(\hat s')
  \biggr\}
  \\
  &=
  \hat V^*(\phi(s)) 
  \\
  &= \bar V(s)
\end{align*}
where the first equality follows from @lem-aggregation-models, the second equality
follows from the fact that $\hat V^*$ is optimal for model $\widehat
{\ALPHABET M}$, and the last equality follows from the definition of $\bar V$.
Thus, $\bar V$ is the fixed point of the Bellman optimality equation for model
$\overline {\ALPHABET M}$. Hence, $\bar V = \bar V^*$. 

The above equation also implies that the policy $\hat π^*(\phi(s))$, which
achieves the arg min in the second equality, is optimal for model
$\overline{\ALPHABET M}$. 
:::

:::{.callout-tip}
### Remark
@lem-aggregation-equiv implies that the questions of value and policy
approximation for model $\widehat {\ALPHABET M}$ is the same as those for
model $\overline {\ALPHABET M}$. The advantage of working with model
$\overline {\ALPHABET M}$ is that it is defined on the same state space as
model $\ALPHABET M$, so we can directly use the results of @thm-mismatch-policy-bound and @thm-mismatch-policy-bound2.
:::

### Approximation bounds

Now to bound the value error between models $\ALPHABET M$ and $\overline
{\ALPHABET M}$, we need to compute $\MISMATCH^* V^*$, which is given by
\begin{align*}
  \MISMATCH^* V^* &= 
  \max_{s \in \ALPHABET S}
  \biggl| 
    \min_{a \in \ALPHABET A} \bigl\{ 
    c(s,a) + γ \int_{\ALPHABET S} p(s'|s,a) V^*(s')ds'
    \bigr\}
    \notag \\
  &  \hskip 4em -
    \min_{a \in \ALPHABET A} \bigl\{ 
   c(\phi(s), a) + γ \int_{\ALPHABET S} p(s'|\phi(s), a) V^*(s') ds' 
    \bigr\}
  \biggr|
  \notag \\
  &= \max_{s \in \ALPHABET S} \bigl| V^*(s) - V^*(\phi(s)) \bigr|
  =: H_{\max}.
\end{align*}

Therefore, from @thm-mismatch-value-bound, we have

:::{#prp-aggregation-value}
$$ \| V^* - \bar V^{\bar π^*} \|_∞ \le \frac{H_{\max}}{1-γ}. $$
:::

To bound the policy error, we also need to compute $\MISMATCH^{\hat π^*}
V^*$. In order to compute this bound, we assume that the model is $(L_c, L_p)$
Lipschitz. 


Assumpt. #ass:lip

:   * For every $a \in \ALPHABET A$, $c(s, a)$ is $L_c$-Lipschitz in $s$
    * For every $a \in \ALPHABET A$, $p(\cdot | s, a)$ is $L_p$-Lipschitz
      in $s$ (with respect to the Kantorovich distance on probability measures). 

Under this assumption, we have

\begin{align*}
  \MISMATCH^{\bar π^*} V^* &\le \max_{s \in \ALPHABET S}
  \sum_{a \in \ALPHABET A} \bar π^*(a | s) \biggl|
    c(s,a) + γ \int_{\ALPHABET S} p(s'|s,a) V^*(s')ds'
  \notag \\
  &  \hskip 4em -
    c(\phi(s), a) - γ \int_{\ALPHABET S} p(s'|\phi(s), a) V^*(s') ds' 
  \biggr|
  \notag \\
  &\le  L_c \max_{s \in \ALPHABET S}d(s, \phi(s)) 
  + 
  γ K(p(\cdot | s, a), p(\cdot | \phi(s), a)) \NORM{V^*}_L )
  \notag \\
  &\le ( L_c  + γ L_p \NORM{V^*}_L ) 
  \underbrace{\max_{s \in \ALPHABET S}d(s, \phi(s)) }_{=: D}
\end{align*}
where the second inequality uses @prp-Kantorovich.

Thus, we have the following:

::: highlight :::
Proposition #prop:policy-aggregation

:   Under @ass:lip, we have
    $$\NORM{V^* - V^{\bar π^*}}_∞ \le 
      \frac{D}{1-γ} \biggl[ L_c + γ L_p \NORM{V^*}_L + 
      \frac{1 + γ}{1-γ} \NORM{V^*}_L \biggr]. $$

    [LP]: ../lipschitz-mdp#thm:Lipschitz-opt

    Furthermore, if $γ L_p < 1$, then from [Theorem 1 of Lipschitz MDPs][LP]
    we know that $\NORM{V^*}_L \le L_c/(1- γ L_p)$. Thus,
    $$\NORM{V^{μ^*} - V^*}_∞ \le 
      \frac{2 D L_c }{ (1-γ)^2 (1-γ L_p) }. $$

:::

#### Proof {-}
<div>
The first bound follows from @thm:policy-bound2, with the additional
observation that $H_{\max} \le \NORM{V^*}_L D$. 

For the second result follows from simple algebra.
</div>
-->

## Notes {-}

The material in this section is adapted from @Bozkurt2023, where the results were presented for unbounded per-step cost.
