---
title: "Model approximation"
keywords:
  - infinite horizon
  - discounted cost
  - Lipschitz continuity
  - approximation bounds
  - state aggregation
---

There are many instances where approximate models are used. For example, in many applications with large state spaces, one often construct a simulation model of the system and use it to identify an optimal policy using simulation based methods (such as reinforcement learning). Often, the simulation model is only an approximation of the true model. In such instances, we want to know the error in using the policy obtained from the simulation model in the real world. In this section, we present bounds on such _sim-to-real_ transfer. 

Consider an MDP $\ALPHABET M = \langle \ALPHABET S, \ALPHABET A, P, c, γ
\rangle$. Suppose the components $\langle \ALPHABET S, \ALPHABET A, γ \rangle$
are known exactly but the components $(P,c)$ are known approximately. Consider the
approximate MDP $\widehat {\ALPHABET M} = \langle \ALPHABET S, \ALPHABET A,
\hat P, \hat c, γ \rangle$. We will call $\ALPHABET M$ to be the _true model_
and $\widehat {\ALPHABET M}$ to be the _approximate model_. 

Let $V^*$ and $\hat V^*$ denote the optimal value functions of the true model
$\ALPHABET M$ and the approximate model $\widehat {\ALPHABET M}$,
respectively. Moreover, let $π^*$ and $\hat π^*$ be optimal policies for the
true model $\ALPHABET M$ and the approximate model $\widehat {\ALPHABET M}$,
respectively. 

We are interested in two questions:

1. **Error in value approximation:** What is the error if $\hat V^*$ is used
   as an approximation for $V^*$?

2. **Error in policy approximation:** What is the error if the policy $\hat
   π^*$ is used instead of the optimal policy $π^*$

## Bounds for model approximation

### Bounding the error for value function approximation {#value-error}

Let $\BELLMAN^π$ and $\BELLMAN^*$ denote the Bellman operator for policy
$π$ and the optimality Bellman operator for model $\ALPHABET M$. Let $\hat
{\BELLMAN}^π$ and $\hat {\BELLMAN}^*$ denote the corresponding
quantities for model $\widehat {\ALPHABET M}$. Define the _Bellman mismatch
functionals_ $\MISMATCH^π$ and $\MISMATCH^*$ as follows: 
\begin{align*}
  \MISMATCH^π v &= \| \BELLMAN^π v - \hat {\BELLMAN}^π v \|_∞, 
  \\
  \MISMATCH^* v &= \| \BELLMAN^* v - \hat {\BELLMAN}^* v \|_∞ .
\end{align*}

Also define the _maximum Bellman mismatch_ as
\begin{align*}
  \MISMATCH^{\max} v &= 
  \max_{(s,a) \in \ALPHABET S, A} \biggl\lvert
    c(s,a) + γ \sum_{s' \in \ALPHABET S} P(s'|s,a)v(s')  \notag \\
  & \hskip 6em 
   -\hat c(s,a) - γ \sum_{s' \in \ALPHABET S} \hat P(s'|s,a) v(s')
   \biggr\rvert.
\end{align*}

:::{#lem-mismatch-bellman}
The following inequalities hold:

* $\sup_{π \in Π} \MISMATCH^π v = \MISMATCH^{\max} v$
* $\MISMATCH^* v \le \MISMATCH^{\max} v$.
:::

The Bellman mismatch functional can be used to bound the performance difference of
a policy between the true and approximate models.

:::{#lem-mismatch-policy-bound}
For any (possibly randomized) policy $π$, 
\begin{equation}\label{eq:bound-0}
   \| V^{π} - \hat V^{π} \|_∞ \le 
   \frac{1}{1-γ} \min\{ \MISMATCH^π V^{π}, \MISMATCH^π \hat V^{π} \}. 
\end{equation}
:::

:::{.callout-note collapse="true"}
#### Proof {-}
We bound the left hand side of \\eqref{eq:bound-0} in two ways. The first way
is as follows:
\begin{align}
  \| V^{π} - \hat V^{π} \|_∞ 
  &=
  \| \BELLMAN^π V^π - \hat {\ALPHABET  B}^π \hat V^π \|_∞ 
  \notag \\
  &\le
  \| \BELLMAN^π V^π - \hat {\ALPHABET  B}^π V^π \|_∞ 
  + 
  \| \hat {\BELLMAN}^π V^π - \hat {\ALPHABET  B}^π \hat V^π \|_∞ 
  \notag \\
  &\le
  \MISMATCH^π V^π + γ \| V^π - \hat V^π \|_∞
  \label{eq:ineq-3}
\end{align}
where the first inequality follows from the triangle inequality, and the
second inequality follows from the definition of the Bellman mismatch functional
and the contraction property of Bellman operators. Rearranging terms
in \\eqref{eq:ineq-3} gives us
\begin{equation}
\| V^{π} - \hat V^{π} \|_∞ \le \frac{ \MISMATCH^π V^{π}}{1 - γ}.
\label{eq:ineq-4}\end{equation}
This gives the first bound.

The second bound is symmetric and obtained by interchanging the roles of $V^π$
and $\hat V^π$.
\begin{align}
  \| V^{π} - \hat V^{π} \|_∞ 
  &=
  \| \BELLMAN^π V^π - \hat {\ALPHABET  B}^π \hat V^π \|_∞ 
  \notag \\
  &\le
  \| \BELLMAN^π V^π - \ALPHABET  B^π \hat V^π \|_∞ 
  + 
  \| \BELLMAN^π \hat V^π - \hat {\ALPHABET  B}^π \hat V^π \|_∞ 
  \notag \\
  &\le
  γ \| V^π - \hat V^π \|_∞
  +
  \MISMATCH^π \hat V^π 
  \label{eq:ineq-13}
\end{align}
Rearranging terms in \\eqref{eq:ineq-13} gives
us
$$\begin{equation}
\| V^{π} - \hat V^{π} \|_∞ \le \frac{ \MISMATCH^π \hat V^{π}}{1 - γ}.
\label{eq:ineq-14}\end{equation}$$
This gives the second bound.
:::


Similar to the above, we can also bound the difference between the optimal
value function of the true and approximate models. 

:::{#thm-mismatch-value-bound}
Let $V^*$ and $\hat V^*$ denote the optimal value functions for $\ALPHABET
M$ and $\widehat {\ALPHABET M}$, respectively. Then,
\begin{equation}\label{eq:bound-1}
    \| V^* - \hat V^* \|_∞ \le 
    \frac{1}{1-γ} \min\{ \MISMATCH^* V^*, \MISMATCH^* \hat V^* \} 
\end{equation}
:::

:::{.callout-note collapse="true"} 
#### Proof {-}
The proof argument is almost the same as the proof argument for
@lem-mismatch-policy-bound. The first was is as follows:
\begin{align}
  \| V^{*} - \hat V^{*} \|_∞ 
  &=
  \| \BELLMAN^* V^* - \hat {\ALPHABET  B}^* \hat V^* \|_∞ 
  \notag \\
  &\le
  \| \BELLMAN^* V^* - \hat {\ALPHABET  B}^* V^* \|_∞ 
  + 
  \| \hat {\BELLMAN}^* V^* - \hat {\ALPHABET  B}^* \hat V^* \|_∞ 
  \notag \\
  &\le
  \MISMATCH^* V^* + γ \| V^* - \hat V^* \|_∞
  \label{eq:ineq-1}
\end{align}
where the first inequality follows from the triangle inequality, and the
second inequality follows from the definition of the Bellman mismatch functional
and the contraction property of Bellman operators. Rearranging terms
in \\eqref{eq:ineq-3} gives us
\begin{equation}
\| V^* - \hat V^* \|_∞ \le \frac{  \MISMATCH^* V^*}{1 - γ}.
\label{eq:ineq-2}\end{equation}
This gives the first bound.

The second bound is symmetric and obtained by interchanging the roles of $V^*$
and $\hat V^*$.
\begin{align}
  \| V^{*} - \hat V^{*} \|_∞ 
  &=
  \| \BELLMAN^* V^* - \hat {\ALPHABET  B}^* \hat V^* \|_∞ 
  \notag \\
  &\le
  \| \BELLMAN^* V^* - \ALPHABET  B^* \hat V^* \|_∞ 
  + 
  \| \BELLMAN^* \hat V^* - \hat {\ALPHABET  B}^* \hat V^* \|_∞ 
  \notag \\
  &\le
  γ \| V^* - \hat V^* \|_∞
  +
  \MISMATCH^* \hat V^* 
  \label{eq:ineq-11}
\end{align}
Rearranging terms in \\eqref{eq:ineq-11} gives us
\begin{equation}
\| V^{*} - \hat V^{*} \|_∞ \le \frac{ \MISMATCH^* \hat V^{*}}{1 - γ}.
\label{eq:ineq-12}\end{equation}
This gives the second bound.
:::

### Bounding the error for policy approximation error 

:::{#thm-mismatch-policy-bound}
The policy $\hat π^*$ is an $α$-optimal policy of $\ALPHABET M$ where
$$\begin{equation} \label{eq:bound}
α := \| V^* - V^{\hat π^*} \|_∞ \le
\frac{1}{1 - γ}\bigl[ Δ_1 +Δ_2 \bigr]
\end{equation}$$
where
$$
 Δ_1 \le \min\{ \MISMATCH^* V^*, \MISMATCH^* \hat V^* \}
 \quad\text{and}\quad
 Δ_2 \le \min\{ \MISMATCH^{\hat π^*} V^{\hat π^*}, \MISMATCH^{\hat π^*}\hat V^{\hat π^*} \}.
$$
:::

:::{.callout-note collapse="true"}  
#### Proof {-}

From triangle inequality we have
$$\begin{equation} \label{eq:triangle-1}
  \| V^* - V^{\hat π^*} \|_∞ \le
  \| V^* - \hat V^{\hat π^*} \|_∞ 
  + 
  \| V^{\hat π^*} - \hat V^{\hat π^*} \|_∞.
\end{equation}$$
The result then follows by bounding the first term using @thm-mismatch-value-bound and
the second term using @lem-mismatch-policy-bound.
:::

Note that depending on how we pick $\MISMATCH_1$ and $\MISMATCH_2$, we get four possible upper
bounds and can bound $α$ by the tightest of them, i.e., 
\begin{equation}\label{eq:combined}
  α \le \frac{1}{1 - γ} \min\bigl\{
  \MISMATCH^* V^* + \MISMATCH^{\hat π^*} V^{\hat π^*}, \space 
  \MISMATCH^* V^* + \MISMATCH^{\hat π^*} \hat V^*, \space
  \MISMATCH^* \hat V^* + \MISMATCH^{\hat π^*} V^{\hat π^*}, \space 
  \MISMATCH^* \hat V^* + \MISMATCH^{\hat π^*} \hat V^{*} \bigr\}
\end{equation}
where we have used the fact that $\hat V^{\hat π^*} = \hat V^*$ to simplify
some of the terms. 

Ideally, we want an upper bound that just depends on the value function of
true model or just on the value function of the approximate model. The last
bound in \\eqref{eq:combined} depends just on the value function of the
approximate model. We state it separately for ease of reference.

:::{#cor-mismatch-combined-bound}
The policy $\hat π^*$ is an $α$-optimal policy of $\ALPHABET M$ where
$$
    α := \| V^* - V^{\hat π^*} \|_∞ \le
    \frac{1}{1-γ} \bigl[ \MISMATCH^* \hat V^* + \MISMATCH^{\hat
    π^*} \hat V^* \bigr]. 
$$
Moreover, since $\MISMATCH^{\max} \hat V^*$ is an upper bound for both
$\MISMATCH^{\hat π^*} \hat V^*$ and $\MISMATCH^*
\hat V^*$, we have
$$
    α \le \frac{2}{(1-γ)}  \MISMATCH^{\max}  \hat V^*. 
$$
::::

In the next lemma, we develop a bound that depends just on the value function true model. 

:::{#thm-mismatch-policy-bound2}
The policy $\hat π^*$ is an $α$-optimal policy of $\ALPHABET M$ where
$$
    α := \| V^* - V^{\hat π^*} \|_∞ \le
    \frac{1}{1-γ} \MISMATCH^{\hat π^*} V^*
    + 
    \frac{(1+γ)}{(1-γ)^2} \MISMATCH^* V^* .
$$

Moreover, since $\MISMATCH^{\max} V^*$ is an upper bound for
both $\MISMATCH^{\hat π^*} V^*$ and $\MISMATCH^* V^*$, we have
$$
    α \le \frac{2}{(1-γ)^2}  \MISMATCH^{\max}  V^*. 
$$
:::

:::{.callout-note collapse="true"}
#### Proof {-}
As before, we consider the following bound by triangle inequality.
\begin{equation} \label{eq:triangle-2}
  \| V^* - V^{\hat π^*} \|_∞ \le
  \| V^* - \hat V^{\hat π^*} \|_∞ 
  + 
  \| V^{\hat π^*} - \hat V^{\hat π^*} \|_∞.
\end{equation}
We will bound the first term of \\eqref{eq:triangle-2} by @thm:value-bound.
But instead of bounding the second term of \\eqref{eq:triangle-2} by
@lem:bound-policy, we consider the following:
\begin{align}
  \| V^{\hat π^*} - \hat V^{\hat π^*} \|_∞ 
  &= 
  \| V^{\hat π^*} - \hat V^{*} \|_∞ 
  = \| \BELLMAN^{\hat π^*} V^{\hat π^*} - 
       \hat {\BELLMAN}^{\hat π^*} \hat V^{*} \|_∞ 
  \notag \\
  &\le \| \BELLMAN^{\hat π^*} V^{\hat π^*} - 
          \BELLMAN^{\hat π^*} V^{*} \|_∞
    +  \| \BELLMAN^{\hat π^*} V^{*} -
       \hat {\BELLMAN}^{\hat π^*} V^{*} \|_∞ 
    + 
       \| \hat {\BELLMAN}^{\hat π^*} V^{*} - 
       \hat {\BELLMAN}^{\hat π^*} \hat V^{*} \|_∞ 
  \notag \\
  &\le γ \| V^* - V^{\hat π^*} \|_∞ + \MISMATCH^{\hat π^*} V^* 
  + γ \| V^* - \hat V^* \|_∞
  \label{eq:ineq-21}.
\end{align}
where the first inequality follows from the triangle inequality and the second
inequality follows from the definition of Bellman mismatch functional and
contraction property of Bellman operator. 

Substituting \\eqref{eq:ineq-21} in \\eqref{eq:triangle-2} and rearranging
terms, we get
\begin{align}
  \| V^* - V^{\hat π^*} \|_∞ 
  &\le
  \frac{1}{1-γ} \MISMATCH^{\hat π^*} V^*
  + 
  \frac{1+γ}{1-γ} \| V^* - \hat V^* \|_∞
  \notag \\
  &\le
  \frac{1}{1-γ} \MISMATCH^{\hat π^*} V^*
  + 
  \frac{(1+γ)}{(1-γ)^2} \MISMATCH^* V^* .
\end{align}
where the second inequality follows from @thm-mismatch-value-bound.
:::

:::{.callout-warning}
### Remark:
Note that the bound of @cor-mismatch-combined-bound is tighter by a factor of
$1/(1-γ)$, but this bound is in terms of $\hat V^*$. In some settings, we
prefer a bound in terms of $V^*$. Using @thm-mismatch-policy-bound2 in such
settings leads to scaling by $1/(1-γ)$. 
:::


## Example: State aggregation (or discretization or quantization)

Consider an MDP $\ALPHABET M = \langle \ALPHABET S, \ALPHABET A, p, c, γ
\rangle$, with large (or possibly continuous) state space. For simplicity, we
assume that $\ALPHABET S$ is continuous (and compact), and that the $p$ is the
density of the transition kernel. Note that we are using the term "probability
density" in the engineering sense (so, it may be a combination of a continuous
function and delta functions) rather than in the precise measure theoretic
sense (where it is the Radon-Nikodym derivative with respect to the Lebesgue
measure).

If exact computations were possible, we can find an optimal solution by
solving a dynamic program. However, since the state space is continuous, we
cannot compute the value functions exactly. The simplest way to proceed is to
discretize or quantize the state space $\ALPHABET S$. In particular, let
$\{\ALPHABET S_1, \dots \ALPHABET S_n\}$ denote a partition of $\ALPHABET S$
(i.e., $\bigcup_{i=1}^n \ALPHABET S_i = \ALPHABET S$ and for any $i \neq j$,
$\ALPHABET S_i \cap \ALPHABET S_j = \emptyset$). Pick a representative point
$\hat s_i \in \ALPHABET S_i$. We can think of the "grid points" $\hat
{\ALPHABET S} = \{\hat s_1, \dots, \hat s_n\}$ as quantization of the state
space $\ALPHABET S$. To simplify the notation, we define a quantization
function $\phi \colon \ALPHABET S \to \hat {\ALPHABET S}$ which maps all
points in $\ALPHABET S_i$ to the representative element $\hat s_i$. 

We consider a finite state MDP $\widehat {\ALPHABET M} = \langle \hat
{\ALPHABET S}, \ALPHABET A, \hat P, \hat c, γ\rangle$, where $\hat c$ is the
restriction of $c$ onto $\hat {\ALPHABET S}$, and $\hat P$ is given by 

$$\hat P(\hat s_j | \hat s_i, a) =
\int_{\ALPHABET S_j} P(s' | \hat s_i, a) ds' = P(\ALPHABET S_j | \hat s_i, a).
$$


Since model $\widehat {\ALPHABET M}$ is discrete, we can find the optimal
value function and policy using dynamic programming. 
We are interested in the questions of quantifying the errors in value
approximation and policy approximation in this setup. Since the quantized
model is defined on a state space $\hat {\ALPHABET S}$ that is different from
the original state space, the approximation bounds derived above are not
directly applicable. Below we present two intermediate models, to be able to
compare models $\ALPHABET M$ and $\widehat {\ALPHABET M}$

### An intermediate model

Define a model $\overline {\ALPHABET M} = \langle \ALPHABET S, \ALPHABET A, 
\bar p, \bar c, γ \rangle$, where
$$
  \bar c(s,a) = c(\phi(s),a)
$$
and
$$
  \bar p(s'|s,a) = p(s'|\phi(s),a).
$$

We have the following property for the $\bar p$ dynamics:

:::{#lem-aggregation-models}
For any $\hat V \colon \hat {\ALPHABET S} \to \reals$, let $V \colon
\ALPHABET S \to \reals$ be its piecewise constant extrapolation from
$\hat {\ALPHABET S}$ to $\ALPHABET S$ (i.e., $V(s) = \hat V(\phi(s)))$.
Then, for any $\hat s \in \ALPHABET S$ and $a \in \ALPHABET A$, we have
$$
\int_{\ALPHABET S_j} \bar p(s'|\hat s, a) V(s') ds' 
=
\hat P(\hat s_j | \hat s, a) \hat V(\hat s_j).
$$
Therefore, 
$$
\int_{\ALPHABET S} \bar p(s'|\hat s, a) V(s') ds' 
=
\sum_{\hat s' \in \hat {\ALPHABET S}} \hat P(\hat s' | \hat s, a) \hat V(\hat s_j).
$$
:::

:::{.callout-note collapse="true"} 
#### Proof {-}
Since $V$ is constant over $\ALPHABET S_j$, we have
\begin{align*}
  \int_{\ALPHABET S_j} \bar p(s'|\hat s,a) V(s') ds' 
  &=
  V(\hat s_j) \int_{\ALPHABET S_j} \bar p(s'|\hat s,a) ds' 
  \notag \\
  &= V(\hat s_j) \bar p(\ALPHABET S_j | \hat s, a)
\end{align*}
The first result now follows from observing that $\bar p(\ALPHABET S_j | \hat
s, a) = p(\ALPHABET S_j | \hat s, a) = \hat P(\hat s_j | \hat s, a)$. The
second result follows from the fact that for any $f \colon \ALPHABET S \to
\reals$, 
$$
  \int_{\ALPHABET S} f(s')  ds' = \int_{\ALPHABET S_1} f(s') ds' + \cdots +
  \int_{\ALPHABET S_n} f(s') d(s').
$$
:::


Let $\bar V^*$, and $\hat V^*$ denote the optimal value functions of models
$\overline {\ALPHABET M}$ and $\widehat {\ALPHABET M}$, respectively.
Moreover, let $\bar π^*$ and $\hat π^*$ denote optimal policies of models
$\widehat {\ALPHABET M}$, and $\overline {\ALPHABET M}$, respectively. Then,
these are related as follows.

:::{#lem-aggregation-equiv}
$\bar V^*$ is a piecewise constant extrapolation of $\hat V^*$ from
$\hat {\ALPHABET S}$ to $\ALPHABET S$. Moreover, the piecewise constant
extrapolation of $\hat π^*$ is optimal for model $\overline {\ALPHABET
M}$. 
:::

:::{.callout-note collapse="true"}  
#### Proof {-}
Let $\bar V$ be the piecewise constant extrapolation of $\hat V^*$ from $\hat
{\ALPHABET S}$ to $\ALPHABET S$. @lem-aggregation-models implies that for any $s \in
\ALPHABET S$, the one step Bellman update of $\bar V$ (in model $\overline
{\ALPHABET M}$) is given by
\begin{align*}
  \min_{a \in \ALPHABET S} \biggl\{
    c(\phi(s),a) + γ \int_{\ALPHABET S} \bar p(s'|\phi(s),a) \bar V(s') ds'
  \biggr\}
  &= 
  \min_{a \in \ALPHABET S} \biggl\{
    \hat c(\phi(s),a) + γ \sum_{\hat s' \in \hat {\ALPHABET S}} 
    \hat P(\hat s' | \phi(s), a) \hat V^*(\hat s')
  \biggr\}
  \\
  &=
  \hat V^*(\phi(s)) 
  \\
  &= \bar V(s)
\end{align*}
where the first equality follows from @lem-aggregation-models, the second equality
follows from the fact that $\hat V^*$ is optimal for model $\widehat
{\ALPHABET M}$, and the last equality follows from the definition of $\bar V$.
Thus, $\bar V$ is the fixed point of the Bellman optimality equation for model
$\overline {\ALPHABET M}$. Hence, $\bar V = \bar V^*$. 

The above equation also implies that the policy $\hat π^*(\phi(s))$, which
achieves the arg min in the second equality, is optimal for model
$\overline{\ALPHABET M}$. 
:::

:::{.callout-tip}
### Remark
@lem-aggregation-equiv implies that the questions of value and policy
approximation for model $\widehat {\ALPHABET M}$ is the same as those for
model $\overline {\ALPHABET M}$. The advantage of working with model
$\overline {\ALPHABET M}$ is that it is defined on the same state space as
model $\ALPHABET M$, so we can directly use the results of @thm-mismatch-policy-bound and @thm-mismatch-policy-bound2.
:::

### Approximation bounds

Now to bound the value error between models $\ALPHABET M$ and $\overline
{\ALPHABET M}$, we need to compute $\MISMATCH^* V^*$, which is given by
\begin{align*}
  \MISMATCH^* V^* &= 
  \max_{s \in \ALPHABET S}
  \biggl| 
    \min_{a \in \ALPHABET A} \bigl\{ 
    c(s,a) + γ \int_{\ALPHABET S} p(s'|s,a) V^*(s')ds'
    \bigr\}
    \notag \\
  &  \hskip 4em -
    \min_{a \in \ALPHABET A} \bigl\{ 
   c(\phi(s), a) + γ \int_{\ALPHABET S} p(s'|\phi(s), a) V^*(s') ds' 
    \bigr\}
  \biggr|
  \notag \\
  &= \max_{s \in \ALPHABET S} \bigl| V^*(s) - V^*(\phi(s)) \bigr|
  =: H_{\max}.
\end{align*}

Therefore, from @thm-mismatch-value-bound, we have

:::{#prp-aggregation-value}
$$ \| V^* - \bar V^{\bar π^*} \|_∞ \le \frac{H_{\max}}{1-γ}. $$
:::

To bound the policy error, we also need to compute $\MISMATCH^{\hat π^*}
V^*$. In order to compute this bound, we assume that the model is $(L_c, L_p)$
Lipschitz. 


Assumpt. #ass:lip

:   * For every $a \in \ALPHABET A$, $c(s, a)$ is $L_c$-Lipschitz in $s$
    * For every $a \in \ALPHABET A$, $p(\cdot | s, a)$ is $L_p$-Lipschitz
      in $s$ (with respect to the Kantorovich distance on probability measures). 

Under this assumption, we have

\begin{align*}
  \MISMATCH^{\bar π^*} V^* &\le \max_{s \in \ALPHABET S}
  \sum_{a \in \ALPHABET A} \bar π^*(a | s) \biggl|
    c(s,a) + γ \int_{\ALPHABET S} p(s'|s,a) V^*(s')ds'
  \notag \\
  &  \hskip 4em -
    c(\phi(s), a) - γ \int_{\ALPHABET S} p(s'|\phi(s), a) V^*(s') ds' 
  \biggr|
  \notag \\
  &\le  L_c \max_{s \in \ALPHABET S}d(s, \phi(s)) 
  + 
  γ K(p(\cdot | s, a), p(\cdot | \phi(s), a)) \NORM{V^*}_L )
  \notag \\
  &\le ( L_c  + γ L_p \NORM{V^*}_L ) 
  \underbrace{\max_{s \in \ALPHABET S}d(s, \phi(s)) }_{=: D}
\end{align*}
where the second inequality uses @prp-Kantorovich.

Thus, we have the following:

::: highlight :::
Proposition #prop:policy-aggregation

:   Under @ass:lip, we have
    $$\NORM{V^* - V^{\bar π^*}}_∞ \le 
      \frac{D}{1-γ} \biggl[ L_c + γ L_p \NORM{V^*}_L + 
      \frac{1 + γ}{1-γ} \NORM{V^*}_L \biggr]. $$

    [LP]: ../lipschitz-mdp#thm:Lipschitz-opt

    Furthermore, if $γ L_p < 1$, then from [Theorem 1 of Lipschitz MDPs][LP]
    we know that $\NORM{V^*}_L \le L_c/(1- γ L_p)$. Thus,
    $$\NORM{V^{μ^*} - V^*}_∞ \le 
      \frac{2 D L_c }{ (1-γ)^2 (1-γ L_p) }. $$

:::

<details><!--{{{ Proof -->
<summary>
#### Proof {-}
</summary>
<div>
The first bound follows from @thm:policy-bound2, with the additional
observation that $H_{\max} \le \NORM{V^*}_L D$. 

For the second result follows from simple algebra.
</div>
</details><!--}}}-->

