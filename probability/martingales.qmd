---
title: Martingales
---

Consider a probability space $(Ω, \mathcal F, P)$. We define a few key terms:

1. A **filteration** $\{\mathcal F_t\}_{t \ge 1}$ is an increasing familty of sub-sigma-fields of $\mathcal F$, i.e., for any $t_1 < t_2$, $\mathcal F_{t_1} \subseteq \mathcal F_{t_2}$. 

2. The most common way of generating a filtration is through a stochastic process. In particular, given a stochastic process $\{X_t\}_{t \ge 1}$, the filtration $\{\mathcal F_t\}_{t \ge 1}$ given by
   $$
     \mathcal F_t = σ(X_{1:t}), \quad t \ge 1
   $$
   is called the **filtration generated by $\{X_t\}_{t \ge 1}$** or the **natural filtration** of $\{X_t\}_{t \ge 1}$. 

3. A family of integrable random variables $\{X_t\}_{t \ge 1}$ is said to be **adapted** with respect to the filtration $\{\mathcal F_t\}_{t \ge 1}$ if $X_t$ is $\mathcal F_t$-measurable for each $t$.

4. A family of integrable random variables $\{X_t\}_{t \ge 1}$ is said to be **predictable** with respect to the filtration $\{\mathcal F_t\}_{t \ge 1}$ if $X_t$ is $\mathcal F_{t-1}$-measurable for each $t$.

:::{.callout-tip}
### Intuition
Intuitively, a process $\{X_t\}_{t \ge 1}$ is adapted with respect to the filtration $\{\mathcal F_t\}_{t \ge 1}$ the value of $X_t$ is fully known at time $t$ based on the information $\mathcal F_t$.
:::

:::{#def-martingale}
A family of integrable random vabiables $\{X_t\}_{t \ge 1}$ adapted to a filtration $\{\mathcal F_t\}_{t \ge 1}$ is said to be a **martingale** if
\begin{equation}\label{eq:martingale}
  X_s = \EXP[X_{t} \mid \mathcal F_s], \quad \forall s < t. 
\end{equation}
:::

:::{.callout-tip}
### Intuition
Property \\eqref{eq:martingale} has the interpretation that $X_s$ is the best predictor for $X_t$ based on the information available at time $s$. 

The condition $\EXP[X_{t+1} \mid \mathcal F_t] = X_t$ may be thought of as a "fair-game" requirement. If we are playing a fair game, then we expect neither to win nor to lose moeny on average. Given the history of our fortunes up to time $s$, our expected fortune $X_{t}$ at a future time $t$ should just be the fortune $X_s$ that we have at time $s$. 
:::

:::{.callout-important}
### Remark
In the definition of a martingale (and its variants that we will define below), it is assumed that the random variables $\{X_t\}_{t \ge 1}$ are indegrable, i.e., $\EXP[|X_t|] < ∞$ for all $t$. Therefore, the conditional expectations used in the definitions are well defined.
:::

Several desirable properties of martingales are shared by families of random variables for which \\eqref{eq:martingale} is replaced by an inequality. 

1. A family of integrable random variables $\{X_t\}_{t \ge 1}$ adapted to a filtration $\{\mathcal F_t\}_{t \ge 1}$ is said to be a **submartingale** if 
  \begin{equation}\label{eq:submartingale}
    X_s \le \EXP[X_{t} \mid \mathcal F_s], \quad \forall s < t. 
  \end{equation}

2. A family of integrable random variables $\{X_t\}_{t \ge 1}$ adapted to a filtration $\{\mathcal F_t\}_{t \ge 1}$ is said to be a **supermartingale** if 
  \begin{equation}\label{eq:supermartingale}
    X_s \ge \EXP[X_{t} \mid \mathcal F_s], \quad \forall s < t. 
  \end{equation}

:::{.callout-warning}
### A note about the names
Note the definitions of the inequalities:
\begin{alignat*}{2}
  \text{supermartingale:} &\quad & X_t &\ge \EXP[X_t \mid \mathcal F_t] \\
  \text{submartingale:}   &\quad& X_t &\le \EXP[X_t \mid \mathcal F_t] 
\end{alignat*}
Thus, at each time, a *sub*martingale is *below* its future expected value, whereas a *supermartingale* is *above* its future expected value. 
:::

:::{.callout-tip}
### Remark
If $\{X_t\}_{t \ge 1}$ is a submartingale, then $\{-X_t\}_{t \ge 1}$ is a supermartingale. This property allows us to translate results between submartingales and supermartingales.
:::

## Examples and Immediate Properties

1. **Martingales generalize the theory of sums of independent random variables.**
   Let $ξ_1, ξ_2, \dots$ be indepdent, integrable random variables. Define $X_0 = 0$ and $X_t = ξ_1 + \dots + ξ_t$. If $\EXP[X_t] = 0$ for $t \ge 1$, then the sequence $\{X_t\}_{t \ge 1}$ is a martingale with respect to the natural filtration:
   $$
   \EXP[X_{t+1} \mid X_{1:t}] = \EXP[X_t + ξ_{t+1} \mid X_{1:t}] = X_t.
   $$

   Similarly, if $ξ_t$ has positive mean, $\{X_t\}_{t \ge 1}$ is a submartingale, and if $ξ_t$ has negative mean, $\{X_t\}_{t \ge 1}$ is a supermartingale.

2. Let $\{X_t\}_{t \ge 1}$ be martingale and let $f$ be a convex function for which each $f(X_t)$ is integrable. Then, a direct application of Jensen's inequality implies that $\{f(X_t)\}_{t \ge 1}$ is a submartingale. 

3. A consequence of the previous result is the following. Let $\{X_t\}_{t \ge 1}$ be a martingale and $p \ge 1$ be such that $|X_t|^p$ is integrable for all $t$. Then $\{|X_t|^p\}_{t \ge 1}$ is a submartingale.

4. Let $\{X_t\}_{t \ge 1}$ be **sub**martingale and let $f$ be a convex **and increasing** function for which each $f(X_t)$ is integrable. Then, $\{f(X_t)\}_{t \ge 1}$ is a submartingale because
   $$
      \EXP[ f(X_{t+1}) \mid \mathcal F_t] 
      \ge
      f( \EXP[ X_{t+1} \mid \mathcal F_t ])
      \ge
      f(X_t)
   $$
   where the first inequality follows from Jensen's inequality and the second from the fact that $\{X_t\}_{t \ge 1}$ is a sub-martingale and $f$ is increasing.

5. A consequence of the previous result is the following. Let $\{X_t\}_{t \ge 1}$ be a submartingale. Then, for any constant $a$,  $\{(X_t - a)^+\}_{t \ge 1}$ is also a submartingale.

6. If $\{X_t\}_{t \ge 1}$ and $\{Y_t\}_{t \ge 1}$ are martingales defined on the same family of $σ$-algebras, then $\{a X_t + b Y_t\}_{t \ge 1}$ is a martingale. 

7. The previous result holds for supermartingales provided $a$ and $b$ are positive. 

8. If $\{X_t\}_{t \ge 1}$ and $\{Y_t\}_{t \ge 1}$ are supermartingales defined on the same family of $σ$-algebras, then $\{X_t \wedge Y_t\}_{t \ge 1}$ is a supermartingale. 

9. Suppose $X$ is an integrable random variable and $\{\mathcal F_t\}_{t \ge 1}$ is a filtration. Define $X_t \coloneqq \EXP[X \mid \mathcal F_t]$. Then $\{X_t\}_{t \ge 1}$ is a martingale with respect to the filtration (follows from the smoothing property of conditional expectations).

10. **Likelihood ratios are martingales.** Suppose $X_1$, $X_2$, \dots are independent random variables with density $f$. Imagine we are considering the alternative hypothesis that these random variables are independent with a different probability density $g$ (but they are *really* distributed according to the density $f$). We assume that $f$ and $g$ have the same support. Define the likelihood ratio
   $$
      Λ_t = \frac{g(X_1)}{f(X_1)} \dots \frac{g(X_t)}{f(X_t)}.
   $$
   Then, since $Λ_{t+1} = Λ_t g(X_{t+1})/f(X_{t+1})$, we have
   \begin{align*}
   \EXP[Λ_{t+1} \mid X_{1:t}] 
   &= Λ_t \EXP\biggl[ \frac{g(X_{t+1})}{f(X_{t+1})} \biggm| X_{1:t} \biggr] \\
   &= Λ_t \EXP\biggl[ \frac{g(X_{t+1})}{f(X_{t+1})} \biggr] \\
   &= Λ_t \int\biggl( \frac{g(x)}{f(x)} \biggr) f(x)dx \\
   &= Λ_t \int g(x)dx \\
   &= Λ_t.
  \end{align*}
  Hence, $\{Λ_t\}_{t \ge 1}$ is a martingale.

11. A martingale $\{X_t\}_{t \ge 1}$ may be written as a sum of increments: $X_t = X_0 + ξ_1 + \dots + ξ_t$. The increments $\{ξ_t\}_{t \ge 1}$ are called **martingale differences**. Each $ξ_t$ is integrable and $\EXP[ξ_t \mid \mathcal F_{t-1}] = 0$ for all $t$.


## Doob's decomposition

:::{#thm-doob-decomposition}
### Doob's decomposition
  Every sequence $\{X_t\}_{t \ge 1}$ of integrable random variables adapted to a filtration $\{\mathcal F_t\}_{t \ge 1}$ can be decomposed into a sum of a martingale and an integrable predictable process, i.e., 
  $$
    X_t = M_t + A_t
  $$
  where $\{M_t\}_{t \ge 1}$ is a martingale and $\{A_t\}_{t \ge 1}$ is a predictable process 
:::

1. **Existence.** We can identify the Doob's decomposition by defining
   $$ M_t = X_0 + \sum_{s=1}^{t}\bigl( X_s - \EXP[ X_s \mid \mathcal F_{s-1} \bigr],$$
   and 
   $$A_t = \sum_{s=1}^t \bigl( \EXP[ X_s \mid \mathcal F_{s-1} ] - X_{s-1} \bigr).$$

2. **Uniqueness.** The decomposition is almost surely unique. If $X_t = M'_t + A'_t$ is another decomposition, then $Y_t = M_t - M'_t$ being the difference of martingales is a martingale. But $M_t - M'_t = A_t - A'_t$, which is a predictable process; combining this with the martingale property of $M_t - M'_t$ implies that
   $$
   A_t - A'_t = \EXP[ A_t - A'_t \mid \mathcal F_t] = 
   \EXP[ M_t - M'_t \mid \mathcal F_t ] = M_{t-1} - M'_{t-1} = A_{t-1} - A'_{t-1}.
   $$
   Since $A_0 = A'_0 = 0$, we get that $A_t = A'_t$ for all $t \ge 1$. 

3. A stochastic process $\{X_t\}_{t \ge 1}$ is a submartingale if and only if its Doob's decomposition gives a predictable process that is almost surely increasing. 

4. A stochastic process $\{X_t\}_{t \ge 1}$ is a supermartingale if and only if its Doob's decomposition gives a predictable process that is almost surely decreasing 

## Convergence of nonnegative supermartingales

We state certain properties of nonnegative supermartingales without proof. This material is taken from @Neveu1975.

1. **Maximal inequality for nonnegative supermartingales.**
For every nonnegative supermartingale $\{X_t\}_{t \ge 1}$, the random variable $\sup_{t \ge 1} X_t$ is a.s.\ finite on the set $\{X_0 < ∞\}$ and, more precisely, satisfies the following inequality:
$$
  \PR\Bigl( \sup_{t \ge 1} X_t \ge a \Bigm| \mathcal F_t \Bigr)
  \le
  \max\biggl( \frac{X_0}{a}, 1 \biggr)
$$
2. **Switching principle for supermartingales.** Given two nonnegative supermartingales $\{X^{(i)}_t\}_{t \ge 1}$, $i \in \{1, 2\}$, and a stopping time $τ$ such that $X^{(1)}_{τ} \ge X^{(2)}_{τ}$ on $\{τ < ∞\}$, the formula
   $$
    X_t(ω) = \begin{cases}
    X^{(1)}_t(ω), & \text{if $t < τ(ω)$} \\
    X^{(2)}_t(ω), & \text{if $t \ge τ(ω)$}
    \end{cases}
    $$
    defines a new nonnegative supermartingale.

3. **Supermartingale convergence theorem.** Every nonnegative supermartingale $\{X_t\}_{t \ge 1}$ converges almost surely. Furthermore, the limit $X_{∞} = \lim_{t \to ∞} X_t$ satisfies the inequality
   $$
     \EXP[ X_{∞} \mid \mathcal F_t] \le X_t.
   $$

4. A nonnegative supermartingale converges in $\mathcal L^1$ to its limit $X_{∞}$ if and only if $\EXP[X_t] \to \EXP[X_{∞}]$.

5. For a nonnegative supermartingale $\{X_t\}_{t \ge 1}$ and a stopping time $τ$, the sequence $\{X_{τ \wedge t}\}_{t \ge 1}$ "stopped at the stopping time $τ$" is also a non-negative supermartingale.

6. **Stopping theorem** Let $\{X_t\}_{t \ge 1}$ be a nonnegative supermartingale. Then, for any _bounded_ stopping times $τ_1$ and $τ_2$, we have
   $$
    X_{τ_1} \ge \EXP[ X_{τ_2} \mid \mathcal F_{τ_1}],
    \quad \text{ on } \{τ_1 \le τ_2 \}.
   $$

:::{.callout-important collapse="false"}
### Interpretation of supermartingales

The following interpretation is taken from @Dellacherie1982b.

Suppose the random variable $X_t$ represents a gambler's fortune at time $t$. Then his successive gains are presented by the random variable $ξ_t = X_t - X_{t-1}$. The gambler may be in an arbitrarily complicated casino, where he may choose between all sorts of games, move from one table to another, bet on other player's fortunes, etc., but it is understood that his decisions are unprophetic, i.e., they can only be taken as functions for the _past_ and not as functions of the future, with the convention that the present, i.e., the game which is in the process of being played, forms part of the future. 

The supermartingale inequality $\EXP[ξ_t \mid \ALPHABET F_t] \le 0$ means that, whatever decisions are taken by the gambler just before the $t$-th game, the _average_ profits from that will be negative. In other words, the game favors the casino—which is what happens in reality! (The martingale equality corresponds to the case of an equitable casino)

Now imagine that the gambler, fearing that he is under the influence of an evil star, confides his fortune to a "luckier" (but still unprophetic)
friend and goes out for fresh air and returns at random times $τ_1 <  τ_2 <  \dots$. The stopping theorem says that **what he observes at these random instances is also a game favorable to the casino** (or merely equitable in case of martingales). In other words, things are no better. 

The restriction on the length of the stopping time has the following meaning: suppose the gambler tells his friend to "call me at the first moment $τ_1$ when my gain $X_{τ_1} - X_0$ is positive. Then call me again at time $τ_2$ when my gain $X_{τ_2} - X_{τ_1}$ is positive. And so on" At such moments, mean gain is also positive and hence stopping theorem seems to be contradicted; however, the stopping theorem affirms that the stopping times $τ_1$, $τ_2$, etc., are not finite. 

:::

The converges result continues to hold for "almost" supermartingales.

:::{#thm-almost-supermartingale}
## Almost supermartingale convergence theorem
Suppose $\{X_t\}_{t \ge 1}$, $\{β_t\}_{t \ge 1}$, $\{Y_t\}_{t \ge 1}$, $\{Z_t\}_{t \ge 1}$ are nonnegative stochastic processes adapted to some filtration $\{\mathcal F_t\}_{t \ge 1}$ that satisfy
\begin{equation}\label{eq:almost-supermartingale}
  \EXP[X_{t+1} \mid \mathcal F_t ] \le 
  (1 + β_t) X_t + Y_t - Z_t, 
  \quad t \ge 1
\end{equation}
Define the set $Ω_0$ by
$$
  Ω_0 = \biggl\{ ω : \sum_{t \ge 1} β_t(ω) < ∞ \biggr\}
  \cap
  \biggl\{ ω : \sum_{t \ge 1} Y_t(ω) < ∞ \biggr\}.
$$
Then, for all $ω \in Ω_0$, we have that

(i) $\lim_{t \to ∞} X_t(ω)$ exists and is finite
(ii) $\sum_{t \ge 1} Z_t(ω) < ∞$.
:::

:::{.callout-note collapse="true"} 
### Proof
 The key idea is that we can fiddle with \\eqref{eq:almost-supermartingale} to make it a supermartingale. Let $b_t = 1/\prod_{\tau = 1}^{t-1} ( 1 + β_{τ} )$. Note that $\{b_t\}_{t \ge 1}$ is $\{\mathcal F_t\}_{t \ge 1}$ predictable. Define the nonnegative processes $\{X'_t\}_{t \ge 1}$ $\{Y'_t\}_{t \ge 1}$ and $\{Z'_t\}_{t \ge 1}$, where 
$$
X'_t = b_t X_t,
\quad
Y'_t = b_{t+1} Y_t,
\quad
Z'_t = b_{t+1} Z_t.
$$
\begin{align}
    \EXP[X'_{t+1} \mid \mathcal F_t] &= 
    b_{t+1} \EXP[ X_{t+1} \mid \mathcal F_t] \notag \\
    &\le b_{t+1}(1 + β_t) X_t + b_{t+1} Y_t  - b_{t+1} Z_t \notag \\
    &= X'_t + Y'_t - Z'_t.
    \label{eq:almost-supermartingale-st-1}
\end{align}

Define
\begin{equation}\label{eq:almost-supermartingale-st-2}
W_t = X'_t - \sum_{s = 1}^{t-1} (Y'_s - Z'_s) .
\end{equation}
From \\eqref{eq:almost-supermartingale-st-1} we have
$$
\EXP[W_{t+1} \mid \mathcal F_t] = 
\EXP\biggl[ X'_{t+1} - \sum_{s=1}^t(Y'_s - Z'_s) \biggm| \mathcal F_t \biggr] 
\le X_t - \sum_{s=1}^{t-1} (Y'_s - Z'_s) 
= W_t.
$$
Therefore, $\{W_t\}_{t \ge 1}$ is a supermartingale, but we cannot immediately apply the supermartingale convergence theorem because we don't know that $W_t$ is bounded from below. 


For an arbitrary $a > 0$, define the stopping time $τ = \inf \bigl\{ t : \sum_{s=1}^{t} Y'_s > a \bigr\}.$ Then, the stopped sequence $\{W_{τ \wedge t}\}_{t \ge 1}$ is also a supermartingale because
$$
\EXP[ W_{τ \wedge (t+1)} \mid \mathcal F_t ]
= W_{\tau} \IND\{ τ \le t \} + \EXP[ W_{t+1} \mid \mathcal F_t ] \IND\{ τ > t\}
\le W_{τ \wedge t}.
$$
Moreover, $W_{τ \wedge t}$ is bounded from below since
$$
W_{t \wedge τ} \ge - \sum_{s = 1}^{τ \wedge (t-1)} Y_s 
\ge -a,$$
So, we can think of $W_{t \wedge τ} + a$ as a nonnegative supermartingale. Therefore, by the supermartingale convergence theorem, 
$\lim_{t \to ∞} W_{τ \wedge t}$ exists and is finite a.s. 

Therefore, on the set $\{ τ = ∞ \}$, i.e., the set $\{ \sum_{s = 1}^∞ Y'_s \le a \}$, $\lim_{t \to ∞} W_t$ exists and is finite.

Since $a$ is arbitary, we have that $\lim_{t \to ∞} W_t$ exists and is finite on $\{ \sum_{s=1}^∞ Y'_s < ∞ \}$. But, we know that $Y'_s \le Y_s$. Therefore, for $ω \in Ω_0$, 
$$\sum_{t \ge 1} Y'_t(ω) \le \sum_{t \ge 1} Y_t(ω) < ∞ . $$
Consequently, for $ω \in Ω_0$, $\lim_{t \to ∞} W_t$ exists and is finite.

Now \\eqref{eq:almost-supermartingale-st-2} implies that 
$$
  \lim_{t \to ∞} \biggl \{ X'_t - \sum_{s=1}^{t-1} (Y'_s - Z'_s) \biggr\} < ∞.
$$
Hence, on $Ω_0$, $\sum_{s=1}^∞ Z'_s < ∞$ and $\lim_{t \to ∞} X'_t$ exists and is finite. 

From @exr-product-sum, it follows that for $ω \in Ω_0$, $b_t(ω)$ has a limit. 
$$
    X_t = X'_t/b'_t
$$
has a limit. Moreover, the inequality, 
$$
  Z_t \le Z'_t \prod_{s \ge 1} (1 + β_s)
$$
implies that $\sum_{t \ge 1} Z_t < ∞$ for $ω \in Ω_0$. 
:::

The following is a determinisitic version of the above result, taken from @Bertsekas2000.

:::{#prp-deterministic-almost-martingale}
Let $\{X_t\}_{t \ge 1}$, $\{Y_t\}_{t \ge 1}$, and $\{Z_t\}_{t \ge 1}$ be sequences such that $\{Z_t\}_{t \ge 1}$ is nonnegative and the sequences satisfy:
$$
  X_{t+1} \le X_t + Y_t - Z_t
$$
and $\sum_{t \ge 1} Y_t < ∞$. Then, either $X_t \to -∞$ or else $X_t$ converges to a finite value and $\sum_{t \ge 1} Z_t < ∞$. 
:::

## Convergence of submartingales

:::{#thm-krickeberg}
### Krickeberg decomposition
Let $\{S_t\}_{t \ge 1}$ be a submartingale for which $\sup_t \EXP[S_t^{+}] < ∞$. Then, there exists a positive martingale $\{M_t\}_{t \ge 1}$ and a positive supermartingale $\{X_t\}_{t \ge 1}$ such that $S_t = M_t - X_t$ almost sure for each $t$.
:::

:::{.callout-tip}
### Remark
The finiteness of $\sup_{t} \EXP[ S_{t}^{+}]$ is equivalent to the finiteness of $\sup_{t} \EXP[|S_t|]$ because $|S_t| = 2S_t{+} - (S_t^{+} - S_t^{-})$ and by the submartingale property, $\EXP[S_t^{+} - S_t^{-}] = \EXP[S_t]$ increases with $t$. 
:::

:::{#cor-submartingale}
  A submartingale with $\sup_{t} \EXP[S_t^{+}] < ∞$ converges almost surely to an integrable limit.
:::


## Exercise {-}

:::{#exr-product-sum}
The purpose of this exercise is to establish the following: For a sequence of nonnegative real numbers $\{a_t\}_{t \ge 1}$
$$\sum_{t \ge 1} a_t < ∞  \iff
\prod_{t \ge 1} (1 + a_t) \text{ converges to a finite non-zero limit}.
$$

1. Show that $\sum_{t \ge 1}a_t \le \prod_{t \ge 1}(1 + a_t)$.

      _Hint_: Expand the right hand side.

2. Show that $\prod_{t \ge 1}(1 + a_t) \le \exp\Bigl(\sum_{t \ge 1} a_t\Bigr)$.

      _Hint_: Use Taylor series expansion of $e^x$.

:::


:::{#exr-martingale-Robbins-Monroe}
  Consider a sequence $\{α_t\}_{t \ge 1}$ adapted to $\{\mathcal F_t\}$ such that
  $$
  \sum_{t=1}^{∞} α_t = \infty
  \quad\text{and}\quad
  \sum_{t=1}^{∞} α_t^2 < ∞.
  $$
  Now suppose $\{X_t\}_{t \ge 1}$ is an adapted sequence of square integrable random variables such that 
  $$
    \EXP[ X_{t+1} | \mathcal F_t ] = 0
    \quad\text{and}\quad
    \EXP[ \| X_{t+1}\|_2^2 \mid \mathcal F_t ] \le B
  $$
  where $B$ is a deterministic constant. Then
  $$
    \sum_{t=1}^T α_t X_{t+1}
    \quad\text{and}\quad
    \sum_{t=1}^T α_t^2 \| X_t\|_2^2,
    \quad
    T = 1,2, \dots
  $$
  converge to finite limits almost surely.
:::

## Notes {-}

The material in the introduction is taken from @Pollard2002, @Chang2007, and @Dellacherie1982b. The latter is an excellent reference for this material.

@thm-almost-supermartingale and its proof is from @Robbins1971. A version of @thm-almost-supermartingale with $Z_t = 0$ is stated as Exercise II-4 of @Neveu1975.

@exr-martingale-Robbins-Monroe is from @Bertsekas2000.
