{
  "hash": "6632fa42314ab63c734ebada61fea794",
  "result": {
    "markdown": "---\ntitle: Finite horizon MDPs\nkeywords:\n   - MDPs\n   - Markov policies\n   - dynamic programming\n   - comparison principle\n   - principle of irrelevant information\nexecute:\n  echo: false\n  cache: true\n  freeze: true\n---\n\n## An example\n\nTo fix ideas, we start with an example. \n\n:::{#exm-peak-control}\nConsider a _controlled_ Markov chain defined over $\\ALPHABET S = \\{-2, -1, 0, 1, 2\\}$ with two control actions, i.e., $\\ALPHABET A = \\{0, 1\\}$. If action $A = 0$ is chosen, the chain evolves according to its \"natural\" dynamics, which are shown below:\n\n![\"Natural\" dynamics of the Markov chain](figures/peak-control1.svg){#fig-peak1}\n\nWhen action $A = 1$ is chosen, the chain evolves according to the \"forced\" dynamics, which are shown below:\n\n![\"Forced\" dynamics of the Markov chain](figures/peak-control2.svg){#fig-peak2}\n\nNote that under the natural dynamics, the Markov chain will settle to a uniform steady-state distribution; under the forced dynamics, the Markov chain settle to a distribution which is unimodal with a peak at state $0$.\n\nSuppose it is desirable to keep the Markov chain close to state $0$. We capture this by the system incurs a _running cost_ equal to $s^2$ in state $s$. In addition, choosing action $A=0$ is free but choosing action $A=1$ has a _correction cost_ $p$. \n\nThus, the decision maker is in a conundrum. It may allow the system to follow its natural dynamics, which causes the system to reach states with larger absolute value and incur a running cost $s^2$; or it may decide to force the system to drift towards state $0$, which reduces the running cost but, in turn, incurs a correction cost $p$. How should the decision maker choose its actions?\n\n:::\n\n\n## Basic model and structure of optimal policies\n\nThe type of model described above is called  Markov decision processes (MDP), and they are the simplest model of a stochastic control system. There are two ways to model MDPs, which we describe below.\n\nThe first method to model MDPs is to think of them as a **controlled** Markov process; in particular, an MDP is a stochastic process $\\{S_t\\}_{t \\ge 1}$, $S_t \\in \\ALPHABET S$, _controlled_ by the process $\\{A_t\\}_{t \\ge 1}$, $A_t \\in \\ALPHABET A$, which satisfies the **controlled Markov property**:\n$$\n  \\PR(S_{t+1} = s_{t+1} \\mid S_{1:t} = s_{1:t}, A_{1:t} = a_{1:t})\n  =\n  \\PR(S_{t+1} = s_{t+1} \\mid S_t = s_t, A_t = a_t).\n$$\nFor models with finite state and action spaces, the right-hand side of the above may be viewed as an element of the  **controlled transition matrix** $P_t(a_t)$. For instnace, in the example described at the beginning of this section, we have\n$$\n\\def\\1{\\tfrac 12}\nP(0) = \\MATRIX{ \\1& \\1&  0&  0&  0\\\\\n                \\1& 0 & \\1&  0&  0\\\\\n                0&  \\1&  0&  \\1& 0\\\\\n                0&  0&  \\1&   0& \\1 \\\\\n                0&  0&   0&  \\1& \\1}\n\\quad\\text{and}\\quad\n\\def\\1{\\tfrac 14}\n\\def\\2{\\tfrac 34}\n\\def\\3{\\tfrac 12}\nP(1) = \\MATRIX{ \\1& \\2&  0&  0&  0\\\\\n                \\1& 0 & \\2&  0&  0\\\\\n                0&  \\1& \\3&  \\1& 0\\\\\n                0&  0&  \\2&   0& \\1 \\\\\n                0&  0&   0&  \\2& \\1}\n$$\nSince the model is time-homogeneous, we have replaced $P_t(a)$ by just $P(a)$. \n\nThis representation is compact and convenient for computational purposes, but I personally feel that it can be a bit opaque for proving the fundamental results of MDP theory. For that reason, I prefer to start with the second representation, which is desibed below.\n\nIn the second representation, the dynamic behavior of an MDP is modeled by an equation \n$$ \\begin{equation} \n  S_{t+1} = f_t(S_t, A_t, W_t) \\label{eq:state}\n\\end{equation}$$\nwhere $S_t \\in \\ALPHABET S$ is the state, $A_t \\in \\ALPHABET A$ is the control input, and $W_t \\in \\ALPHABET W$ is the noise. An agent/controller observes the state and chooses the control input $A_t$. \n\nWe call this the **functional representation** of the MDP. Eq. \\\\eqref{eq:state} is a _non-linear_ _stochastic_ state-space model—_non-linear_ because $f_t$ can be any nonlinear function; _stochastic_ because the system is driven by stochastic noise $\\{W_t\\}_{t \\ge 1}$. \n\nNote that the controlled Markov chain representation can be easily translated to a functional representation by taking $W_t$ to be a uniform $[0,1]$ random variable and using a differnet [:Smirnov transformation](https://en.wikipedia.org/wiki/Inverse_transform_sampling) for each state action pair $(S_t, A_t)$.\n\n\nAt each time, the system incurs a cost that may depend on the current state and control action. This cost is denoted by $c_t(S_t, A_t)$. The system operates for a time horizon $T$. During this time, it incurs a total cost\n$$ \\sum_{t=1}^T c_t(S_t, A_t). $$\n\nThe initial state $S_1$ and the noise process $\\{W_t\\}_{t \\ge 1}$ are random variables defined on a common probability space (these are called _primitive random variables_) and are mutually independent. This seemingly benign assumption is critical for the theory that we present to go through. \n\n\nSuppose we have to design such a controller. We are told the probability distribution of the initial state and the noise. We are also told the system update functions $(f_1, \\dots, f_T)$ and the cost functions $(c_1, \\dots, c_T)$. \n\nOur objective to determine a **control policy**, i.e., a function for choosing the control actions. The control policy can be as sophisticated as we want. In principle, it can analyze the entire history of observations and control actions to choose the current control action. Thus, the control action can be written as\n$$ A_t = π_t(S_{1:t}, A_{1:t-1}),$$\nwhere $S_{1:t}$ is a shorthand for $(S_1, \\dots, S_t)$ and a similar\ninterpretation holds for $A_{1:t-1})$. The function $π_t$ is called the\n**control law** at time $t$. \n\n\nWe want to choose a _control policy_ $π = (π_1, \\dots, π_T)$ to minimize the expected total cost\n$$ J(π) := \\EXP\\bigg[ \\sum_{t=1}^T c_t(S_t, A_t) \\bigg]. $$\nHow should we proceed?\n\nAt first glance, the problem looks intimidating. It appears that we have to\ndesign a very sophisticated controller: one that analyzes all past data to\nchoose a control input. However, this is not the case. A remarkable result is\nthat the optimal controller can discard all past data and choose the\ncontrol input based only on the current state of the system. Formally, we have\nthe following:\n\n:::{#thm-MDP-markov}\n#### Optimality of Markov policies\nFor the system model described above, there is no loss of optimality in\nchoosing the control action according to\n$$ A_t = π_t(S_t), \\quad t=1, \\dots, T.$$\nSuch a control policy is called a _Markov policy_.\n:::\n\nIn the context of the example presented at the beginning of this section, @thm-MDP-markov says that the decision maker can decide whether to choose action $0$ or $1$ based on the current state, without any loss of optimality.\n\nIn general, @thm-MDP-markov claims that the cost incurred by the best Markov policy is the same as the cost incurred by the best history dependent policy. This appears to be a tall claim, so lets see how we can prove it. The main idea of the proof is to repeatedly apply [Blackwell's principle of irrelevant information][Blackwell] [@Blackwell1964]\n\n[Blackwell]: ../stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information\n\n::: {#lem-MDP-two-step-lemma}\n## Two-Step Lemma\nConsider an MDP that operates for two steps ($T=2$). Then there is no loss\nof optimality in restricting attention to a Markov control policy at time\n$t=2$.\n:::\n\nNote that $π_1$ is Markov because it can only depend $S_1$. \n\n:::{.callout-note collapse=\"true\"}\n#### Proof {-}\n\nFix $π_1$ and look at the problem of optimizing $π_2$. The total cost is\n$$ \\EXP[ c_1(S_1, π_1(S_1)) + c_2(S_2, π_2(S_{1:2}, A_1)) ]$$\nThe choice of $π_2$ does not influence the first term. So, for a fixed $π_1$,\nminimizing the total cost is the equivalent to minimizing the second term.\nNow, from Blackwell's principle of irrelevant information, there\nexists a $π_2^* \\colon S_2 \\mapsto A_2$ such that for any $π_2$\n$$\\EXP[c_2(S_2, π_2^*(S_2) ] \\le \\EXP[c_2(S_2, π_2(S_{1:2}, A_2) ].$$\n:::\n\n:::{#lem-three-step-lemma}\n### Three-Step Lemma\nConsider an MDP that operates for three steps ($T=3$). Assume that the\ncontrol law $π_3$ at time $t=3$ is Markov, i.e., $A_3 = π_3(S_3)$. Then, there\nis no loss of optimality in restricting attention to Markov control law at\ntime $t=2$.\n:::\n\n:::{.callout-note collapse=\"true\"}\n#### Proof {-}\n\nFix $π_1$ and $π_3$ and look at optimizing $π_2$. The total cost is\n$$ \\EXP[ c_1(S_1, π_1(S_1)) + c_2(S_2, π_2(S_{1:2}, A_1)) + c_3(S_3, π_3(S_3)].$$\n\nThe choice of $π_2$ does not affect the first term. So, for a fixed $π_1$ and\n$π_3$, minimizing the total cost is the same as minimizing the last two terms. Let us look at the last term carefully. Bu the law of iterated expectations, we have\n$$ \\EXP[ c_3(S_3, π_3(S_3) ] = \\EXP[ \\EXP[ c_3(S_3, π_3(S_3)) | S_2, A_2 ] ]. $$\nNow,\n\\begin{align*}\n  \\EXP[ c_3(S_3, π_3(S_3)) | S_2 = s_2, A_2 = a_2 ] &= \n  \\sum_{s_3 \\in \\ALPHABET S} c_3(s_3, π_3(s_3)) \\\\\n  &= \\PR( w_2 \\in \\ALPHABET W : f_2(s_2, a_2, w_2) = s_3 )\n  \\\\\n  &=: h_2(s_2, a_2).\n\\end{align*}\nThe key point is that $h_2(s_2, a_2)$ does not depend on $π_1$ or $π_2$. \n\nThus, the total expected cost affected by the choice of $π_2$ can be written\nas \n\\begin{align*}\n  \\EXP[ c_2(S_2, A_2) + c_3(S_3, A_3) ] &= \\EXP[ c_2(S_2, A_2) + h_2(S_2, A_2)\n  ] \\\\\n  &=: \\EXP[ \\tilde c_2(S_2, A_2) ].\n\\end{align*}\nNow, by Blackwell's principle of irrelevant information, there exists a\n$π_2^* : S_2 \\mapsto A_2$ such that for any $π_2$, we have\n$$ \\EXP[ \\tilde c_2(S_2, π_2^*(S_2))] \\le  \\EXP[ \\tilde c_2(S_2, π_2(S_{1:2},\nA_1) ].$$\n:::\n\nNow we have enough background to present the proof of optimality of Markov policies.\n\n:::{.callout-note collapse=\"true\"}\n#### Proof of @thm-MDP-markov\n\nThe main idea is that any system can be thought of as a two- or three-step\nsystem by aggregating time. Suppose that the system operates for $T$ steps. \nIt can be thought of as a two-step system where $t \\in \\{1, \\dots, T - 1\\}$\ncorresponds to step 1 and $t = T$ corresponds to step 2. From the two-step\nlemma, there is no loss of optimality in restricting attention to Markov\ncontrol law at step 2 (i.e., at time $t=T$), i.e., \n$$ A_T = π_T(S_T). $$\n\nNow consider a system where we are using a Markov policy at time $t=T$. This\nsystem can be thought of as a three-step system where $t \\in \\{1, \\dots,\nT-2\\}$ corresponds to step 1, $t = T-1$ corresponds to step 2, and $t=T$\ncorresponds to step 3. Since the controller at time $T$ is Markov, the\nassumption of the three step lemma is satisfied. Thus, by that lemma, there is\nno loss of optimality in restricting attention to Markov controllers at step 2\n(i.e., at time $t=T-1$), i.e., \n$$A_{T-1} = π_{T-1}(S_{T-1}).$$\n\nNow consider a system where we are using a Markov policy at time $t \\in\n\\{T-1, T\\}$. This can be thought of as a three-step system where $t \\in \\{1,\n\\dots, T - 3\\}$ correspond to step 1, $t = T-2$ correspond to step 2, and $t\n\\in \\{T-1, T\\}$ correspond to step 3. Since the controllers at time $t \\in\n\\{T-1, T\\}$ are Markov, the assumption of the three-step lemma is satisfied.\nThus, by that lemma, there is no loss of optimality in restricting attention\nto Markov controllers at step 2 (i.e., at time $t=T-2$), i.e.,\n$$A_{T-2} = π_{T-2}(S_{T-2}).$$\n\nProceeding this way, we continue to think of the system as a three step system\nby different relabeling of time. Once we have shown that the controllers at\ntimes $t \\in \\{s+1, s+2, \\dots, T\\}$ are Markov, we relabel time as follows:\n$t=\\{1, \\dots, s-1\\}$ corresponds to step 1, $t = s$ corresponds to step 2,\nand $t \\in \\{s+1, \\dots, T\\}$ corresponds to step 3. Since the controllers at\ntime $t \\in \\{s+1, \\dots, T\\}$ are Markov, the assumption of the three-step\nlemma is satisfied. Thus, by that lemma, there is no loss of optimality in\nrestricting attention to Markov controllers at stage 2 (i.e. at time $s$),\ni.e.,\n$$A_τ = π_τ(S_τ).$$\n\nProceeding until $s=2$, completes the proof.\n:::\n\n## Performance of Markov policies {#performance}\n\nWe have shown that there is no loss of optimality to restrict attention to\nMarkov policies. One of the advantages of Markov policies is that their performance can be computed recursively. In particular, given any Markov\npolicy $π = (π_1, \\dots, π_T)$, define _the cost-to-go functions_ or _value function_ as\nfollows:\n$$V^{π}_t(s) = \\EXP^π \\bigg[ \\sum_{τ = t}^{T} c_τ(S_τ, π_τ(S_τ)) \\biggm| S_t =\ns\\bigg]. $$\nNote that $V^{π}_t(s)$ only depends on the future policy $(π_t, \\dots, π_T)$. These functions can be computed recursively as follows: we start with a terminal value function $V^{π}_{T+1}(s) ≡ 0$ and then for $t \\in \\{T, T-1, \\dots, 1\\}$, recursively compute:\n\\begin{align}\n  V^{π}_t(s) &= \\EXP^π \\bigg[ \\sum_{τ = t}^{T} c_τ(S_τ, π_τ(S_τ)) \\biggm| S_t =\n  s \\bigg] \\notag \\\\\n  &= \\EXP^π \\bigg[ c_t(s, π_t(s)) + \\EXP^π \\bigg[ \\sum_{τ = t+1}^T\n    c_τ(S_τ, π_τ(S_τ)) \\biggm| S_{t+1} \\bigg] \\biggm| S_t = s \\bigg]\n  \\notag \\\\\n  &= \\EXP^π\\big[ c_t(s, π_t(s)) + V^{π}_{t+1}(S_{t+1}; π) \\big| S_t = s \\big]. \\label{eq:finite-policy-evaluation}\n\\end{align}\n\nThe formula of Eq. \\eqref{eq:finite-policy-evaluation} is called the **policy evaluation formula.** \n\nFor the controlled Markov chain representation, the formula can be written in a vector form. In particular, we will think of $V^π_t$ to be a vector in $\\reals^n$, where $n = \\ABS{\\ALPHABET S}$. For any policy $π = (π_1, \\dots, π_T)$, define the $n × n$ transition matrices $(P^{π}_1, \\dots, P^{π}_T)$ as\n$$\n  P^{π}_t(s'|s) = P_t(s'|s, π_t(s)), \n  \\quad t \\in \\{1,\\dots, T\\}.\n$$\nFurthermore, define $n$ dimensional cost vectors $(c^{π}_1, \\dots, c^{π}_T)$ as\n$$\n  c^π_t(s) = c_t(s, π_t(s)), \n  \\quad t \\in \\{1,\\dots, T\\}.\n$$\nThen, the policy evaluation formula \\eqref{eq:finite-policy-evaluation} is equivalent to the following: start with a terminal value function $V^π_{T+1} ≡ 0$ and then for $t \\in \\{T, T-1,\\dots, 1\\}$, recursively compute:\n$$\n  V^{π}_t = c^{π}_t + P^{π}_t V^{π}_{t+1}.\n$$\n\n### @exm-peak-control (continued)\n\nAs an example, consider the following time-homogeneous policy @exm-peak-control:\n$$\n  π_t(s) = \\begin{cases}\n    1 & \\text{if } |s| = 2 \\\\\n    0 & \\text{otherwise}\n  \\end{cases}\n$$\nSuppose $p = 1$, i.e., $c(s,a) = s^2 + a$. We now compute the value function $\\{V^{π}_t\\}_{t=1}^T$ for $T = 5$. \n\nFor this policy\n$$\n  c^{π} = \\MATRIX{ 5 \\\\ 1 \\\\ 0 \\\\ 1 \\\\  5 }\n  \\quad\\text{and}\\quad\n  \\def\\1{\\tfrac 14}\n  \\def\\2{\\tfrac 34}\n  \\def\\3{\\tfrac 12}\n  P^π = \\MATRIX{ \\1& \\2&  0&  0&  0\\\\\n                \\3& 0 & \\3&  0&  0\\\\\n                0&  \\3&  0&  \\3& 0\\\\\n                0&  0&  \\3&   0& \\3 \\\\\n                0&  0&   0&  \\2& \\1}\n$$\nWe start with $V^π_{t+1} = \\VEC(0,0,0,0,0)$ and run the following recursion.\n\n\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\ncπ = [5, 1, 0, 1, 5] \nPπ = [1//4  3//4  0     0     0 \n      1//2  0     1//2  0     0\n      0     1//2  0     1//2  0\n      0     0     1//2  0     1//2\n      0     0     0     3//4  1//4]\n\nT = 5\nn = size(cπ,1)\n\nVπ = [ zeros(n) for t ∈ 1:T+1 ]\n\nfor t ∈ T:-1:1\n  Vπ[t] = cπ + Pπ*Vπ[t+1]\nend\n\ndisplay(Vπ)\n```\n\n::: {.cell-output .cell-output-display}\n```\n6-element Vector{Vector{Float64}}:\n [13.3515625, 9.046875, 7.4375, 9.046875, 13.3515625]\n [11.09375, 7.4375, 5.0, 7.4375, 11.09375]\n [9.375, 5.0, 3.5, 5.0, 9.375]\n [7.0, 3.5, 1.0, 3.5, 7.0]\n [5.0, 1.0, 0.0, 1.0, 5.0]\n [0.0, 0.0, 0.0, 0.0, 0.0]\n```\n:::\n:::\n\n\n@fig-peak-evaluation shows the value functions computed as part of the policy evaluation.\n\n![Value function for policy evaluation of @exm-peak-control. Note that the value functions are computed by proceeding backwards in time.](figures/peak-control-evaluation1.svg){#fig-peak-evaluation}\n\n## Dynamic Programming Decomposition {#DP}\n\nNow we are ready to state the main result for MDPs.\n\n:::{#thm-MDP-DP}\n### Dynamic program\nRecursive define _value functions_ $\\{V^*_t\\}_{t = 1}^{T+1} \\colon \\ALPHABET S\n\\to \\reals$ as follows: \n$$ \\begin{equation} \\label{eq:DP-1}\n  V^*_{T+1}(s) = 0 \n\\end{equation} $$ \nand for $t \\in \\{T, \\dots, 1\\}$:\n$$\\begin{align}\n   Q^*_t(s,a) &= c(s,a) + \\EXP[ V^*_{t+1}(S_{t+1}) | S_t = s, A_t = a] \n   \\nonumber \\\\\n   &= c(s,a) + \\EXP[ V^*_{t+1}(f_t(s,a,W_t)) ], \\label{eq:DP-2}\n\\end{align}$$\nand define\n$$ \\begin{equation} \\label{eq:DP-3}\n  V^*_t(s) = \\min_{a \\in \\ALPHABET A} Q^*_t(s,a).\n\\end{equation} $$\nThen, a Markov policy is optimal if and only if it satisfies\n$$ \\begin{equation} \\label{eq:verification} \n  π_t^*(s) = \\arg \\min_{a \\in \\ALPHABET A} Q_t(s,a).\n\\end{equation} $$\n:::\n\nFor the controlled Markov chain representation, the dynamic program of @thm-MDP-DP can be written more succinctly. We will view $\\{V^*_t\\}_{t=1}^T$ as vectors in $\\reals^n$, where $n = \\ABS{\\ALPHABET S}$. Moreover, we will view $\\{Q^*_t\\}_{t=1}^T$ and $\\{c_t\\}_{t=1}^T$ as matrices in $\\reals^{n × m}$, where $m = \\ABS{\\ALPHABET A}$. \n\nThen, the dynamic program of @thm-MDP-DP may be written as follows: start with a terminal value function $V^*_{T+1} ≡ 0$ and then for $t \\in \\{T, T-1, \\dots, 1\\}$, recursively compute:\n\\begin{align*}\n  Q^*_t &= c_t + [ P_t(1) V^*_{t+1} \\mid P_t(2) V^*_{t+1} \\mid \\cdots \\mid P_t(m) V^*_{t+1} ], \\\\\n  V^*_t &= \\min(Q^*_t, \\hbox{\\tt dim}=2), \\\\\n  π^*_t &= \\arg\\min(Q^*_t, \\hbox{\\tt dim}=2).\n\\end{align*}\n\n### @exm-peak-control (continued)\n\nNow we present the dynamic programming solution for @exm-peak-control:\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\n# We use the default indexing (1,...,n) for convenience. \n\n(n,m) = (5,2)\nS = 1:n\nA = 1:m\nT = 5\n\nP = [ zeros(n,n) for a ∈ A ]\nP[1] = [1//2  1//2  0     0     0 \n        1//2  0     1//2  0     0\n        0     1//2  0     1//2  0\n        0     0     1//2  0     1//2\n        0     0     0     1//2  1//2]\n\nP[2] = [1//4  3//4  0     0     0 \n        1//4  0     3//4  0     0\n        0     1//4  1//2  1//4  0\n        0     0     3//4  0     1//4\n        0     0     0     3//4  1//4]\n\nc = zeros(n,m)\nfor s ∈ S, a ∈ A\n  # use s-3 and a-1 to convert to \"natural\" indices -2:2 and 0:1\n  c[s,a] = (s-3)^2 + (a-1)\nend\n\nV = [ zeros(n)     for t ∈ 1:T+1 ]\nπ = [ zeros(Int,n) for t ∈ 1:T   ]\n\nfor t ∈ T:-1:1\n  Q = c + hcat(P[1]*V[t+1], P[2]*V[t+1])\n\n  # Could be done more efficiently with a single pass\n  V[t] = vec(minimum(Q, dims=2))\n  # Subtract 1 to get to \"natural\" indices 0:1\n  π[t] = argmin.(eachrow(Q)) .- 1\nend\n\ndisplay(V)\ndisplay(π)\n```\n\n::: {.cell-output .cell-output-display}\n```\n6-element Vector{Vector{Float64}}:\n [12.4453125, 7.8984375, 6.40625, 7.8984375, 12.4453125]\n [10.46875, 6.4375, 4.375, 6.4375, 10.46875]\n [8.75, 4.375, 3.0, 4.375, 8.75]\n [6.5, 3.0, 1.0, 3.0, 6.5]\n [4.0, 1.0, 0.0, 1.0, 4.0]\n [0.0, 0.0, 0.0, 0.0, 0.0]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\n5-element Vector{Vector{Int64}}:\n [1, 1, 1, 1, 1]\n [1, 1, 0, 1, 1]\n [0, 1, 0, 1, 0]\n [0, 0, 0, 0, 0]\n [0, 0, 0, 0, 0]\n```\n:::\n:::\n\n\n@fig-peak-optimal shows the value functions and optimal policy computed from the dynamic program\n\n![Optimal value function and optimal policy for @exm-peak-control. The red color indicates that the optimal action is $1$; black indicates that the optimal action is $0$. Note that the computations are done by proceeding backwards in time.](figures/peak-control-optimal1.svg){#fig-peak-optimal}\n\n### Back to the proof\n\nInstead of proving @thm-MDP-DP, we prove a related result.\n\n:::{#thm-comparison-principle}\n### The comparison principle \nFor any Markov policy $π$\n$$ V^{π}_t(s) \\ge V_t(s) $$\nwith equality at $t$ if and only if the _future policy_ $π_{t:T}$\nsatisfies the verification step \\\\eqref{eq:verification}. \n::: \n\nNote that the comparison principle immediately implies that the policy\nobtained using dynamic programming is optimal. \n\nThe comparison principle also allows us to interpret the value functions. The\nvalue function at time $t$ is the minimum of all the cost-to-go functions over\nall future policies. The comparison principle also allows us to interpret the\noptimal policy (the interpretation is due to Bellman and is colloquially\ncalled Bellman's principle of optimality).\n\n:::{.callout-important}\n## Bellman's principle of optimality.\nAn optimal policy has the property that whatever the initial state and the\ninitial decisions are, the remaining decisions must constitute an optimal\npolicy with regard to the state resulting from the first decision.\n:::\n\n:::{.callout-note collapse=\"true\"}\n#### Proof of the comparison principle {-}\nThe proof proceeds by backward induction. Consider any Markov policy $π =\n(π_1, \\dots, π_T)$. For $t = T$, \n$$ \\begin{align*}\n  V_T(s) &= \\min_{a \\in \\ALPHABET A} Q_T(s,a) \\\\\n  &\\stackrel{(a)}= \\min_{a \\in \\ALPHABET A} c_T(s,a) \\\\\n  &\\stackrel{(b)}\\le c_T(s, π_T(s)) \\\\\n  &\\stackrel{(c)}= V^{π}_T(s),\n\\end{align*} $$\nwhere $(a)$ follows from the definition of $Q_T$, $(b)$ follows from the\ndefinition of minimization, and $(c)$ follows from the definition of $J_T$. \nEquality holds in $(b)$ iff the policy $π_T$ is optimal. This result forms the basis of induction.\n\nNow assume that the statement of the theorem is true for $t+1$. Then, for $t$\n$$ \\begin{align*}\n  V_t(s) &= \\min_{a \\in \\ALPHABET A} Q_t(s,a) \\\\\n  &\\stackrel{(a)}= \\min_{a \\in \\ALPHABET A} \\Big\\{\n  c_t(s,a) + \\EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a] \n  \\Big\\}\n  \\\\\n  &\\stackrel{(b)}\\le  \\Big\\{\n  c_t(s,π_t(s)) + \\EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = π_t(s)] \n  \\Big\\} \\\\\n  &\\stackrel{(c)}\\le  \\Big\\{\n  c_t(s,π_t(s)) + \\EXP[ J_{t+1}(S_{t+1}; π) | S_t = s, A_t = π_t(s)] \n  \\Big\\} \\\\\n  &\\stackrel{(d)}= V^{π}_t(s),\n\\end{align*} $$\nwhere $(a)$ follows from the definition of $Q_t$, $(b)$ follows from the\ndefinition of minimization, $(c)$ follows from the induction hypothesis, and\n$(d)$ follows from the definition of $J_t$. We have equality in step $(b)$ iff\n$π_t$ satisfies the verification step \\\\eqref{eq:verification} and have equality\nin step $(c)$ iff $π_{t+1:T}$ is optimal (this is part of the induction\nhypothesis). Thus, the result is true for time $t$ and, by the principle of\ninduction, is true for all time.\n:::\n\n## More examples\n\n### Machine repair\n\n::: {#exm-machine-repair}\n#### Machine repair\nConsider a manufacturing process, where the machine used for manufacturing deteriorates over time. Let $\\ALPHABET S = \\{0, 1, \\dots n \\}$ represent the condition of the machine. The higher the value of $s$, the worse the condition of the equipment. \n\nA decision maker observes the state of the machine and has two options: continue operating the machine or replace it with a a new and identical piece of equipment. Operating the machine is state $s$ costs $h(s)$, where $h(⋅)$ is a weakly increasing function; replace the machine costs a constant amount $K$. \n\nWhen the machine is operated, it's state deteriorates according to \n$$\n  S_{t+1} = \\min( S_t + W_t , n)\n$$\nwhere $\\{W_t\\}_{t \\ge 1}$ is an i.i.d.\\ process with PMF $μ$.\n:::\n\nThe above system may be modelled as an MDP with state space $\\ALPHABET S$ and action space $\\ALPHABET A = \\{0, 1\\}$ where $0$ means operating the machine and $1$ means replacing the machine. \n\nFor instance, consider $n = 5$ and $W \\sim \\text{Bernoulli}(p)$. The evolution of the Markov chain under action $A_t = 0$ and $A_t = 1$ are shown in @fig-machine-repair. \n\n:::{#fig-machine-repair layout-ncol=2}\n\n![Dynamics under action $A_t = 0$](figures/machine-repair1.svg){#exm-machine-repair1}\n\n![Dynamics under action $A_t = 1$](figures/machine-repair2.svg){#exm-machine-repair2}\n\nState dynamics for the MDP of @exm-machine-repair\n:::\n\nThus,\n$$\n  P(0) = \\MATRIX{q& p& 0& 0& 0& 0\\\\\n                 0& q& p& 0& 0& 0\\\\\n                 0& 0& q& p& 0& 0\\\\\n                 0& 0& 0& q& p& 0\\\\\n                 0& 0& 0& 0& q& p\\\\\n                 0& 0& 0& 0& 0& 1}\n  \\quad\\hbox{and}\\quad\n  P(1) = \\MATRIX{1& 0& 0& 0& 0& 0\\\\\n                 1& 0& 0& 0& 0& 0\\\\\n                 1& 0& 0& 0& 0& 0\\\\\n                 1& 0& 0& 0& 0& 0\\\\\n                 1& 0& 0& 0& 0& 0\\\\\n                 1& 0& 0& 0& 0& 0}\n$$\nwhere we have set $q = 1-p$ for notational convenience.\n\nThe per-step cost is given by\n$$\n  c(s,a) = h(s) + λa.\n$$\n\nSolving the DP for $p=0.2$, $λ=10$, and $T=5$ gives the following:\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code code-fold=\"true\" code-summary=\"Show code\"}\nusing SparseArrays\n\n# We use the default indexing (1,...,n) for convenience. \n\n(n,m) = (6,2)\nS = 1:n\nA = 1:m\nT = 5\nλ = 10\n\np = 0.2\nq = 1-p\n\nP = [ spzeros(n,n) for a ∈ A ]\nfor i in S\n  P[1][i,i] = q\n  P[1][i,min(i+1,n)] += p\n\n  P[2][i,1] = 1\nend\n\nc = zeros(n,m)\nfor s ∈ S, a ∈ A\n  # use s-3 and a-1 to convert to \"natural\" indices -2:2 and 0:1\n  c[s,a] = 2s + λ*a\nend\n\nV = [ zeros(n)     for t ∈ 1:T+1 ]\nπ = [ zeros(Int,n) for t ∈ 1:T   ]\n\nfor t ∈ T:-1:1\n  Q = c + hcat(P[1]*V[t+1], P[2]*V[t+1])\n\n  # Could be done more efficiently with a single pass\n  V[t] = vec(minimum(Q, dims=2))\n  # Subtract 1 to get to \"natural\" indices 0:1\n  π[t] = argmin.(eachrow(Q)) .- 1\nend\n\ndisplay(V)\ndisplay(π)\n```\n\n::: {.cell-output .cell-output-display}\n```\n6-element Vector{Vector{Float64}}:\n [64.0, 73.36000000000001, 76.4, 78.4, 80.4, 82.4]\n [50.400000000000006, 58.400000000000006, 63.2, 65.2, 67.2, 69.2]\n [37.2, 43.2, 49.2, 52.400000000000006, 54.400000000000006, 56.400000000000006]\n [24.400000000000002, 28.400000000000002, 32.400000000000006, 36.4, 40.4, 44.0]\n [12.0, 14.0, 16.0, 18.0, 20.0, 22.0]\n [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```\n5-element Vector{Vector{Int64}}:\n [0, 0, 1, 1, 1, 1]\n [0, 0, 1, 1, 1, 1]\n [0, 0, 0, 1, 1, 1]\n [0, 0, 0, 0, 0, 0]\n [0, 0, 0, 0, 0, 0]\n```\n:::\n:::\n\n\n@fig-machine-repair-plot shows the optimal value function and optimal policy as a function of time.\n\n![Optimal value function and optimal policy for @exm-machine-repair. The red color indicates that the optimal action is $1$; black indicates that the optimal action is $0$. Note that the computations are done by proceeding backwards in time.](figures/machine-repair-plot1.svg){#fig-machine-repair-plot}\n\n<!-- TODO:\n#### Rate control/admission control of a queue\n\n-->\n\n## Variations of a theme \n\n### Cost depends on next state {#sec-cost-depending-on-next-state}\n\nIn the basic model that we have considered above, we assumed that the per-step\ncost depends only on the current state and current actions. In some\napplications, such as the [inventory management](inventory-management.html), it is more natural to have a cost function where\nthe cost depends on the current state, current action, and the next state.\nConceptually, such problems can be treated in the same way as the standard\nmodel.\n\nIn particular, suppose we have a per-step cost given by\n$c_t(S_t,A_t,S_{t+1})$, where the objective is to minimize\n$$ J(π) = \\EXP\\Bigl[ \\sum_{t=1}^T c_t(S_t, A_t, S_{t+1}) \\Bigr]. $$\n\nDefine \n$$ \\tilde c_t(s, a) = \\EXP[ c_t(s, a, S_{t+1}) | S_t = s, A_t = a ] \n= \\EXP[ c_t(s,a, f_t(s,a, W_t) ]. $$\nThen, by the towering property of conditional expectation, we can write\n\n$$ \\begin{align*} \n J(π) &= \\EXP\\Bigl[ \\sum_{t=1}^T \\EXP[ c_t(S_t, A_t, S_{t+1}) | S_t, A_t] \\Bigr] \\\\\n &= \\EXP\\Bigl[ \\sum_{t=1}^T \\tilde c_t(S_t, A_t) \\Bigr].\n\\end{align*} $$\n\nThus, we can equivalently consider this as our standard model with the\nper-step cost given by $\\tilde c_t(S_t, A_t)$. We can write the recursive step\nof the dynamic program as follows:\n$$ Q^*_t(s,a) = \\EXP[ c_t(s,a, S_{t+1}) + V^*_{t+1}(S_{t+1}) | S_t = s, A_t = a ].$$\n\nFor numerically solving the dynamic program when the cost is time-homogeneous\n(i.e., does not depend on $t$), it is more efficient to compute $\\tilde c$\nonce and recuse that in the dynamic program recursion.\n\n\n### Discounted cost\n\nIn some applications, it is common to consider a discounted expected cost given by\n$$ J(π) = \\EXP\\Bigl[ \\sum_{t=1}^T γ^{t-1} c_t(S_t, A_t) \\Bigr] $$\nwhere $γ \\in (0,1)$ is called the discount factor.\n\n:::{.callout-tip}\n#### Discount Factor\n\nThe idea of using discounting in MDPs is due to @Blackwell1965.\n\nThere are two interpretations of the discount factor $γ$. The first\ninterpretation is an economic interpretation to determine the _present value_\nof a utility that will be received in the future. For example, suppose a\ndecision maker is indifferent between receiving 1 dollar today or $s$ dollars\ntomorrow. This means that the decision maker discounts the future at a rate\n$1/s$, so $γ = 1/s$.\n\nThe second interpretation is that of an absorbing state. Suppose we\nare operating a machine that generates a value of \\$1 each day. However,\nthere is a probability $p$ that the machine will break down at the end of\nthe day. Thus, the expected return for today is \\$1 while the expected\nreturn for tomorrow is $(1-p)$ (which is the probability that the machine\nis still working tomorrow). In this case, the discount factor is defined\nas $(1-p)$. See @Shwartz2001 for a detailed discussion of this alternative.\n:::\n\n\n\nThe recursive step of the dynamic program for such models can be written as\n$$ Q^*_t(s,a) = c_t(s,a) + γ \\, \\EXP[ V^*_{t+1}( S_{t+1}) | S_t = s, A_t = a ].$$\n\n### Multiplicative cost {#multiplicative-cost}\n\nSo far, we have assumed that the cost is additive. The dynamic proramming\ndecomposition also works for models with multiplicative cost. In particular,\nsuppose that the performance of any policy is given by \n$$ J(π) = \\EXP\\Bigl[ \\prod_{t=1}^T c_t(S_t, A_t) \\Bigr] $$\nwhere the per-step cost function is positive. Then, it can be shown that the\noptimal policy is given by the following dynamic program.\n\n:::{#prp-DP-multiplicative}\n## Dynamic Program for multiplicative cost\nInitialize $V_{T+1}(s) = 1$ and recursively compute\n$$ \\begin{align*}\n Q^*_t(s,a) &= c_t(s,a) \\EXP[ V^*_{t+1}(S_{t+1}) | S_t = s, A_t = a ], \\\\\n V^*_t(s) &= \\min_{a \\in \\ALPHABET A} Q^*_t(s,a).\n\\end{align*} $$\n:::\n\n### Exponential cost function {#exponential-cost}\n\nA special class of multiplicative cost function is exponential of sum:\n$$J(π) = \\EXP\\Bigl[ \\exp\\Bigl( \\theta \\sum_{t=1}^T c_t(S_t, A_t) \\Bigr) \\Bigr]. $$\n\nWhen $\\theta > 0$, the above captures risk-averse preferences and when\n$\\theta < 0$, it corresponds to risk-seeking preferences. This is equivalent\nto a multiplicative cost\n$$J(π) = \\EXP\\Bigl[ \\prod_{t=1}^T \\exp( \\theta c_t(S_t, A_t)) \\Bigr]. $$\nTherefore, the dynamic program for multiplicative cost is also applicable for\nthis model.\n\nSee notes on [risk-sensitive MDPs] for more details\n\n[risk-sensitive MDPs]: ../risk-sensitive/risk-sensitive-mdps.qmd\n\n\n### Optimal stopping {#optimal-stopping}\n\nLet $\\{S_t\\}_{t \\ge 1}$ be a Markov chain. At each time $t$, a decision maker\nobserves the state $S_t$ of the Markov chain and decides whether to continue\nor stop the process. If the decision maker decides to continue, he incurs a\n_continuation cost_ $c_t(S_t)$ and the state evolves. If the DM decides to\nstop, he incurs a _stopping cost_ of $d_t(S_t)$ and the problem is terminated.\nThe objective is to determine an optimal _stopping time_ $\\tau$ to minimize\n$$J(\\tau) := \\EXP\\bigg[ \\sum_{t=1}^{\\tau-1} c_t(S_t) + d_\\tau(S_\\tau)\n\\bigg].$$\n\nSuch problems are called _Optimal stopping problems_. \n\nDefine the _cost-to-go function_ of any stopping rule as\n$$V^{\\tau}_t(s) = \\EXP\\bigg[ \\sum_{τ = t}^{\\tau - 1} c_{\\tau}(S_t) +\nd_\\tau(S_\\tau) \\,\\bigg|\\, \\tau > t \\bigg]$$\nand the _value function_ as\n$$V^*_t(s) = \\inf_{\\tau} V^{\\tau}_t(s). $$\nThen, it can be shown that the value functions satisfy the following\nrecursion:\n\n:::{#prp-DP-stopping}\n**Dynamic Program for optimal stopping**\n$$ \\begin{align*}\nV^*_T(s) &= s_T(s) \\\\\nV^*_t(s) &= \\min\\{ s_t(s), c_t(s) + \\EXP[ V^*_{t+1}(S_{t+1}) | S_t = s].\n\\end{align*}$$\n:::\n\nSee the notes on [optimal stopping] for more details.\n\n[optimal stopping]: optimal-stopping.qmd\n\n\n### Minimax setup\n\n_To be written_\n\n## Continuous state and action spaces {#sec-mdp-cts-spaces}\n\nThe fundamental ideas discussed above also hold for continuous state and action spaces provided one carefully deals with measurability. We first fix some notation:\n\n- For a set $\\ALPHABET S$, let $\\mathscr B(\\ALPHABET S)$ denote the Borel sigma-algebra on $\\ALPHABET S$.\n- We use $\\ALPHABET M(\\ALPHABET X, \\ALPHABET Y)$ to denote the set of measurable functions from the space $\\ALPHABET X$ to the space $\\ALPHABET Y$ (we implicitly assume that the sigma-algebras on $\\ALPHABET X$ and $\\ALPHABET Y$ is the respective Borel sigma-algebras). When $\\ALPHABET Y$ is $\\reals$, we will sometimes use the notation $\\ALPHABET M(\\ALPHABET X)$.\n\n<!--\n2. Let $\\ALPHABET M_{∞}(\\ALPHABET X)$ denote the set of measurable real valued functions with bounded sup-norm, i.e., \n   $$\n    \\ALPHABET M_{∞}(\\ALPHABET X) = \n    \\bigl\\{ f \\in \\ALPHABET M(\\ALPHABET X) : \\| f \\|_{∞} < ∞ \\bigr\\}.\n   $$\n-->\n\nIn order to talk about _expected cost_, in continuous state spaces we have to assume that the per-step cost is measurable, i.e., $c \\in \\ALPHABET M(\\ALPHABET S × \\ALPHABET A)$, and the $P \\colon \\ALPHABET S × \\ALPHABET A × \\mathscr B(\\ALPHABET S) \\to [0,1]$ is a stochastic kernel, i.e., for every $(s,a) \\in \\ALPHABET S × \\ALPHABET A$, $p(s,a,\\cdot)$ is probability measure on $\\ALPHABET S$, and for every $B \\in \\ALPHABET S$, the function $p(\\cdot, \\cdot, B) \\in \\ALPHABET M(\\ALPHABET S × \\ALPHABET A)$. These assumption imply that for every measurable policy $π \\in \\ALPHABET M(\\ALPHABET S, \\ALPHABET A)$, the performance $J(\\pi)$ is well defined. However, these assumptions are not sufficient to establish the optimality of dynamic programmming. \n\nTo highlight the technical issues, let us consider an MDP with $T = 1$, i.e., a stochastic optimization problem. Define \n$$\n  V^*(s) = \\inf_{a \\in \\ALPHABET A} c(s,a)\n  \\quad\\text{and}\\quad\n  π^*(s) = \\arg\\inf_{a \\in \\ALPHABET A} c(s,a).\n$$\n\nWe present a few examples below from @Blackwell1965 to highlight technical issues with non-finite state models.\n\n:::{#exm-Blackwell-no-optimal-policy}\n## No optimal policy\n\nLet $\\ALPHABET S = \\{s_\\circ\\}$, $\\ALPHABET A = \\integers_{\\ge 0}$ Consider $c(s_\\circ,a) = (a+1)/a$. Here $v^*(s_\\circ) = 1$ but there is no policy which achieves this cost.\n:::\n\nIn the above example, there is no optimal policy, but given any $ε > 0$, we can identify an $ε$-optimal policy. The next example shows that we can have a much severe situation where the value function is not measurable. The example relies on the following fact:\n\n> There exist Borel sets in $\\reals^2$ whose projection on $\\reals$ is not Borel. See [:the wikipedia article on projections][projections] for a discussion.\n\n[projections]: https://en.wikipedia.org/wiki/Projection_(measure_theory)\n\n:::{#exm-Blackwell-non-measurable-value-function}\n## Non-measurable value function\n\nLet $\\ALPHABET S = \\ALPHABET A = [0,1]$ and let $B \\subset \\ALPHABET S × \\ALPHABET A$ such that $B$ is measurable but its projection on $D$ onto $\\ALPHABET S$ is not. Consider $c(s,a) = -\\IND\\{ (s,a) \\in B \\}$. Note that \n$$\n  v(s) = \\inf_{a \\in \\ALPHABET A} c(s,a) = \n  \\inf_{a \\in \\ALPHABET A} -\\IND\\{ (s,a) \\in B \\} =\n  -\\IND\\{s \\in D \\}\n$$\nwhich is not Borel measurable.\n:::\n\nSee @Piunovskiy2011 for various examples on what can go wrong in MDPs. In particular, Example 1.4.15 of @Piunovskiy2011 extends @exm-Blackwell-non-measurable-value-function to provide an example where there is no $ε$-optimal policy! (@Blackwell1965 also has such an example, but it is much harder to parse).\n\n<!-- See example 1.4.15 -->\n\nThere are two ways to resolve the issue with non-existence of optimal policies: either assume that $\\ALPHABET A$ is compact or that the function that we are minimizing is coercive, so that the $\\arg\\inf$ can be replaced by an $\\arg\\min$. Or work with $ε$-optimal policies. \n\nResolving the measurability issue is more complicated. There are \nvarious **measurable selection theorems** in the literature to identify sufficient conditions for measurability of the value function and optimal policy. See @HernandezLerma1996 and [-@HernandezLerma1999] for an accessible treatment of continous state MDPs.\n\n## Exercises {-}\n\n::: {#exr-monotonicity-in-time}\n#### Monotonicity in time\n\nConsider an MDP where the dynamics and per-step cost are time-homogeneous, i.e., the function $f_t$ and the per-step cost $c_t$ do not depend on $t$ (except, possibly at the terminal time $t=T$). Suppose that\n$V^*_{T-1}(s) \\le V^*_T(s)$ for all $s \\in \\ALPHABET S$. Then, show\nthat\n$$ V^*_{t}(s) \\le V^*_{t+1}(s), \\quad\n   \\text{for all $s \\in \\ALPHABET S$ and $t$}.$$\n\nSimilarly, if we have that $V^*_{T-1}(s) \\ge V^*_T(s)$ for all $s \\in\n\\ALPHABET S$, then\n$$ V^*_{t}(s) \\ge V^*_{t+1}(s), \\quad\n   \\text{for all $s \\in \\ALPHABET S$ and $t$}.$$\n\n:::\n\n::: {#exr-maximizing-tail-probabilities}\n#### Dynamic programming for maximizing tail probabilities\n\nConsider a dynamical system that evolves as follows:\n$$\n  S_{t+1} = f_t(S_t, A_t, W_t)\n$$\nwhere $\\{S_1, W_1, \\dots, W_T\\}$ are independent random variables and the\ncontrol actions $A_t$ are chosen according to a history dependent policy \n$π = (π_1, \\dots, π_T)$:\n$$\n  A_t = π_t(S_{1:t}, A_{1:t-1}).\n$$\nGiven a sequence of functions $h_t \\colon \\ALPHABET S \\mapsto \\reals$, the\ncost of a policy $π$ is given by the probability that $h_t(S_t)$\nexceeds a given threshold $α \\in \\reals$ at some time, i.e.,\n$$\n  J(π) = \\PR^{π}\\left( \\max_{0 \\le t \\le T} h_t(S_t) \\ge α \\right).\n$$\n\nShow that the above cost can be put in an additive form that would enable\nus to use the theory developed in the class to tackle this setup.\n\n:::\n\n:::{#exr-internet-of-things}\n\n#### Optimal strategy in internet of things\n\nConsider an IoT (Internet of Things) device which is observing an autoregressive process\n$\\{X_t\\}_{t \\ge 0}$, $X_t \\in \\integers$, which starts at $X_1 = 0$ and for\n$t > 1$ evolves as \n$$ X_{t+1} = X_t + W_t $$\nwhere $\\{W_t\\}_{t \\ge 1}$ is an i.i.d. process with $W_t \\in \\{ -5, \\dots,\n5 \\}$ with \n$$ \\PR(W_t = w) =\n\\begin{cases}\n  \\frac{1}{5} - \\frac{|w|}{25}, & \\text{if } |w| \\le 5 \\\\\n  0, & \\text{otherwise}\n\\end{cases}$$\n\nThe IoT device can either transmit its observation (denoted by $A_t = 1$)\nor not (denoted by $A_t = 0$). Transmitting a packet has a cost $\\lambda$\nwhile not transmitting has no cost. \n\nWhen $A_t = 0$, the receiver estimates the state of the process as the\npreviously transmitted observation $Z_t$ and incurs a cost $(X_t -\nZ_t)^2$. \n\nThe above system can be modeled as an MDP with state $\\{S_t \\}_{t \\ge 0}$,\nwhere $S_t = X_t - Z_t$. It can be shown that the dynamics of $\\{S_t\\}_{t\n\\ge 1}$ are as follows:\n$$ S_{t+1} = \\begin{cases}\n    S_t + W_t, & \\text{if } A_t = 0 \\\\\n    W_t, & \\text{if } A_t = 1\n  \\end{cases} $$\n\nThe per-step cost is given by\n$$ c(s,a) = \\lambda a + (1-a)s^2. $$\n\nThe objective of this exercise is to find the optimal policy for the above problem using dynamic programming. \n\nIn this model, the state space is unbounded which makes it difficult to use dynamic programming. So, we construct approximate dynamics as follows. We pick a large number $B$ and assume that the dynamics are:\n$$ S_{t+1} = \\begin{cases}\n    [S_t + W_t]_{-B}^B, & \\text{if } A_t = 0 \\\\\n    [W_t]_{-B}^B, & \\text{if } A_t = 1\n  \\end{cases} $$\n\nSo, we can effective consider the state space to be $\\{-B, \\dots, B\\}$. \n\n\na. Solve the dynamic program for $T = 20$, $λ = 100$, and $B = 100$.\n\nb. Plot the value function for $t \\in \\{1, 5, 10, 19 \\}$ on the same plot.\n\nc. Plot the optimal policy for $t \\in \\{1, 5, 10, 19 \\}$.\n\nd. Change the value of $B$ in the set $\\{50, 60, 70, 80 \\}$ to make sure\nthat our truncation does not have a significant impact on the value\nfunction and the optimal policy.\n\n:::\n\n## Notes {-}\n\nThe proof idea for the optimality of Markov policies is based on a proof by @Witsenhausen1979 on the structure of optimal coding policies for real-time communication. Note that the proof does not require us to find a dynamic programming decomposition of the problem. This is in contrast with the standard textbook proof where the optimality of Markov policies is proved as part of the dynamic programming decomposition. \n\n@exr-internet-of-things is adapted from @Chakravorty2018.\n\n<!-- FIXME\nAdd notes on the history of MDPs. Zormello's proof of existence of value of chess. Results on inventory management. Bellman's book. Howard's book (Howard's paper on history of MDP). Bellman's anecdote on the term \"dynamic programming\".\n-->\n\n",
    "supporting": [
      "intro_files"
    ],
    "filters": [],
    "includes": {}
  }
}