---
title: Inventory management (revisted)
keywords:
  - inventory management
  - base-stock policy
  - reward shaping
  - structural results
  - stochastic optimization
  - infinite horizon
  - discounted cost
execute:
  echo: false
  cache: true
  freeze: true
---

:::{.callout-note icon=false appearance="simple"}
# <i class="bi bi-journal-text text-primary"></i> Summary
One of the potential benefits of modeling a system as infinite horizon discounted cost MDP is that it can be simpler to identify an optimal policy. We illustrate this using the inventory management example.
:::

Let's reconsider the model for [inventory management] and assume that it runs for an infinite horizon. We assume that the per-step cost is given by $$c(s,a,s_{+}) = p a + γ h(s_{+}), $$ where 
$$ h(s) = \begin{cases}
  c_h s, & \text{if $s \ge 0$} \\
  -c_s s, & \text{if $s < 0$},
\end{cases}$$
where $c_h$ is the per-unit holding cost, $c_s$ is the per-unit shortage cost, and $p$ is the per-unit procurement cost. Note that we have assumed that the holding or shortage cost is discounted because this cost is incurred at the end of the time period.  

Recall that in the finite horizon setting, the optimal policy was a _base-stock_ policy characterized by thresholds $\{s^*_t\}_{t \ge 1}$. In the infinite horizon discounted setting, we expect the optimal policy to be time-homogeneous, i.e., the thresholds $\{s^*_t\}_{t \ge 1}$ be a constant $s^*$ and not to depend on time.

```{julia}
#| output: false
# Install packages
using Pkg; Pkg.activate(".")
# for pkg in ["IJulia", "Revise", "Distributions", "OffsetArrays", "DataFrames", "JSON"]
#    Pkg.add(pkg)
# end
# for url in [ "https://github.com/adityam/MDP.jl.git"]
#   Pkg.add(url=url)
# end
# # Installing Jupyter Cache
# Pkg.add("Conda")
# using Conda
# Conda.add("jupyter-cache")

using Revise

using Distributions: Exponential, Binomial, pdf, cdf
using OffsetArrays
using MDP
using DataFrames
```

As an illustration, let's reconsider the example used for the [finite horizon
setting][finite] (where $c_h = 2$, $c_s = 5$, $p=1$, and the demand is
Binomial(50,0.4)). We consider the discount factor $γ = 0.9$. The value
function and optimal policy is this case are shown below.

[finite]: inventory-management.html#fig1

```{julia}
#| output: false

function generate_binomial_demand(n,q,ch,cs,p,γ)
  Pw = Binomial(n,q)
  h(s) = (s >= 0) ? ch*s : -cs*s

  L = 10n
  S = -L:L
  A = 0:2L+1
  W = 0:n

  function bellmanUpdate!(v_next, π_next, v; discount=1.0)
      # Assume that v is of size 2L + 1
      # Construct a post-decision state value function
      H = OffsetArray(zeros(3L+1),-L:2L)
      # h(z) = E[ h(z-W) + γ V(z-W) ]
      for z ∈ -L:2L, w ∈ W
          next_s = clamp(z-w, -L, L)
          H[z] += ( h(z-w) + discount * v[next_s] )* pdf(Pw,w)
      end

      # V(s) = min_{a } { p*a + H(s+a) }
      for s ∈ S
          opt = 0
          val = H[s]

          for a ∈ A
              next_z = clamp(s+a, -L, 2L)
              newVal = p*a + H[next_z]
              if newVal <= val
                  opt = a
                  val = newVal
              end
          end
          v_next[s] = val
          π_next[s] = opt
      end
  end

  model = DynamicModel(bellmanUpdate!; objective=:Min)
  v_initial = OffsetArray(zeros(size(S)), S)
  (V, π) = valueIteration(model, v_initial, discount=γ)

  return (S,V,π)
end
```

```{julia}
#| output: false
df_bin = DataFrame(state=Float64[], value=Float64[], policy=Float64[], holding=Float64[], shortage=Float64[])

Ch_bin = range(start=0.5, step=0.5, stop=5.0)
Cs_bin = range(start=0.5, step=0.5, stop=5.0)
for ch in Ch_bin, cs in Cs_bin
 (S,V,π) = generate_binomial_demand(50, 0.4, ch, cs, 1., 0.9)
 for s in S
    push!(df_bin, (s, V[s], π[s], ch, cs))
 end
end
```

```{julia}
ojs_define(binomial = df_bin)
```

```{ojs}
//| layout-ncol: 2
viewof ch_bin = Object.assign(Inputs.range([0.5, 5], {label: "ch", step: 0.5, value: 2.0 }), {style: '--label-width:20px'})
viewof cs_bin = Object.assign(Inputs.range([0.5, 5], {label: "cs", step: 0.5, value: 5.0 }), {style: '--label-width:20px'})
```

```{ojs}
//| layout-ncol: 2
//| fig-cap: Optimal policy for the model with binomial demand. 
//| fig-subcap:
//|     - Value function
//|     - Optimal policy

ValuePlotBin = Plot.plot({
  grid: true,
  x: { domain: [-30, 70] },
  y: { domain: [200, 400] },
  marks: [
    // Axes
    Plot.ruleX([0]),
    // Plot.ruleY([0]),
    // Data
    Plot.line(binomial.filter(d => d.shortage == cs_bin && d.holding == ch_bin),
              {x: "state", y: "value", stroke: "black"}),
  ]}
)

PolicyPlotBin = Plot.plot({
  grid: true,
  x: { domain: [0, 30] },
  y: { domain: [0, 30] },
  marks: [
    // Axes
    Plot.ruleX([0]),
    // Plot.ruleY([0]),
    // Data
    Plot.line(binomial.filter(d => d.shortage == cs_bin && d.holding == ch_bin),
              {x: "state", y: "policy", stroke: "black"}),
  ]}
)
```

---

We are interested in the following question: _Is it possible to identify the optimal threshold of the base-stock policy without explicitly solving the dynamic program?_ In this section, we show that the answer is affirmative.

As a first step, we modify the per-step cost using [reward shaping]. In
particular, we consider the following potential function

$$\varphi(s) = h(s) + \frac1{γ} p s - \frac{1}{1-γ}p\mu,$$
where $\mu = \EXP[W]$ is the expected number of arrivals at each time period. 

Now consider a new cost function
$$\begin{align*}
  c'(s,a,s_{+}) &= c(s,a,s_{+}) + \varphi(s) - γ \varphi(s_{+}) \\
  &= pa + γ h(s_{+}) + h(s) + \frac{1}{γ} p s - \frac{1}{1-γ} p \mu
  - γ h(s_{+}) - p s_{+} - \frac{γ}{1-γ} p \mu \\
  &= h(s) + \frac{1-γ}{γ} ps + p w - p \mu.
\end{align*} $$
Note that 
$$ \EXP[ c'(s,a,S_{+}) | S = s, A = a ] = h(s) + \frac{1-γ}{γ} ps 
=: c^*(s). $$
Thus, the optimal policy of the original model is the same as that in which
the per-step cost is given by $c^*(s)$. 

Recall that the optimal policy in the original model was a base stock 
policy. For the infinite horizon model, the threshold will become
time-invariant. Thus, the optimal policy will be of the form
$$
  π(s) = \begin{cases}
  s^* - s, & \text{if $s \le s^*$} \\
  0, & \text{otherwise}.
\end{cases}$$

The infinite horizon dynamic programming with this modified cost is given by 
$$\begin{equation}\label{eq:DP}
  V(s) = \min_{a \in \reals_{\ge 0}} 
   \bigl\{ c^*(s) + γ \EXP[ V(s + a - W) ] \bigr\}. 
\end{equation}$$

Using the structure of the optimal policy identified above, we have two
properties. First,
$$\begin{equation}\label{eq:opt}
V(s) = c^*(s) + γ \EXP[ V(s^* - W) ], \qquad s \le s^*.
\end{equation}$$
Second, at $s = 0$, $π(0) = s^*$ (and recall that $c^*(0) = 0$). Therefore,
$$\begin{equation}\label{eq:opt-policy}
  s^* = \arg\min_{a \in \reals_{\ge 0}} γ \EXP[V(a - W)]
\end{equation}$$

Let $F(s^*)$ denote $\EXP[V(s^*-W)]$. Then, substituting $s = s^* - W$ in 
\\eqref{eq:opt} and taking expectations, we get
$$F(s^*) = \EXP[ c^*(s^* - W) ] + γ F(s^*).$$
Thus, 
$$ \EXP[V(s^* - W)] = F(s^*) = \frac{1}{1-γ} \EXP[ c^*(s^*-W) ]. $$

Substituting the above in \\eqref{eq:opt-policy}, we get
$$ s^* = \arg\min_{s^* \ge 0} \frac{γ}{1-γ} \EXP[ c^*(s^* - W) ].$$
Consequently, we have the following:

::: {#thm-inventory-inf}
The optimal threshold $s^*$ is given by the value of $s^*$ which minimizes $\EXP[ c^*(s^*-W) ]$. In particular, if $F$ denotes the CDF of the demand, then
$$\begin{equation}
 s^* = F^{-1}\left( \frac{c_s - p(1-γ)/γ}{c_h + c_s} \right).
 \label{eq:opt-threshold}
\end{equation}$$
::: 

::: {.callout-note collapse="true"}
#### Proof {-}
The proof idea is the same approach as that used for the [newsvendor
problem](../stochastic/newsvendor.html). In particular, let $f$ denote the
distribution of $W$, $F$ denote the CDF, and $μ = \EXP[W]$. Then,
$$\begin{align}
\EXP[c^*(s^*-W)] &= \EXP[h(s^*-W)] + \frac{1-γ}{γ}p(s^* - μ) 
\notag \\
&= \int_{0}^{s^*} c_h(s^*-w)f(w)dw + 
   \int_{s^*}^{∞} c_s(w-s^*)f(w)dw + \frac{1-γ}{γ}p(s^* - μ) \notag
\end{align}$$
Thereore, 
$$\begin{align}
\frac{∂ \EXP[c^*(s^*-W)]}{∂s^*} &=
  \int_{0}^{s^*} c_h f(w)dw
+ \int_{s^*}^{\infty}[-c_s] f(w)dw 
+ \frac{1-γ}{γ}p \notag \\
&= c_h F(s^*) - c_s(1 - F(s^*))
+ \frac{1-γ}{γ}p \notag \\
\end{align}$$
To find the optimal threshold, we set the derivative to $0$ and simplify to
obtain \\eqref{eq:opt-threshold}.
:::

We can use \\eqref{eq:opt-threshold} to find the optimal threshold for the
example used above. In particular, we need to find the value of $s^*$ at which
the CDF of the demand equals the value $(c_s - p(1-γ)/γ)/(c_h + c_s)$, as
shown in @fig-threshold below.

```{julia}
#| output: false
function generate_demand()
  n, q = 50, 0.4
  ch, cs, p = 2, 5, 1

  Pw = Binomial(n,q)

  demand_df = DataFrame(demand=Float64[], probability=Float64[], cumulative=Float64[])
  for k in 0:n
    push!(demand_df, (k, pdf(Pw,k), cdf(Pw,k)) )
  end
  return demand_df
end
demand_df = generate_demand()
```

```{julia}
ojs_define(W = demand_df)
```

```{ojs}
//| layout-ncol: 2
viewof ch_val = Object.assign(Inputs.range([0.5, 5], {label: "ch", step: 0.5, value: 2.0 }), {style: '--label-width:20px'})
viewof cs_val = Object.assign(Inputs.range([0.5, 5], {label: "cs", step: 0.5, value: 5.0 }), {style: '--label-width:20px'})
```


```{ojs}
p  = 1
discount = 0.9
F_opt = (cs_val - p*(1-discount)/discount)/(ch_val + cs_val)
```

```{ojs}
//| fig-cap: The optimal threshold for the example shown above.
//| label: fig-threshold
demand_CDF_plot = Plot.plot({
  grid: true,
  marks: [
    // Axes
    Plot.ruleX([0]),
    Plot.ruleY([0]),
    // Data
    Plot.line(W, {x:"demand", y:"cumulative",curve:"step-after"}),
    Plot.line([ [0,F_opt], [W[W.length-1].demand, F_opt] ], {stroke: "red"})
  ]
})
```

---

## Exercises {-}

```{julia}
#| output: false

function generate_exponential_demand(μ, ch, cs, p, γ)
  discount = γ 

  δ = 0.1
  L = 500

  S = -L:L
  A = 0:2L+1
  W = 0:min(L,300)

  point(ℓ) = ℓ * δ

  Pw_true = Exponential(μ)
  Pw = OffsetArray(zeros(size(W)), W)
  for w ∈ W
      if w != W[end]
          Pw[w] = cdf(Pw_true, point(w+1)) - cdf(Pw_true, point(w))
      else
          Pw[w] = 1 - cdf(Pw_true, point(w))
      end
  end

  h(s) = (s >= 0) ? ch*s : -cs*s

  function bellmanUpdate!(v_next, π_next, v; discount=1.0)
      # Assume that v is of size 2L + 1
      # Construct a post-decision state value function
      H = OffsetArray(zeros(3L+1),-L:2L)
      # h(z) = E[ h(z-W) + γ V(z-W) ]
      for z ∈ -L:2L, w ∈ W
          next_s = clamp(z-w, -L, L)
          H[z] += ( h(z-w) + discount * v[next_s] )*Pw[w]
      end
  
      # V(s) = min_{a } { p*a + H(s+a) }
      for s ∈ S
          opt = 0
          val = H[s]
  
          for a ∈ A
              next_z = clamp(s+a, -L, 2L)
              newVal = p*a + H[next_z]
              if newVal <= val
                  opt = a
                  val = newVal
              end
          end
          v_next[s] = val
          π_next[s] = opt
      end
  end
  model = DynamicModel(bellmanUpdate!; objective=:Min)
  v_initial = OffsetArray(zeros(size(S)), S)

  (V,π) = valueIteration(model, v_initial; discount=discount)

  threshold = μ * log( (ch + cs) / (ch + p*(1-discount)/discount) )
  return (δ,S,V,π,threshold)
end

df_exp = DataFrame(state=Float64[], value=Float64[], policy=Float64[], holding=Float64[], shortage=Float64[])

df_threshold = DataFrame(threshold=Float64[], holding=Float64[], shortage=Float64[])

Ch_exp = range(start=0.5, step=0.5, stop=5.0)
Cs_exp = range(start=0.5, step=0.5, stop=5.0)
for ch in Ch_exp, cs in Cs_exp
 (δ,S,V,π,threshold) = generate_exponential_demand(5, ch, cs, 1., 0.9)
 for s in S
    push!(df_exp, (s*δ, V[s], π[s], ch, cs))
 end
 push!(df_threshold, (threshold, ch, cs))
end
```

```{julia}
ojs_define(exponential = df_exp)
ojs_define(threshold_exp = df_threshold)
```


:::{#exr-inventory-exponential}
Suppose that the arrival process is exponential with rate $1/\mu$, i.e.,
the density of $W$ is given by $e^{-s/\mu}/\mu$. Show that the optimal
threshold is given by
$$ s^* = \mu \log \left[ \frac{ c_h + c_s} { c_h + p (1-γ)/γ} \right]. $$

 _Hint_: Recall that the CDF the exponential distribution is
 $F(s) = 1 - e^{-s/μ}$.
:::

The plots below verify this result numerically. We simulate the system with parameters $μ = 5$, $p = 1$, $γ = 0.9$. 

```{ojs}
//| layout-ncol: 2
viewof ch_exp = Object.assign(Inputs.range([0.5, 5], {label: "ch", step: 0.5, value: 2.0 }), {style: '--label-width:20px'})
viewof cs_exp = Object.assign(Inputs.range([0.5, 5], {label: "cs", step: 0.5, value: 5.0 }), {style: '--label-width:20px'})
```

```{ojs}
//| fig-cap: Optimal policy for model with exponential demand. The black curve shows the optimal policy computed via value iteration and the red line shows the threshold computed via the formula of @exr-inventory-exponential.

ValuePlotExp = Plot.plot({
  grid: true,
  x: { domain: [-11, 11] },
  y: { domain: [0, 200] },
  marks: [
    // Axes
    Plot.ruleX([0]),
    Plot.ruleY([0]),
    Plot.ruleX(threshold_exp.filter(d => d.shortage == cs_exp && d.holding == ch_exp), 
               {x: "threshold", stroke: "red"} ),
    // Data
    Plot.line(exponential.filter(d => d.shortage == cs_exp && d.holding == ch_exp),
              {x: "state", y: "policy", stroke: "black"}),
  ]}
)
```

## Notes {-}

The idea of using [reward shaping] to derive a closed form expression for inventory management is taken from @Whittle1982. It is interesting to note that @Whittle1982 uses the idea of reward shaping more than 17 years before the paper by @Ng1999 on reward shaping. It is possible that Whittle was using the results of @Porteus1975. As far as I know, the explicit formula of the threshold presented in @thm-inventory-inf has not appeared in the literature before.

As established in the notes on [Lipschitz MDPs], it can be shown that the optimal value function for the inventory management model above is Lipschitz continuous.

In the analysis above, we did not formally verify that all the functions exist and infinimizations were well defined. For a detailed discussion of such technical details, see @Feinberg2016.

[inventory management]: inventory-management.qmd
[reward shaping]: reward-shaping.qmd
[Lipschitz MDPs]: lipschitz-mdps.qmd#inventory-example
