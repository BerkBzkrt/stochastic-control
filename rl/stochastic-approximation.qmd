---
title: "Stochastic approximation"
keywords:
  - reinforcement learning
  - stochastic approximation
execute:
  echo: false
  cache: true
  freeze: true
---

Suppose $f \colon \reals^d \to \reals^d$ and it is desired to find a solution $θ^*$ to the equation $f(θ) = 0$. There are many methods for determining the value of
$θ$ by successive approximation where we start with an initial guess $θ_0$ and
then recursively obtain a new value $θ_{t+1}$ as a function of the previously
obtained $θ_0, \dots, θ_{k}$, the values $f(θ_1), \dots, f(θ_{t})$, and
possibly those of the derivatives $f'(θ_0), \dots, f'(θ_{t})$, etc. If 
$$
  \lim_{t \to ∞} θ_t = θ^*,
$$
irrespective of the initial condition $θ_0$, then the successive approximation
method is effective. 

In many applications, the function $f$ may be unknown so it is not possible to
obtain the value $f(θ)$, but it may be possible to conduct an experiment to
get the value of $f(θ)$ with noise. Stochastic approximation refers to
recursive algorithms of the form
$$ \begin{equation} \label{eq:SA}
  θ_{t+1} = θ_t + α_t[ f(θ_t) + ξ_{t+1} ], \quad t \ge 0
\end{equation} $$
where $\{α_t\}_{t \ge 0}$ is a sequence of positive numbers and 
$\{ξ_t\}_{t \ge 0}$ is a noise sequence.

We are interested in the limit behavior of the sequence $\{θ_t\}_{t \ge 0}$. 

::: {#exm-urn-model}
Consider an initially empty urn to which black or red balls are added one at a time. Let $r_t$ denote the number of red balls at time $t$ and $θ_t = r_t/t$ denote the _fraction_ of red balls at time $t$. Suppose that 
$$
  \PR(\text{next ball is red} \mid \text{all past}) = p(\text{fraction of red balls})
$$
where $p \colon [0,1] \to [0,1]$ is prespecified. 

In this model, the sequence $\{r_t\}_{t \ge 1}$ follows the recursion:
$$
  r_{t+1} = r_t + w_{t+1}
$$
where $w_{t+1} = \IND\{(t+1)\text{-st ball is red}\}$. Therefore,
$$
  θ_{t+1} = θ_t + \frac{1}{t+1}( w_{t+1} - θ_t )
$$
with $θ_0 = 0$. We can rewrite the above equation as
$$
  θ_{t+1} = θ_t + \frac{1}{t+1}\bigl[ (p(θ_t) - θ_t ) + (w_{t+1} - p(θ_t)) \bigr]
$$
Define $ξ_{t+1} = w_{t+1} - p(θ_t)$. Note that $\{ξ_t\}_{t \ge 1}$ is Martingale difference sequence, i.e., $\EXP[ ξ_{t+1} \mid θ_0, w_{1:t} ] = 0$. Thus the above equation is of the form \\eqref{eq:SA} with $f(θ_t) = p(θ_t) - θ_t$. 
:::

The key idea behind stochastic approximation is that under appropriate
conditions, the iteration \\eqref{eq:SA} almost surely converges to the
equilibrium point of the ODE
$$ \begin{equation} \label{eq:ODE}
  \dot θ(t) = f(θ(t))
\end{equation} $$
with initial conditions $θ(0) = θ_0$. For instance, for @exm-urn-model, this means that under appropriate conditions, the discrete-time iterates $\{θ_t\}_{t \ge 0}$ converge to the solution of the ODE \\eqref{eq:ODE}. In particular, they would converge to the equilibrium set $H = \{ θ : p(θ) = θ \}$. 

```{julia}
#| output: false
# Install packages
using Pkg; Pkg.activate(".")
for pkg in ["IJulia", "Plots", "LaTeXStrings", "SpecialFunctions",
            "Distributions", "DataFrames", "JSON", "Revise"]
    Pkg.add(pkg)
end
# Installing Jupyter Cache
# Pkg.add("Conda")
# using Conda
# Conda.add("jupyter-cache")

using Revise

```

```{julia}
#| output: false
# Initializtions
using Plots, LaTeXStrings
current_theme = :bright
current_colors = theme_palette(current_theme)
theme(current_theme)

# Use the incomplete beta function as an example
# The function returns a tuple and we are only interested in the first value
using SpecialFunctions: beta_inc
p(θ) = beta_inc(5,5, θ)[1]
```

```{julia}
#| column: margin
#| label: fig-p-vs-theta
#| fig-cap: Example of $p$ such that there are multiple solutions to $p(θ) = θ$.
default(linewidth=2, size=(300,200), xlab=L"\theta")
plt = plot(p, 0, 1, label=L"p(\theta)")
plot!(plt, θ->θ, 0, 1, label=L"\theta")
```

Suppose that $p(θ)$ is such that there exists a $θ_\circ$ such that $p(θ) > θ$ for $θ \in (θ_\circ, 1)$ and $p(θ) < θ$ for $θ \in (0, θ_\circ)$. See @fig-p-vs-theta, for example. Then, the set of equilibrium points are $H = \{0, θ_\circ, 1\}$. Out of these $\{0, 1\}$ are stable and $θ_\circ$ is unstable. The stochastic approximation theory shows that the iterations \eqref{eq:SA} will converge to either $0$ or $1$. Thus, along each sample path, the iterates $\{θ_t\}_{t \ge 1}$ will be 'locked into' one color which will dominate. 

Multiple runs starting with different initial conditions are shown in @fig-urn-multiple-runs. Note that the discrete-time iterates are stochastic and oscillate, as expected. However, the eventually converge to the stable equilibrium points of the ODE \eqref{eq:ODE}

```{julia}
# Copied from notebook: julia-examples/stochastic-approximation-urn-model/urn-model.ipynb
using Distributions, Random
using DataFrames
```

```{julia}
#| echo: true

Random.seed!(42)

T = 10       # Number of time steps
M = 25       # Number of different runs
α = zeros(T)
for t in 1:T
    α[t] = 1/(t+1)
end

θ = zeros(T,M)
for m in 1:M
    θ[1,m] = mod((m/M + rand()/M),1)
    for t in 1:T-1
        θ[t+1,m] = θ[t,m] + α[t]*(rand(Bernoulli(p(θ[t,m]))) - θ[t,m])
    end
end
```

```{julia}
# Convert data to observable
df = DataFrame(time=Int[], run=Int[], value=Float64[])
for t in 1:T, m in 1:M
    push!(df, (t, m, θ[t,m]))
end
ojs_define(theta = df)
```

```{ojs}
//| label: fig-urn-multiple-runs
//| fig-cap: Multiple runs of the urn model (@exm-urn-model) for different initial conditions.
SimulationPlot = Plot.plot({
  grid: true,
  marks: [
    // Axes
    Plot.ruleX([1]),
    Plot.ruleY([0]),
    // Data
    Plot.line(theta, {x: "time", y: "value", z: "run", stroke: "#9ecae1" })
  ]}
)
```

We now summarize the sufficient conditions of convergence.

## List of assumptions

Let $\mathcal F_t = σ(θ_{1:t}, ξ_{1:t}, α_{1:t})$. We state the following set of assumptions but note that not every assumption is needed for every result.

### Assumptions on the function $f$

**F1.** $θ^*$ is a solution of the equation $f(θ) = 0$.

**F1'.** $θ^*$ is _the unique_ solution of the equation $f(θ) = 0$.

**F2.** The function $f$ is [globally Lipschitz-continuous][Lipschitz] with constant $L$, i.e., for any $θ_1, θ_2 \in \reals^d$,
$$
  \| f(θ_1) - f(θ_2) \|_{2} \le L \| θ_1 - θ_2 \|_2.
$$
**F2'.** The function $f$ is twice differentiable and is globally Lipschitz continuous with constant $L$. 

:::{.callout-tip}
### Implication
Assumption (F2) implies that for each $θ \in \reals^d$, there is a unique function $s(\cdot, θ)$ that satisfies the ODE
$$
  \frac{ds(t,θ)}{dt} = f(s(t,θ)), \quad
  s(0,θ) = θ.
$$
:::

**F3.** The equilibrium $θ^*$ of the ODE $\dot θ = f(θ)$ is globally asymptotically  stable. 

**F3'.** The equilibrium $θ^*$ of the ODE $\dot θ = f(θ)$ is globally _exponentially_ stable. Thus, there exists constants $μ \ge 1$ and $γ > 0$ such that
$$
\| s(t,θ) - θ^*\|_2 \le μ\|θ - θ^*\|_2 \exp(-γ t), 
\quad
\forall t \ge 0, \forall θ \in \reals^d.
$$

**F4.** There is a finite constant $K$ such that 
$$
\| \nabla^2 f_i(θ) \|_{S} \cdot \| θ - θ^*\|_2 \le K, 
\quad
\forall i \in \{1, \dots, d\}, 
\forall θ \in \reals^d,
$$
where $\|\cdot\|_S$ denotes the spectral norm of a matrix (i.e., the largest singular value). 

:::{.callout-tip}
### Implication
Assumption (F4) implies that 
$$
\left| \frac{∂^2 f_i(θ)}{∂θ_j ∂θ_k}\right| \cdot \| θ - θ^*\|_2 \le K,
\quad
\forall i.j,k, \in \{1,\dots, d\},
\forall θ \in \reals^d.
$$
:::

### Conditions on the noise

**N1.** $\{ξ_t\}_{t \ge 0}$ is a [martingale difference sequence][MDS] with respect to $\{ \mathcal F_t\}_{t \ge 1}$, i.e.,
$$ \EXP[ ξ_{t+1} | \mathcal F_t ] = 0, \text{ a.s.}, \quad \forall t \ge 1. $$

**N2.** The noise $\{ξ_t\}_{t \ge 1}$ satisfies
$$
\EXP[ \| ξ_{t+1}^2 \|_2^2 \mid \mathcal F_t ] \le 
σ^2( 1 + \| θ_t - θ^*\|_{2}^2), 
\quad \text{a.s. } \forall t \ge 1
$$
for some finite constant $σ^2$. 

[MDS]: ../probability/martingales.qmd#examples

### Conditions on the learning rate

**R1.** $\sum_{t \ge 1} α_t^2 < ∞$.

**R2.** $\sum_{t \ge 1} α_t = ∞$.

**R3.** There exists constants $\underline α, \bar α \in (0,1)$ such that
    $\underline α \le α_t \le \bar α$ for all $t \ge 1$. 


## Gladyshev's result

The following is a restatement of the result of @Gladyshev1965.

:::{#thm-Gladyshev}
Suppose assumptions (F1'), (N1), and (N2) hold. In addition, the function $f(\cdot)$ is passive, i.e., for each $0 < ε < M < ∞$, 
$$ \sup_{ε < \| θ - θ^*\|_2 < M} 
  \langle θ - θ^*, f(θ) \rangle 
 < 0$$
 and
 $$\|f(θ)\|_2 \le K \|θ - θ^*\|_2, \quad K < ∞.$$
 Then,

 1. If (R1) holds, then $\{θ_t\}$ is bounded almost surely.
 2. In addition, if (R2) holds, then $θ_t \to θ^*$ almost surely as $t \to ∞$. 
:::

:::{.callout-tip}
### Remark
The second assumption $\|f(θ)\|_2 \le K \| θ - θ^*\|_2$ implies that $f(⋅)$ is continuous as $θ^*$, but it need not be continuous anywhere else.
:::


<!-- :::{.callout-note collapse="false"}
#### Proof
From \\eqref{eq:SA}, we get
\begin{align}
  \EXP[ \|θ_{t+1} - θ^*\|_2^2 \mid \mathcal F_t] 
   &\stackrel{(a)}= 
  \|θ_t - θ^*\|_2^2 -2 α_t \langle θ_t - θ^*,  f(θ_t) \rangle 
  + α_t^2 \bigl[ \| f(θ_t) \|_2^2 + ξ^2 \bigr]
  \notag \\
  &\stackrel{(b)}\le
\end{align}
where $(a)$ uses (N1)
::: -->

## Borkar-Meyn's result {#sec-borkar-meyn}

The following is a restatement of the result of @Borkar2000.

:::{#thm-borkar-meyn}
Suppose assumptions (F2), (N1), and (N2) hold. In addition:

a. There exists a limit function $f_{∞} \colon \reals^d \to \reals^d$ such that
$$
  \lim_{r \to ∞} \frac{f(r θ)}{r} = f_{∞}(θ), \quad
  \forall θ \in \reals^d.
$$

b. Origin is asymptotically stable equilibrium of the ODE
$$
  \dot θ(t) = f_{∞}(θ(t)).
$$

Then, 

1. If (R1) and (R2) hold, then $\{θ_t\}_{t \ge 1}$ is bounded almost surely. 
2. In addition, if (F3) holds, then $θ_t \to θ^*$ almost surely as $t \to ∞$. 
3. If (F3) and (R3) hold, then:
    
   - there exists a $α^* > 0$ and $C_1 < ∞$ such that if $\bar α \le α^*$ then 
     $$ 
        \limsup_{n \to ∞} \EXP[ \| θ_k\|^2 ] \le C_1.
     $$

   - if $\bar α \le α^*$ then $θ_t \to θ^*$ in probability. In particular, for any $ε > 0$, there exists a $b_1 = b_1(ε) < ∞$ such that 
     $$
        \limsup_{n \to ∞} \PR( \| θ_k - θ^* \| \ge ε ) \le b_1 \bar α.
     $$

   - In addition, if (F3') holds, then $θ_t \to θ^*$ in mean square. In particular, there exists a $b_2 < ∞$ such that for any initial condition $θ_0 \in \reals^d$, 
     $$
        \limsup_{n \to ∞} \EXP[ \| θ_k - θ^*\|^2 ] \le b_2 \bar α. 
     $$
:::

:::{.callout-tip}
### Rates of convergence

@Borkar2000 also provides rates of convergence of stochastic approximation under stronger assumptions. 
:::

[Lipschitz]: ../mdps/lipschitz-mdps.qmd

## Vidyasagar's result

The following is a restatement of the result of @Vidyasagar2023.

:::{.callout-tip}
#### Function classes
Consider a continuous function $f \colon \reals_{\ge 0} \to \reals_{\ge 0}$. 

* The function $f$ is said to belong to class $\mathcal K$ if $f(0) = 0$ and $f(\cdot)$ is strictly increasing.
* The function $f \in \ALPHABET K$ is said to belong to class $\ALPHABET K \ALPHABET R$ if, in addition, $f(r) \to ∞$ as $s \to ∞$. 
* The function $f$ is said to belong to class $\ALPHABET B$ if $f(0) = 0$ and, in addition for all $0 < ε < M < ∞$, we have
  $$
  \inf_{ε \le r \le M} f(r) > 0.
  $$
:::

:::{#exm-class-K-vs-B}
Observe that every function $f$ of class $\ALPHABET K$ also belongs to class $\ALPHABET B$ but the converse is not true. For example, let
$$
  f(r) = \begin{cases}
  r, & \text{if } r \in [0,1], \\
  e^{-(r-1)}, & \text{if } r > 1.
  \end{cases}
$$
Then, $f$ belongs to class $\ALPHABET B$. 

However, since $f(r) \to 0$ as $r \to ∞$, $f$ cannot be bounded below by any function of class $\ALPHABET K$. 
:::

:::{#thm-vidyasagar-1}
Suppose assumptions (F1), (F2), (N1), and (N2) hold. In addition, suppose that there exists a twice differentiable Lyapunov function $V \colon \reals^d \to \reals_{\ge 0}$ that satisfies the following conditions:

* There exist constants $a, b > 0$ such that
  \begin{equation}\label{eq:vidyasagar-cond-1}
    a \| θ - θ^*\|_2^2 \le V(θ) \le b \| θ - θ^* \|_2^2, 
    \quad \forall θ \in \reals^d.
  \end{equation}

* There is a finite constant $M$ such that
  \begin{equation}\label{eq:vidyasagar-cond-2}
  \| \GRAD^2 V(θ) \|_S \le 2M,
    \quad \forall θ \in \reals^d.
  \end{equation}
Then,

1. If $\dot V(θ) \coloneqq \langle \GRAD V(θ), f(θ) \rangle \le 0$ for all $θ \in \reals^d$ and (R1) holds, then the iterates $\{θ_t\}_{t \ge 1}$ are bounded almost surely.

2. If, in addition, (R2) holds and there exists a function $\phi \in \ALPHABET B$ such that
   \begin{equation}\label{eq:vidyasagar-cond-3}
    \dot V(θ) \le - \phi(\| θ - θ^*\|_2),
    \quad \forall θ \in \reals^d.
  \end{equation}
   Then, $θ_t \to θ^*$ almost surely as $t \to ∞$. 
:::

:::{.callout-tip}
#### Discussion of the conditions

It is worthwhile to compare the conditions of @thm-borkar-meyn and @thm-vidyasagar-1. 

1. In @thm-borkar-meyn, it is assumed that (F1') holds while in @thm-vidyasagar-1, it is assumed that (F1) holds. That is, there is no assumption that $θ^*$ is the unique solution of $f(θ) = 0$. 

2. The assumptions on $\dot V$ in part 1 of @thm-vidyasagar-1 imply only that $θ^*$ is a _locally stable_ equilibrim of the ODE \\eqref{eq:ODE}. This is in contrast to @thm-borkar-meyn imply that $θ^*$ is _globally asymptotically stable_.

3. The assumptions in part 2 of @thm-vidyasagar-2 ensure that $θ^*$ is globally asymptotically stable equilibrium of the ODE \\eqref{eq:ODE}. Therefore, assumption (F1') is implicit in the second part of @thm-vidyasagar-1.

:::

:::{.callout-note collapse="true"} 
#### Proof
We first start by establishing a bound on $\EXP[V(θ_{t+1}) \mid \ALPHABET  F_t]$. To do so, observe that by Taylor series, we have
$$
  V(θ + η) = V(θ) + \langle \GRAD V(θ), η \rangle
  + \frac 12 \langle η, \GRAD^2 V(θ + λη)η \rangle
$$
for some $λ \in [0,1]$. Since $\NORM{\GRAD^2 V(θ+λη)}_S \le 2M$, it follows that
$$
  V(θ + η) \le V(θ) + \langle \GRAD V(θ), η \rangle
  + M \NORM{η}_2^2.
$$
Now apply the above bound with $θ = θ_t$ and $η = θ_{t+1} - θ_t = α_t f(θ_t) + α_t ξ_{t+1}$. This gives
\begin{align*}
V(θ_{t+1}) &\le V(θ_t) 
+ α_t \langle \GRAD V(θ_t), f(θ_t) \rangle
+ α_t \langle \GRAD V(θ_t), ξ_{t+1} \rangle
  \notag \\
&\quad  + α_t^2 M \bigl[ \NORM{f(θ_t)}_2^2 + \NORM{ξ_{t+1}}_2^2 + 
2 \langle f(θ_t), ξ_{t+1} \rangle
\bigr] 
\end{align*}
Recall that $\langle V(θ), f(θ) \rangle \eqqcolon \dot V(θ)$. 
Now, we can bound $\EXP[V(θ_{t+1}) \mid \ALPHABET F_t]$ using assumptions (N1) and (N2). 
\begin{equation*}
\EXP[V(θ_{t+1}) \mid \ALPHABET F_t] \le V(θ_t) 
+ α_t \dot V(θ_t)
 + α_t^2 M \bigl[ \NORM{f(θ_t)}_2^2 + σ^2(1 + \NORM{θ_t - θ^*}_2^2)
\bigr] 
\end{equation*}
Assumption (F1) and (F2) implies that 
$$
\NORM{f(θ_t)}_2^2 = \NORM{f(θ_t) - f(θ^*)}_2^2 \le L^2 \NORM{θ_t - θ^*}_2^2.
$$
Substituting in the above bound, we get:
\begin{equation}\label{eq:vidyasagar-1-pf-step-1}
\EXP[V(θ_{t+1}) \mid \ALPHABET F_t] \le V(θ_t) 
+ α_t \dot V(θ_t)
+ α_t^2 M \bigl[ σ^2 + (σ^2 + L^2)\NORM{θ_t - θ^*}_2^2 \bigr].
\end{equation}

### Proof of part 1. {-}

Under the stated assumptions, we can simplify \\eqref{eq:vidyasagar-1-pf-step-1}
\begin{align*}
\EXP[V(θ_{t+1}) \mid \ALPHABET F_t] 
&\stackrel{(a)}\le 
V(θ_t) 
+ α_t^2 M \bigl[ σ^2 + (σ^2 + L^2)\NORM{θ_t - θ^*}_2^2 \bigr]
  \notag \\
&\stackrel{(b)}\le 
\biggl[ 1 + \frac{α_t^2 M}{a} (L^2 + σ^2) \biggr] V(θ_t) 
+ 
  α_t^2 M σ^2
\end{align*}
where $(a)$ follows from the assumption that $\dot V(θ) < 0$ and 
$(b)$ follows from $V(θ) \ge a \NORM{θ - θ^*}_2^2$. 

Thus, $\{V(θ_t)\}_{t \ge 1}$ is an "almost" supermartingale. Apply @thm-almost-supermartingale with $X_t = V(θ_t)$, $β_t = α_t^2 M(L^2 + σ^2)/a$, $Y_t = α_t^2 M σ^2$, and $Z_t = 0$. Then, from (R1) it follows that $\lim_{t \to ∞} V(θ_t)$ exists almost surely and is finite. Condition \\eqref{eq:vidyasagar-cond-1} implies that $\{θ_t\}_{t \ge 1}$ is almost surely bounded. 

### Proof of part 2. {-}

Under the stated assumptions, we can simplify \\eqref{eq:vidyasagar-1-pf-step-1}
\begin{align*}
\EXP[V(θ_{t+1}) \mid \ALPHABET F_t] 
&\stackrel{(c)}\le 
\biggl[ 1 + \frac{α_t^2 M}{a} (L^2 + σ^2) \biggr] V(θ_t) 
+ 
  α_t^2 M σ^2
- α_t \phi(\NORM{θ_t - θ^*}_2)
\end{align*}
where the first two terms are simplified in the same way as above and the last term corresponds to the upper bound on $\dot V(θ_t) \le -\phi(\NORM{θ_t - θ^*}_2^2)$. 

We can again apply @thm-almost-supermartingale with $X_t = V(θ_t)$, $β_t = α_t^2 M(L^2 + σ^2)/a$, $Y_t = α_t^2 M σ^2$, and $Z_t = α_t \phi(\NORM{θ_t - θ^*}_2)$. 
Thus, we can conclude that 
there exists a random variable $ζ$ such that $V(θ_t) \to ζ$ and
\begin{equation}\label{eq:vidyasagar-1-pf-step-2}
\sum_{t \ge 1} α_t \phi(\NORM{θ_t - θ^*}_2^2) < ∞,
\quad \mathrm{a.s.}
\end{equation}

Let $Ω_1 \subset Ω$ denote the values of $ω$ for which
$$
  \sup_{t \ge 1} V(θ_t(ω)) < ∞,
  \lim_{t \to ∞} V(θ_t(ω)) = ζ(ω),
  \sum_{t \ge 1} α_t \phi(\NORM{θ_t(ω) - θ^*}_2) < ∞.
$$
From @thm-almost-supermartingale, we know that $P(Ω_1) = 1$. We will now show that $ζ(ω) = 0$ for all $ω \in Ω_1$ by contradiction. Assume that for some $ω \in Ω_1$, we have $ζ(ω) = 2 ε > 0$. Choose a $T$ such that $V(θ_t(ω)) \ge ε$ for all $t \ge T$. Define $V_M = \sup_{t \ge 1} V(θ_t(ω))$. Then, we have that
$$
  \sqrt{\frac{ε}{b}} \le \NORM{θ_t}_2 \le \sqrt{\frac{V_M}{a}},
  \quad \forall t \ge T.
$$

Define $δ = \inf_{\sqrt{ε/b} \le r \le \sqrt{V_M/a}} \phi(r)$ and observe that $δ > 0$ because $\phi$ belongs to class $B$. Therefore,
$$
\sum_{t \ge T} α_t \phi(\NORM{θ_t - θ^*}_2) \ge
\sum_{t \ge T} α_t δ = ∞,
$$
due to (R2). But this contradicts \\eqref{eq:vidyasagar-1-pf-step-2}. Hence, there is no $ω \in Ω_1$ such that $ζ(ω) > 0$. Therefore, $ζ = 0$ almost surely, i.e., $V(θ_t) \to 0$ almost surely. Finally, it follows from \\eqref{eq:vidyasagar-cond-1} that $θ_t \to θ^*$ almost surely as $t \to ∞$. 
:::


@thm-vidyasagar-1 requires the existence of a suitable Lyapunov function that satisfies various conditions. Verifying whether or not such a function exists can be a bottleneck. 

If can be shown (see Theorem 4 of @Vidyasagar2023) that the conditions on $V$ in @thm-vidyasagar-1 ensure that the equilibrium $θ^*$ of the ODE \\eqref{eq:ODE} is globally asymptotically stable. By strengthening this assumption to global _exponential_ stability of $θ^*$ and adding a few other conditions, it is possible to establish a "converse" Lyapunov theorem that establishes the existence of such a $V$. This is done below.

:::{#thm-vidyasagar-2} 
Suppose assumptions (F1'), (F2'), (F3) and (F4) hold. Then, there exists a twice differentiable function $V \colon \reals^d \to \reals_{\ge 0}$ such that $V$ and its derivative $\dot V \colon \reals^d \to \reals_{\ge 0}$ defined as $\dot V(θ) \coloneqq \langle \langle \GRAD V(θ), f(θ) \rangle$ together satisfy the following conditions: there exist positive constants $a$, $b$, $c$, and a finite constant $M$ such that for all $θ \in \reals^d$:

* $a\NORM{θ - θ^*}_2^2 \le V(θ) \le b\NORM{θ - θ^*}_2^2$,
* $\dot V(θ) \le -c\NORM{θ - θ^*}_2^2$,
* $\NORM{\GRAD^2 V(θ)}_S \le 2M$.
:::

Combining @thm-vidyasagar-1 and @thm-vidyasagar-2, we get the following "self-contained" theorem:

:::{#thm-vidyasagar-3}
Suppose assumptions (F1'), (F2'), (F3), and (F4) as well as assumptions (N1) and (N2) hold. Then,

1. If (R1) holds then $\{θ_t\}_{t \ge 1}$ is bounded almost surely.
2. If, in addition, (R2) holds then $\{θ_t\}_{t \ge 1}$ converges almost surely to $θ^*$ as $t \to ∞$. 
:::

## Example: Temporal difference learning for a pseudo-contraction {#pseudo-contraction}

Suppose $\BELLMAN \colon \reals^d \to \reals^d$ is a pseudo-contraction with respect to the Eucledian norm, i.e., it has a fixed point $θ^*$ and a radius of contraction $γ \in (0, 1)$ such that 
\begin{equation}\label{eq:pseudo-contraction}
  \NORM{\BELLMAN θ - θ^*}_2 \le γ\NORM{θ - θ^*}_2, 
  \quad \forall θ \in \reals^n.
\end{equation}
We assume that there is an oracle. When we give an input $θ$ to the oracle, the oracle returns $\BELLMAN θ + ξ$, where $ξ$ is an independent noise. Suppose we run temporal difference update of the form:
\begin{equation}\label{eq:TD}
  θ_{t+1} = (1 - α_t) θ_t + α_t \bigl[ \BELLMAN θ_t + ξ_{t+1} \bigr]
\end{equation}
where $\{α_t\}_{t \ge 1}$ learning rate. 

As before, we define $\ALPHABET F_t = σ(θ_{1:t}, ξ_{1:t}, α_{1:t})$. Then, we have the following result:

:::{#prp-TD-pseudo-contraction}
Suppose assumptions (N1) and (N2) hold. Then,

1. If (R1) holds, then the iterates $\{θ_t\}_{t \ge 1}$ are bounded almost surely.

2. If, in addition, (R2) holds, then $θ_t \to θ^*$ almost surely as $t \to ∞$.
:::

:::{.callout-note collapse="true"} 
#### Proof
Observe that \eqref{eq:TD} can be viewed as a special case of \eqref{eq:SA} with $f(θ) = \BELLMAN θ - θ$. We will prove the result using @thm-vidyasagar-1 with 
$$
  V(θ) = \NORM{θ - θ^*}_2^2
$$
as the candidate Lyapunov function. Clearly, $V$ satisfies \eqref{eq:vidyasagar-cond-1} and \eqref{eq:vidyasagar-cond-2}. Consider
\begin{align}
\dot V(\theta) &\coloneqq \langle \GRAD V(θ), f(θ) \rangle 
\notag \\
&= \langle θ - θ^*, \BELLMAN θ - θ \rangle 
\notag \\
&= \langle θ - θ^*, \BELLMAN θ - θ^* \rangle -
   \langle θ - θ^*, θ - θ^* \rangle
\notag \\
&\stackrel{(a)}\le
   \NORM{θ - θ^*}_2 \NORM{ \BELLMAN θ - θ^* }_2
   - 
   \NORM{θ - θ^*}_2^2
\notag \\
&\stackrel{(b)}\le -(1-γ) \NORM{θ - θ^*}_2^2
\end{align}
where $(a)$ follows from Cauchy-Schwartz inequality and $(b)$ follows from \eqref{eq:pseudo-contraction}. Thus, $\dot V(θ)$ satisfies \eqref{eq:vidyasagar-cond-3} with $\phi(x) = (1-γ)x^2$. Thus, the result follows from @thm-vidyasagar-1.
:::



## Notes {-}

The stochastic approximation algorithm was introduced by @Robbins1951. See @Lai2003 for a historical overview. The classical references on this material is @Borkar2008, @Chen1991, @Kushner1997. The idea using martingales to study the convergence of stochastic approximation was introduced by @Blum1954. Also see @Gladyshev1965.

@exm-urn-model is borrowed from @Borkar2008, who points out that it was proposed by @Arthur1994 to model the phenomenon of decreasing returns in economics.

The material in this section is adapted from @Vidyasagar2023.
